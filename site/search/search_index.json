{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sobre mim engenheiro de dados mestrando em engenharia biom\u00e9dica, aplicando intelig\u00eancia artificial no diagn\u00f3stico de doen\u00e7as de pele proficiente em portugu\u00eas, ingl\u00eas e franc\u00eas veja alguns dos meus projetos Apresenta\u00e7\u00e3o Buscar conhecimento, viver experi\u00eancias e usar a engenharia como agente de transforma\u00e7\u00e3o social. Esses s\u00e3o fatores que influenciaram e influenciam nas minhas escolhas acad\u00eamicas e profissionais. A universidade p\u00fablica proporciona diversas oportunidades de aprendizado, pesquisa e extens\u00e3o do conhecimento. Por meio de participa\u00e7\u00f5es em projetos de extens\u00e3o, programa de interc\u00e2mbio e atividades extracurriculares, comecei a desenvolver tudo aquilo que considero importante, sejam conhecimentos t\u00e9cnicos ou habilidades interpessoais. As experi\u00eancias profissionais fizeram com que eu aprimorasse a minha capacidade de modelar problemas, de analisar dados e de apresentar resultados de forma simples e objetiva. Sou graduado em Engenharia de Controle e Automa\u00e7\u00e3o pela UFPE, com per\u00edodo sandu\u00edche na INP-ENSEEIHT (Fran\u00e7a). Trabalhei no setor industrial com Gest\u00e3o de Energia e Efici\u00eancia Energ\u00e9tica no Polo Automotivo Jeep (Goiana/PE) e tamb\u00e9m no setor de Engenharia Cl\u00ednica do Em\u00edlio Ribas Medicina Diagn\u00f3stica (Fortaleza/CE), com foco na \u00e1rea de An\u00e1lises Cl\u00ednicas. Atualmente, pesquiso na \u00e1rea de computa\u00e7\u00e3o biom\u00e9dica, aplicando aprendizado de m\u00e1quina, aprendizado profundo e processamento de imagem para o diagn\u00f3stico de doen\u00e7as de pele. Al\u00e9m disso, trabalho desenvolvendo solu\u00e7\u00f5es de engenharia de dados para diferentes tipos de clientes. Algumas tecnologias que tenho conhecimento Linguagem de Programa\u00e7\u00e3o Python, Shell, C, VBA Banco de Dados Modelagem, SQL, MySQL Workbench, SGBD (MySQL, MariaDB, PostgreSQL) Softwares e Ferramentas Conteineriza\u00e7\u00e3o Docker, Docker Compose Versionamento Git Orquestra\u00e7\u00e3o de pipeline Apache Airflow Aprendizagem de M\u00e1quina Weka, Keras, TensorFlow Business Intelligence Power BI, Google Data Studio","title":"Sobre mim"},{"location":"#_1","text":"","title":""},{"location":"#sobre-mim","text":"engenheiro de dados mestrando em engenharia biom\u00e9dica, aplicando intelig\u00eancia artificial no diagn\u00f3stico de doen\u00e7as de pele proficiente em portugu\u00eas, ingl\u00eas e franc\u00eas veja alguns dos meus projetos","title":"Sobre mim"},{"location":"#apresentacao","text":"Buscar conhecimento, viver experi\u00eancias e usar a engenharia como agente de transforma\u00e7\u00e3o social. Esses s\u00e3o fatores que influenciaram e influenciam nas minhas escolhas acad\u00eamicas e profissionais. A universidade p\u00fablica proporciona diversas oportunidades de aprendizado, pesquisa e extens\u00e3o do conhecimento. Por meio de participa\u00e7\u00f5es em projetos de extens\u00e3o, programa de interc\u00e2mbio e atividades extracurriculares, comecei a desenvolver tudo aquilo que considero importante, sejam conhecimentos t\u00e9cnicos ou habilidades interpessoais. As experi\u00eancias profissionais fizeram com que eu aprimorasse a minha capacidade de modelar problemas, de analisar dados e de apresentar resultados de forma simples e objetiva. Sou graduado em Engenharia de Controle e Automa\u00e7\u00e3o pela UFPE, com per\u00edodo sandu\u00edche na INP-ENSEEIHT (Fran\u00e7a). Trabalhei no setor industrial com Gest\u00e3o de Energia e Efici\u00eancia Energ\u00e9tica no Polo Automotivo Jeep (Goiana/PE) e tamb\u00e9m no setor de Engenharia Cl\u00ednica do Em\u00edlio Ribas Medicina Diagn\u00f3stica (Fortaleza/CE), com foco na \u00e1rea de An\u00e1lises Cl\u00ednicas. Atualmente, pesquiso na \u00e1rea de computa\u00e7\u00e3o biom\u00e9dica, aplicando aprendizado de m\u00e1quina, aprendizado profundo e processamento de imagem para o diagn\u00f3stico de doen\u00e7as de pele. Al\u00e9m disso, trabalho desenvolvendo solu\u00e7\u00f5es de engenharia de dados para diferentes tipos de clientes.","title":"Apresenta\u00e7\u00e3o"},{"location":"#algumas-tecnologias-que-tenho-conhecimento","text":"","title":"Algumas tecnologias que tenho conhecimento"},{"location":"#linguagem-de-programacao","text":"Python, Shell, C, VBA","title":"Linguagem de Programa\u00e7\u00e3o"},{"location":"#banco-de-dados","text":"Modelagem, SQL, MySQL Workbench, SGBD (MySQL, MariaDB, PostgreSQL)","title":"Banco de Dados"},{"location":"#softwares-e-ferramentas","text":"","title":"Softwares e Ferramentas"},{"location":"#conteinerizacao","text":"Docker, Docker Compose","title":"Conteineriza\u00e7\u00e3o"},{"location":"#versionamento","text":"Git","title":"Versionamento"},{"location":"#orquestracao-de-pipeline","text":"Apache Airflow","title":"Orquestra\u00e7\u00e3o de pipeline"},{"location":"#aprendizagem-de-maquina","text":"Weka, Keras, TensorFlow","title":"Aprendizagem de M\u00e1quina"},{"location":"#business-intelligence","text":"Power BI, Google Data Studio","title":"Business Intelligence"},{"location":"projects/","text":"Projetos Engenharia de dados Extra\u00e7\u00e3o de dados e Carga autom\u00e1tica para nuvem (AWS S3) Constru\u00e7\u00e3o de uma pipeline com est\u00e1gios de extra\u00e7\u00e3o de dados e de carregamento autom\u00e1tico na nuvem (bucket do AWS S3). Raspagem de dados do LinkedIn Extrair periodicamente do LinkedIn as vagas abertas para um determinado cargo e local de trabalho e salvar os detalhes de cada uma delas em uma planilha do Google Sheets. Modelagem de banco de dados Modelar um banco de dados, criar um gerador de massa e execut\u00e1-lo em docker. Ci\u00eancia de dados Detec\u00e7\u00e3o de fraude em seguro de ve\u00edculo Minera\u00e7\u00e3o de dados, desenvolvimento e avalia\u00e7\u00e3o de algoritmos de classifica\u00e7\u00e3o.","title":"Projetos"},{"location":"projects/#projetos","text":"","title":"Projetos"},{"location":"projects/#engenharia-de-dados","text":"Extra\u00e7\u00e3o de dados e Carga autom\u00e1tica para nuvem (AWS S3) Constru\u00e7\u00e3o de uma pipeline com est\u00e1gios de extra\u00e7\u00e3o de dados e de carregamento autom\u00e1tico na nuvem (bucket do AWS S3). Raspagem de dados do LinkedIn Extrair periodicamente do LinkedIn as vagas abertas para um determinado cargo e local de trabalho e salvar os detalhes de cada uma delas em uma planilha do Google Sheets. Modelagem de banco de dados Modelar um banco de dados, criar um gerador de massa e execut\u00e1-lo em docker.","title":"Engenharia de dados"},{"location":"projects/#ciencia-de-dados","text":"Detec\u00e7\u00e3o de fraude em seguro de ve\u00edculo Minera\u00e7\u00e3o de dados, desenvolvimento e avalia\u00e7\u00e3o de algoritmos de classifica\u00e7\u00e3o.","title":"Ci\u00eancia de dados"},{"location":"projects/de/etl-aws/","text":"Extra\u00e7\u00e3o de dados e Carga autom\u00e1tica para nuvem (AWS S3) Atividade proposta durante a participa\u00e7\u00e3o na trilha de Engenharia de Dados do Programa de Forma\u00e7\u00e3o em Dados Encantech, realizado durante os meses de Mar\u00e7o, Abril e Maio de 2022, em uma parceria entre as Lojas Renner S.A. e a CESAR School. Consiste na constru\u00e7\u00e3o de uma pipeline com est\u00e1gios de extra\u00e7\u00e3o de dados e de carregamento autom\u00e1tico na nuvem (bucket do AWS S3). acesso aos arquivos Overview A ideia \u00e9 simular o armazenamento na nuvem de backups de um banco de dados em funcionamento. O usu\u00e1rio realiza o dump no banco de dados quando achar mais pertinente (ex: hor\u00e1rio em que ningu\u00e9m esteja acessando o banco de dados); Em paralelo, um servi\u00e7o verifica periodicamente a exist\u00eancia do arquivo resultante do dump: Se houver arquivo, este \u00e9 carregado em um bucket do AWS S3; Se n\u00e3o houver arquivo, registra-se a tentativa no arquivo de log e nada mais \u00e9 executado. Resumo do funcionamento: Pontos de aten\u00e7\u00e3o Arquivo resultante do dump no banco de dados: existe uma pasta compartilhada entre o docker respons\u00e1vel pelo dump no banco de dados e o docker respons\u00e1vel pelo envio deste arquivo para a nuvem. Os seguintes cen\u00e1rios s\u00e3o poss\u00edveis: Dump realizado e pasta vazia: armazena o arquivo na pasta; Dump realizado e pasta cheia: sobrescreve o arquivo antigo; Pasta com arquivo e tentativa de envio para nuvem: envia o arquivo e o exclui da pasta compartilhada; Pasta sem arquivo e tentativa de envio para nuvem: nada \u00e9 executado. Armazenamento na nuvem: pastas divididas por data. Mais de um dump na mesma data: adiciona-se um \u00edndice no final do nome do arquivo. Periodicidade de envio para nuvem: editar o arquivo de agendamento crontab, vinculado ao Dockerfile. Pensando apenas na implementa\u00e7\u00e3o deste projeto, definiu-se um per\u00edodo de 1 minuto. Arquivo de log <data>,<hora>,<mensagem> pode ser: database dump completed - dump no banco de dados finalizado; there is no file to upload - tentativa de envio do arquivo de dump para nuvem, por\u00e9m a pasta encontra-se vazia; file successfully uploaded - arquivo de dump enviado para o respectivo bucket do AWS S3; erro apresentado ao tentar enviar arquivo para a nuvem (retorno da cl\u00e1usula except). Exemplo: Seguran\u00e7a - Credenciais da AWS Para usar a AWS com o boto3 (o AWS SDK para Python) \u00e9 necess\u00e1rio indicar as chaves de acesso do seu usu\u00e1rio ('Access key ID' e 'Secret access key'). Por seguran\u00e7a, estas chaves n\u00e3o devem ser compartilhadas com ningu\u00e9m. Al\u00e9m disso, outra informa\u00e7\u00e3o levada em conta para esta se\u00e7\u00e3o \u00e9 que o nome do bucket escolhido no projeto \u00e9 \u00fanico (cada nome de bucket deve ser exclusivo em todas as contas da AWS em todas as regi\u00f5es da AWS em uma parti\u00e7\u00e3o). A abordagem escolhida foi a de registrar essas tr\u00eas informa\u00e7\u00f5es (Access key ID, Secret access key e nome do bucket) em um \u00fanico arquivo fora do reposit\u00f3rio deste projeto. Em uma tentativa de execu\u00e7\u00e3o, se faz necess\u00e1rio a cria\u00e7\u00e3o deste arquivo com suas pr\u00f3prias credenciais. Scripts Estrutura project \u2502 README.md \u2502 docker-compose.yml \u2502 \u2514\u2500\u2500\u2500db_data \u2502 \u2502 FACULDADE.sql \u2502 \u2514\u2500\u2500\u2500dockerfiles \u2502 \u2514\u2500\u2500\u2500mysql-client \u2502 \u2502 Dockerfile \u2502 \u2514\u2500\u2500\u2500python \u2502 \u2502 Dockerfile \u2502 \u2502 crontab \u2502 \u2514\u2500\u2500\u2500logs \u2502 \u2502 2022-06-02.txt \u2502 \u2514\u2500\u2500\u2500scripts \u2502 database_dump.sh \u2502 send-to-s3.py Dockerfiles MySQL Client FROM ubuntu:focal RUN apt-get update && apt-get install -y mysql-client ENV TZ = America/Sao_Paulo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone && \\ apt-get update && apt-get install -y tzdata && dpkg-reconfigure -f noninteractive tzdata CMD tail -f /dev/null Python FROM python:3.8-alpine3.14 COPY ./crontab /var/spool/cron/crontabs/root RUN pip install boto3 ENV TZ = America/Sao_Paulo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone CMD crond -l 2 -f && tail -f /var/log/cron.log crontab: * * * * * python /bin/send-to-s3.py Docker compose version : '3.1' services : database : image : mariadb container_name : database hostname : database environment : - MARIADB_ROOT_PASSWORD=password - MARIADB_DATABASE=FACULDADE volumes : - ./db_data/FACULDADE.sql:/docker-entrypoint-initdb.d/FACULDADE.sql dump : image : mysql-client container_name : dump hostname : dump environment : - MARIADB_ROOT_PASSWORD=password depends_on : - database volumes : - ./dumps/:/dumps/ - ./scripts/database_dump.sh:/database_dump.sh - ./logs/:/logs/ command : - ./database_dump.sh send-to-s3 : image : python-s3 container_name : send-to-s3 hostname : send-to-s3 depends_on : - dump volumes : - ./dumps/:/dumps/ - ../credentials.txt:/credentials.txt - ./scripts/send-to-s3.py:/bin/send-to-s3.py - ./logs/:/logs/ Extra\u00e7\u00e3o Script em bash que realiza o dump no banco de dados, salva o arquivo resultante em uma pasta compartilhada e registra opera\u00e7\u00e3o no arquivo de log. #!/bin/bash date = $( date + '%F' ) dateAndTime = $( date + '%F,%H:%M:%S' ) mysqldump -hdatabase -uuser -ppassword FACULDADE > dumps/datadump.sql echo \" ${ dateAndTime } ,database dump completed\" >> logs/ \" ${ date } \" .txt Carga Quando o script \u00e9 chamado, antes de executar qualquer a\u00e7\u00e3o, \u00e9 verificado se existe algum arquivo a ser enviado. if __name__ == '__main__' : DUMP_FILE = '/dumps/datadump.sql' current_date = datetime . today () . strftime ( \"%Y-%m- %d \" ) current_hour = datetime . today () . strftime ( \"%H:%M:%S\" ) log_file = f \"../logs/ { current_date } .txt\" if os . path . exists ( DUMP_FILE ): main () else : with open ( log_file , 'a+' ) as file : file . write ( f \" { current_date } , { current_hour } ,there is no file to upload \\n \" ) Existindo o arquivo, abre-se uma conex\u00e3o com a AWS e este \u00e9 devidamente carregado em um bucket da AWS S3. def main (): KEY_ID , ACCESS_KEY , BUCKET_NAME = get_credentials ( '../credentials.txt' ) s3 = boto3 . client ( 's3' , region_name = 'us-east-1' , aws_access_key_id = KEY_ID , aws_secret_access_key = ACCESS_KEY ) send_dump_file = AWSobjectWrapper ( s3 ) send_dump_file . create_bucket_if_necessary ( BUCKET_NAME ) send_dump_file . upload_dump_file ( DUMP_FILE ) Abaixo segue o que \u00e9 a classe AWSobjectWrapper e seus m\u00e9todos. class AWSobjectWrapper : def __init__ ( self , s3_object ): self . object = s3_object def create_bucket_if_necessary ( self , s3_bucket ): self . bucket_name = s3_bucket existing_buckets = [ bucket [ 'Name' ] for bucket in self . object . list_buckets ()[ 'Buckets' ]] if self . bucket_name not in existing_buckets : self . object . create_bucket ( Bucket = self . bucket_name ) def count_files_in_folder ( self ): self . files_per_day = 0 existing_files = self . object . list_objects ( Bucket = self . bucket_name ) if 'Contents' in existing_files : self . files_per_day = sum ( 1 for content in existing_files . get ( 'Contents' ) \\ if current_date in content . get ( 'Key' )) def message_log_file ( self , message ): with open ( log_file , 'a+' ) as file : file . write ( f \" { current_date } , { current_hour } , { message } \\n \" ) def upload_dump_file ( self , source_file ): self . count_files_in_folder () try : self . object . upload_file ( Filename = source_file , Bucket = self . bucket_name , Key = f ' { current_date } /datadump { self . files_per_day + 1 } .sql' ) except Exception as e : self . message_log_file ( e ) else : self . message_log_file ( 'file successfully uploaded' ) os . remove ( source_file )","title":"EL na AWS"},{"location":"projects/de/etl-aws/#extracao-de-dados-e-carga-automatica-para-nuvem-aws-s3","text":"Atividade proposta durante a participa\u00e7\u00e3o na trilha de Engenharia de Dados do Programa de Forma\u00e7\u00e3o em Dados Encantech, realizado durante os meses de Mar\u00e7o, Abril e Maio de 2022, em uma parceria entre as Lojas Renner S.A. e a CESAR School. Consiste na constru\u00e7\u00e3o de uma pipeline com est\u00e1gios de extra\u00e7\u00e3o de dados e de carregamento autom\u00e1tico na nuvem (bucket do AWS S3). acesso aos arquivos","title":"Extra\u00e7\u00e3o de dados e Carga autom\u00e1tica para nuvem (AWS S3)"},{"location":"projects/de/etl-aws/#overview","text":"A ideia \u00e9 simular o armazenamento na nuvem de backups de um banco de dados em funcionamento. O usu\u00e1rio realiza o dump no banco de dados quando achar mais pertinente (ex: hor\u00e1rio em que ningu\u00e9m esteja acessando o banco de dados); Em paralelo, um servi\u00e7o verifica periodicamente a exist\u00eancia do arquivo resultante do dump: Se houver arquivo, este \u00e9 carregado em um bucket do AWS S3; Se n\u00e3o houver arquivo, registra-se a tentativa no arquivo de log e nada mais \u00e9 executado. Resumo do funcionamento:","title":"Overview"},{"location":"projects/de/etl-aws/#pontos-de-atencao","text":"Arquivo resultante do dump no banco de dados: existe uma pasta compartilhada entre o docker respons\u00e1vel pelo dump no banco de dados e o docker respons\u00e1vel pelo envio deste arquivo para a nuvem. Os seguintes cen\u00e1rios s\u00e3o poss\u00edveis: Dump realizado e pasta vazia: armazena o arquivo na pasta; Dump realizado e pasta cheia: sobrescreve o arquivo antigo; Pasta com arquivo e tentativa de envio para nuvem: envia o arquivo e o exclui da pasta compartilhada; Pasta sem arquivo e tentativa de envio para nuvem: nada \u00e9 executado. Armazenamento na nuvem: pastas divididas por data. Mais de um dump na mesma data: adiciona-se um \u00edndice no final do nome do arquivo. Periodicidade de envio para nuvem: editar o arquivo de agendamento crontab, vinculado ao Dockerfile. Pensando apenas na implementa\u00e7\u00e3o deste projeto, definiu-se um per\u00edodo de 1 minuto.","title":"Pontos de aten\u00e7\u00e3o"},{"location":"projects/de/etl-aws/#arquivo-de-log","text":"<data>,<hora>,<mensagem> pode ser: database dump completed - dump no banco de dados finalizado; there is no file to upload - tentativa de envio do arquivo de dump para nuvem, por\u00e9m a pasta encontra-se vazia; file successfully uploaded - arquivo de dump enviado para o respectivo bucket do AWS S3; erro apresentado ao tentar enviar arquivo para a nuvem (retorno da cl\u00e1usula except). Exemplo:","title":"Arquivo de log"},{"location":"projects/de/etl-aws/#seguranca-credenciais-da-aws","text":"Para usar a AWS com o boto3 (o AWS SDK para Python) \u00e9 necess\u00e1rio indicar as chaves de acesso do seu usu\u00e1rio ('Access key ID' e 'Secret access key'). Por seguran\u00e7a, estas chaves n\u00e3o devem ser compartilhadas com ningu\u00e9m. Al\u00e9m disso, outra informa\u00e7\u00e3o levada em conta para esta se\u00e7\u00e3o \u00e9 que o nome do bucket escolhido no projeto \u00e9 \u00fanico (cada nome de bucket deve ser exclusivo em todas as contas da AWS em todas as regi\u00f5es da AWS em uma parti\u00e7\u00e3o). A abordagem escolhida foi a de registrar essas tr\u00eas informa\u00e7\u00f5es (Access key ID, Secret access key e nome do bucket) em um \u00fanico arquivo fora do reposit\u00f3rio deste projeto. Em uma tentativa de execu\u00e7\u00e3o, se faz necess\u00e1rio a cria\u00e7\u00e3o deste arquivo com suas pr\u00f3prias credenciais.","title":"Seguran\u00e7a - Credenciais da AWS"},{"location":"projects/de/etl-aws/#scripts","text":"","title":"Scripts"},{"location":"projects/de/etl-aws/#estrutura","text":"project \u2502 README.md \u2502 docker-compose.yml \u2502 \u2514\u2500\u2500\u2500db_data \u2502 \u2502 FACULDADE.sql \u2502 \u2514\u2500\u2500\u2500dockerfiles \u2502 \u2514\u2500\u2500\u2500mysql-client \u2502 \u2502 Dockerfile \u2502 \u2514\u2500\u2500\u2500python \u2502 \u2502 Dockerfile \u2502 \u2502 crontab \u2502 \u2514\u2500\u2500\u2500logs \u2502 \u2502 2022-06-02.txt \u2502 \u2514\u2500\u2500\u2500scripts \u2502 database_dump.sh \u2502 send-to-s3.py","title":"Estrutura"},{"location":"projects/de/etl-aws/#dockerfiles","text":"MySQL Client FROM ubuntu:focal RUN apt-get update && apt-get install -y mysql-client ENV TZ = America/Sao_Paulo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone && \\ apt-get update && apt-get install -y tzdata && dpkg-reconfigure -f noninteractive tzdata CMD tail -f /dev/null Python FROM python:3.8-alpine3.14 COPY ./crontab /var/spool/cron/crontabs/root RUN pip install boto3 ENV TZ = America/Sao_Paulo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone CMD crond -l 2 -f && tail -f /var/log/cron.log crontab: * * * * * python /bin/send-to-s3.py","title":"Dockerfiles"},{"location":"projects/de/etl-aws/#docker-compose","text":"version : '3.1' services : database : image : mariadb container_name : database hostname : database environment : - MARIADB_ROOT_PASSWORD=password - MARIADB_DATABASE=FACULDADE volumes : - ./db_data/FACULDADE.sql:/docker-entrypoint-initdb.d/FACULDADE.sql dump : image : mysql-client container_name : dump hostname : dump environment : - MARIADB_ROOT_PASSWORD=password depends_on : - database volumes : - ./dumps/:/dumps/ - ./scripts/database_dump.sh:/database_dump.sh - ./logs/:/logs/ command : - ./database_dump.sh send-to-s3 : image : python-s3 container_name : send-to-s3 hostname : send-to-s3 depends_on : - dump volumes : - ./dumps/:/dumps/ - ../credentials.txt:/credentials.txt - ./scripts/send-to-s3.py:/bin/send-to-s3.py - ./logs/:/logs/","title":"Docker compose"},{"location":"projects/de/etl-aws/#extracao","text":"Script em bash que realiza o dump no banco de dados, salva o arquivo resultante em uma pasta compartilhada e registra opera\u00e7\u00e3o no arquivo de log. #!/bin/bash date = $( date + '%F' ) dateAndTime = $( date + '%F,%H:%M:%S' ) mysqldump -hdatabase -uuser -ppassword FACULDADE > dumps/datadump.sql echo \" ${ dateAndTime } ,database dump completed\" >> logs/ \" ${ date } \" .txt","title":"Extra\u00e7\u00e3o"},{"location":"projects/de/etl-aws/#carga","text":"Quando o script \u00e9 chamado, antes de executar qualquer a\u00e7\u00e3o, \u00e9 verificado se existe algum arquivo a ser enviado. if __name__ == '__main__' : DUMP_FILE = '/dumps/datadump.sql' current_date = datetime . today () . strftime ( \"%Y-%m- %d \" ) current_hour = datetime . today () . strftime ( \"%H:%M:%S\" ) log_file = f \"../logs/ { current_date } .txt\" if os . path . exists ( DUMP_FILE ): main () else : with open ( log_file , 'a+' ) as file : file . write ( f \" { current_date } , { current_hour } ,there is no file to upload \\n \" ) Existindo o arquivo, abre-se uma conex\u00e3o com a AWS e este \u00e9 devidamente carregado em um bucket da AWS S3. def main (): KEY_ID , ACCESS_KEY , BUCKET_NAME = get_credentials ( '../credentials.txt' ) s3 = boto3 . client ( 's3' , region_name = 'us-east-1' , aws_access_key_id = KEY_ID , aws_secret_access_key = ACCESS_KEY ) send_dump_file = AWSobjectWrapper ( s3 ) send_dump_file . create_bucket_if_necessary ( BUCKET_NAME ) send_dump_file . upload_dump_file ( DUMP_FILE ) Abaixo segue o que \u00e9 a classe AWSobjectWrapper e seus m\u00e9todos. class AWSobjectWrapper : def __init__ ( self , s3_object ): self . object = s3_object def create_bucket_if_necessary ( self , s3_bucket ): self . bucket_name = s3_bucket existing_buckets = [ bucket [ 'Name' ] for bucket in self . object . list_buckets ()[ 'Buckets' ]] if self . bucket_name not in existing_buckets : self . object . create_bucket ( Bucket = self . bucket_name ) def count_files_in_folder ( self ): self . files_per_day = 0 existing_files = self . object . list_objects ( Bucket = self . bucket_name ) if 'Contents' in existing_files : self . files_per_day = sum ( 1 for content in existing_files . get ( 'Contents' ) \\ if current_date in content . get ( 'Key' )) def message_log_file ( self , message ): with open ( log_file , 'a+' ) as file : file . write ( f \" { current_date } , { current_hour } , { message } \\n \" ) def upload_dump_file ( self , source_file ): self . count_files_in_folder () try : self . object . upload_file ( Filename = source_file , Bucket = self . bucket_name , Key = f ' { current_date } /datadump { self . files_per_day + 1 } .sql' ) except Exception as e : self . message_log_file ( e ) else : self . message_log_file ( 'file successfully uploaded' ) os . remove ( source_file )","title":"Carga"},{"location":"projects/de/modelagem-bd/","text":"Modelagem de banco de dados e execu\u00e7\u00e3o em docker Atividade proposta durante a participa\u00e7\u00e3o na trilha de Engenharia de Dados do Programa de Forma\u00e7\u00e3o em Dados Encantech, realizado durante os meses de Mar\u00e7o, Abril e Maio de 2022, em uma parceria entre as Lojas Renner S.A. e a CESAR School. Consiste na modelagem de um banco de dados, da cria\u00e7\u00e3o de um gerador de massa e da execu\u00e7\u00e3o em um docker container. acesso aos arquivos Mini mundo Uma faculdade contratou voc\u00ea como engenheiro de dados e pediu seu apoio para a constru\u00e7\u00e3o de um modelo de banco de dados para gerenciar seus funcion\u00e1rios, divididos entre administrativos e professores, permitindo tamb\u00e9m alocar um professor por mat\u00e9ria por semestre, permitindo assim a rota\u00e7\u00e3o entre os professores durante os semestres. O sistema tamb\u00e9m deve registrar os dados de matr\u00edcula dos alunos, o ano letivo e suas mat\u00e9rias que ser\u00e3o cursadas por semestre, vinculando aos professores que ir\u00e3o ministrar aulas nesse per\u00edodo. Os dados referentes aos alunos devem conter sua filia\u00e7\u00e3o, sua data de nascimento, endere\u00e7o completo, telefone para contato, e-mail e seu hist\u00f3rico de notas por mat\u00e9rias. Modelagem O modelo desenvolvido visa garantir que os requisitos do mini mundo sejam respeitados, mas vai al\u00e9m, considerando outras rela\u00e7\u00f5es existentes quando se fala de uma faculdade. Vale considerar que a normaliza\u00e7\u00e3o do banco de dados tamb\u00e9m foi levada em conta. Modelo conceitual: Modelo l\u00f3gico: Modelo f\u00edsico : gerado a partir do MySQL Workbench. Gerador de massa O modelo criado conta com um total de 26 tabelas. Para gerar um script capaz de popular todo o banco de dados, a estrat\u00e9gia foi popular as tabelas seguindo a ordem abaixo, pois como h\u00e1 cruzamentos de informa\u00e7\u00f5es, \u00e9 necess\u00e1rio garantir que n\u00e3o haver\u00e1 problemas de depend\u00eancia. tabelas de dom\u00ednio (exemplos: CIDADE, DEPARTAMENTO); tabelas que dependem apenas das tabelas de dom\u00ednio (exemplo: PESSOA); tabelas de especializa\u00e7\u00f5es (exemplos: ALUNO, PROFESSOR); tabelas de relacionamentos (exemplo: AULA). Para tal, os arquivos de suporte foram: lista de nomes (30 nomes), lista de sobrenomes (434 sobrenomes) e lista de CEPs v\u00e1lidos (732.763 CEPs, contendo rua, n\u00famero, bairro, cidade, estado). A partir desses arquivos, diversas combina\u00e7\u00f5es foram geradas, permitindo a popula\u00e7\u00e3o do banco completo sem replicar registros. As tabelas de dom\u00ednio que precisam de mais informa\u00e7\u00f5es foram definidas manualmente: STATUS_ALUNO: [\"Cursando\", \"Trancado\", \"Formado\", \"Jubilado\"] STATUS_FUNCIONARIO: [\"Ativo\", \"Desligado\", \"F\u00e9rias\", \"Afastado\"] TITULACAO: [\"Graduado\", \"Especialista\", \"Mestre\", \"Doutor\"] SETOR: [\"Secretariado\", \"Coordena\u00e7\u00e3o\", \"Biblioteca\", \"Facilities\"] TURNO: [\"Matutino\", \"Vespertino\", \"Noturno\", \"Integral\"] TIPO_AVALIACAO: [\"Prova 1\", \"Prova 2\", \"Prova 3\", \"Prova de Recupera\u00e7\u00e3o\", \"Prova de Final\"] DEPARTAMENTO: [\"DADOS\", \"EXATAS\", \"BIOLOGICAS\", \"HUMANAS E SOCIAIS\", \"ARTES\", \"LINGUAS\"] CURSO_GRADUACAO: [\"Engenharia de Dados\", \"Ci\u00eancia de Dados\", \"An\u00e1lise de Dados\", \"Engenharias de Machine Learning\", \"Estat\u00edstica\", \"Matem\u00e1tica\", \"F\u00edsica\", \"Qu\u00edmica\", \"Biologia\", \"Medicina\", \"Enfermagem\", \"Fisioterapia\", \"Filosofia\", \"Administra\u00e7\u00e3o\", \"Sociologia\", \"Pedagogia\", \"Artes C\u00eanicas\", \"M\u00fasica\", \"Turismo\", \"Artes Visuais\", \"Portugu\u00eas\", \"Linguas Estrangeiras\", \"Linguas Cl\u00e1ssicas\", \"Filologia\"] O script desenvolvido em python cria o DML em SQL para cada uma das tabelas, abaixo um exemplo do que foi feito para a tabela STATUS_FUNCIONARIO: with open ( \"INSERTS_FACULDADE_DOMINIOS/STATUS_FUNCIONARIO.sql\" , \"w\" , encoding = \"utf-8\" ) as f : ID_STATUS_LIST = [ 1 , 2 , 3 , 4 ] DESCRICAO_STATUS_LIST = [ \"Ativo\" , \"Desligado\" , \"F\u00e9rias\" , \"Afastado\" ] for i in range ( len ( ID_STATUS_LIST )): ID_STATUS = ID_STATUS_LIST [ i ] DESCRICAO_STATUS = DESCRICAO_STATUS_LIST [ i ] sql = f 'INSERT INTO STATUS_FUNCIONARIO (ID_STATUS, DESCRICAO_STATUS) VALUES ( { ID_STATUS } , \" { DESCRICAO_STATUS } \")' f . write ( sql + \";\" + \" \\n \" ) No final, as instru\u00e7\u00f5es de inser\u00e7\u00e3o foram divididas em dois arquivos: popular tabelas de dom\u00ednio e popuplar tabelas fato . Docker Agora com os scripts de DDL e DML em m\u00e3os, o banco de dados ser\u00e1 executado um docker a partir de uma imagem do SGBD MariaDB. version : '3.1' services : database : image : mariadb container_name : mariadb-bd environment : - MARIADB_ROOT_PASSWORD=password Ap\u00f3s subir o container, basta executar os scripts de DDL e DML na ordem abaixo. Com isso, chegamos em uma simula\u00e7\u00e3o de um banco de dados em opera\u00e7\u00e3o. defini\u00e7\u00e3o do banco de dados: scripts_sql/DDL_FACULDADE.sql popular tabelas de dom\u00ednio: scripts_sql/DML_TABELAS_DOMINIO.sql popular tabelas fato: scripts_sql/DML_MASSA.sql","title":"Modelagem de banco de dados"},{"location":"projects/de/modelagem-bd/#modelagem-de-banco-de-dados-e-execucao-em-docker","text":"Atividade proposta durante a participa\u00e7\u00e3o na trilha de Engenharia de Dados do Programa de Forma\u00e7\u00e3o em Dados Encantech, realizado durante os meses de Mar\u00e7o, Abril e Maio de 2022, em uma parceria entre as Lojas Renner S.A. e a CESAR School. Consiste na modelagem de um banco de dados, da cria\u00e7\u00e3o de um gerador de massa e da execu\u00e7\u00e3o em um docker container. acesso aos arquivos","title":"Modelagem de banco de dados e execu\u00e7\u00e3o em docker"},{"location":"projects/de/modelagem-bd/#mini-mundo","text":"Uma faculdade contratou voc\u00ea como engenheiro de dados e pediu seu apoio para a constru\u00e7\u00e3o de um modelo de banco de dados para gerenciar seus funcion\u00e1rios, divididos entre administrativos e professores, permitindo tamb\u00e9m alocar um professor por mat\u00e9ria por semestre, permitindo assim a rota\u00e7\u00e3o entre os professores durante os semestres. O sistema tamb\u00e9m deve registrar os dados de matr\u00edcula dos alunos, o ano letivo e suas mat\u00e9rias que ser\u00e3o cursadas por semestre, vinculando aos professores que ir\u00e3o ministrar aulas nesse per\u00edodo. Os dados referentes aos alunos devem conter sua filia\u00e7\u00e3o, sua data de nascimento, endere\u00e7o completo, telefone para contato, e-mail e seu hist\u00f3rico de notas por mat\u00e9rias.","title":"Mini mundo"},{"location":"projects/de/modelagem-bd/#modelagem","text":"O modelo desenvolvido visa garantir que os requisitos do mini mundo sejam respeitados, mas vai al\u00e9m, considerando outras rela\u00e7\u00f5es existentes quando se fala de uma faculdade. Vale considerar que a normaliza\u00e7\u00e3o do banco de dados tamb\u00e9m foi levada em conta. Modelo conceitual: Modelo l\u00f3gico: Modelo f\u00edsico : gerado a partir do MySQL Workbench.","title":"Modelagem"},{"location":"projects/de/modelagem-bd/#gerador-de-massa","text":"O modelo criado conta com um total de 26 tabelas. Para gerar um script capaz de popular todo o banco de dados, a estrat\u00e9gia foi popular as tabelas seguindo a ordem abaixo, pois como h\u00e1 cruzamentos de informa\u00e7\u00f5es, \u00e9 necess\u00e1rio garantir que n\u00e3o haver\u00e1 problemas de depend\u00eancia. tabelas de dom\u00ednio (exemplos: CIDADE, DEPARTAMENTO); tabelas que dependem apenas das tabelas de dom\u00ednio (exemplo: PESSOA); tabelas de especializa\u00e7\u00f5es (exemplos: ALUNO, PROFESSOR); tabelas de relacionamentos (exemplo: AULA). Para tal, os arquivos de suporte foram: lista de nomes (30 nomes), lista de sobrenomes (434 sobrenomes) e lista de CEPs v\u00e1lidos (732.763 CEPs, contendo rua, n\u00famero, bairro, cidade, estado). A partir desses arquivos, diversas combina\u00e7\u00f5es foram geradas, permitindo a popula\u00e7\u00e3o do banco completo sem replicar registros. As tabelas de dom\u00ednio que precisam de mais informa\u00e7\u00f5es foram definidas manualmente: STATUS_ALUNO: [\"Cursando\", \"Trancado\", \"Formado\", \"Jubilado\"] STATUS_FUNCIONARIO: [\"Ativo\", \"Desligado\", \"F\u00e9rias\", \"Afastado\"] TITULACAO: [\"Graduado\", \"Especialista\", \"Mestre\", \"Doutor\"] SETOR: [\"Secretariado\", \"Coordena\u00e7\u00e3o\", \"Biblioteca\", \"Facilities\"] TURNO: [\"Matutino\", \"Vespertino\", \"Noturno\", \"Integral\"] TIPO_AVALIACAO: [\"Prova 1\", \"Prova 2\", \"Prova 3\", \"Prova de Recupera\u00e7\u00e3o\", \"Prova de Final\"] DEPARTAMENTO: [\"DADOS\", \"EXATAS\", \"BIOLOGICAS\", \"HUMANAS E SOCIAIS\", \"ARTES\", \"LINGUAS\"] CURSO_GRADUACAO: [\"Engenharia de Dados\", \"Ci\u00eancia de Dados\", \"An\u00e1lise de Dados\", \"Engenharias de Machine Learning\", \"Estat\u00edstica\", \"Matem\u00e1tica\", \"F\u00edsica\", \"Qu\u00edmica\", \"Biologia\", \"Medicina\", \"Enfermagem\", \"Fisioterapia\", \"Filosofia\", \"Administra\u00e7\u00e3o\", \"Sociologia\", \"Pedagogia\", \"Artes C\u00eanicas\", \"M\u00fasica\", \"Turismo\", \"Artes Visuais\", \"Portugu\u00eas\", \"Linguas Estrangeiras\", \"Linguas Cl\u00e1ssicas\", \"Filologia\"] O script desenvolvido em python cria o DML em SQL para cada uma das tabelas, abaixo um exemplo do que foi feito para a tabela STATUS_FUNCIONARIO: with open ( \"INSERTS_FACULDADE_DOMINIOS/STATUS_FUNCIONARIO.sql\" , \"w\" , encoding = \"utf-8\" ) as f : ID_STATUS_LIST = [ 1 , 2 , 3 , 4 ] DESCRICAO_STATUS_LIST = [ \"Ativo\" , \"Desligado\" , \"F\u00e9rias\" , \"Afastado\" ] for i in range ( len ( ID_STATUS_LIST )): ID_STATUS = ID_STATUS_LIST [ i ] DESCRICAO_STATUS = DESCRICAO_STATUS_LIST [ i ] sql = f 'INSERT INTO STATUS_FUNCIONARIO (ID_STATUS, DESCRICAO_STATUS) VALUES ( { ID_STATUS } , \" { DESCRICAO_STATUS } \")' f . write ( sql + \";\" + \" \\n \" ) No final, as instru\u00e7\u00f5es de inser\u00e7\u00e3o foram divididas em dois arquivos: popular tabelas de dom\u00ednio e popuplar tabelas fato .","title":"Gerador de massa"},{"location":"projects/de/modelagem-bd/#docker","text":"Agora com os scripts de DDL e DML em m\u00e3os, o banco de dados ser\u00e1 executado um docker a partir de uma imagem do SGBD MariaDB. version : '3.1' services : database : image : mariadb container_name : mariadb-bd environment : - MARIADB_ROOT_PASSWORD=password Ap\u00f3s subir o container, basta executar os scripts de DDL e DML na ordem abaixo. Com isso, chegamos em uma simula\u00e7\u00e3o de um banco de dados em opera\u00e7\u00e3o. defini\u00e7\u00e3o do banco de dados: scripts_sql/DDL_FACULDADE.sql popular tabelas de dom\u00ednio: scripts_sql/DML_TABELAS_DOMINIO.sql popular tabelas fato: scripts_sql/DML_MASSA.sql","title":"Docker"},{"location":"projects/de/scraping-jobs/","text":"Raspagem de dados do LinkedIn Requisitos para rodar a aplica\u00e7\u00e3o: arquivo com credenciais para acessar o Google Sheets e ter o Docker instalado. acesso aos arquivos Overview Periodicamente extrair do LinkedIn as vagas abertas de Data Engineer no Brasil e salvar os detalhes de cada uma delas em uma planilha do Google Sheets. O script em shell extrai periodicamente as vagas abertas e salva em uma pasta compartilhada um arquivo .txt com o link de cada uma delas; O script em python: periodicamente (n\u00e3o sincronizado com o script em shell) verifica os arquivos presentes na pasta compartilhada; para cada arquivo encontrado, varre todos os links existentes; em cada link de vaga de emprego extrai-se: o t\u00edtulo da vaga, a empresa que est\u00e1 ofertando, o local de trabalho e a descri\u00e7\u00e3o; todas essas informa\u00e7\u00f5es s\u00e3o salvas em uma planilha do Google Sheets, onde cada aba corresponde a um arquivo; ap\u00f3s finalizar a carga na planilha, o arquivo com os links das vagas \u00e9 movido para uma subpasta da pasta compartilhada, de modo a manter o registro de todas as extra\u00e7\u00f5es realizadas e salvas. Resumo do funcionamento: Mais detalhes Automa\u00e7\u00e3o da aplica\u00e7\u00e3o a aplica\u00e7\u00e3o \u00e9 executada com um cron job e, para efeito de demonstra\u00e7\u00e3o, foram escolhidos intervalos de 2 e 11 minutos para os scripts em shell e em python, respectivamente; exist\u00eancia de uma aba de consolida\u00e7\u00e3o, que guarda um resumo dos dados de todas as outras abas existentes. Planilha final Abaixo \u00e9 a captura da tela assim que a planilha \u00e9 aberta. Agora, um exemplo do conte\u00fado presente nas abas referentes a cada arquivo da pasta compartilhada. \u00c9 interessante pontuar que o resultado obtido ao realizar a coleta de dados na pesquisa de vagas no LinkedIn conta sempre com 25 vagas de emprego. Quando voc\u00ea est\u00e1 conectado a uma conta do LinkedIn, o conte\u00fado \u00e9 apresentado em diversas p\u00e1ginas de 25 vagas, o que permitiria a itera\u00e7\u00e3o via c\u00f3digo ao mudar um par\u00e2metro presente no URL da pesquisa do LinkedIn. Por\u00e9m, da forma que est\u00e1 sendo feita a aplica\u00e7\u00e3o, n\u00e3o \u00e9 realizado nenhum login antes da coleta de dados. Por conta disso, a p\u00e1gina do LinkedIn \u00e9 mostrada de maneira diferente, n\u00e3o por p\u00e1ginas contendo 25 vagas de emprego, mas sim com uma rolagem (scroll) infinita que apresenta mais 25 novas vagas assim que carregada. Dessa forma, para conseguir extrair mais de 25 vagas por vez, se faz necess\u00e1rio o acr\u00e9scimo de um algoritmo que interaja com a p\u00e1gina, o que, considerando o objetivo deste projeto, n\u00e3o foi realizado aqui. Por fim, \u00e9 poss\u00edvel observar todas as abas existentes na planilha. \u00c9 interessante observar a quest\u00e3o da periodicidade de execu\u00e7\u00e3o dos scripts em shell e em python e tamb\u00e9m checar como os arquivos est\u00e3o organizados na pasta \"job-links/\". O script em shell (extrair links das vagas) executa diversas vezes antes do script em python (extrair informa\u00e7\u00f5es de cada link e salvar na planilha), por isso, as abas presentes na planilha correspondem aos arquivos presentes na pasta \"job-links/sent/\". Ainda h\u00e1 outros arquivos na pasta \"job-links/\", mas a aplica\u00e7\u00e3o foi parada antes do script em python executar novamente. Filtragem do conte\u00fado HTML P\u00e1gina de vagas de emprego do LinkedIn de acordo com os filtros de t\u00edtulo e local de trabalho; No resultado da coleta de dados da p\u00e1gina que lista todas as vagas abertas, \u00e9 poss\u00edvel observar que cada uma das 25 vagas s\u00e3o identificadas por um ID, acess\u00edvel em \"data-entity-urn\"; Ap\u00f3s extrair os IDs das vagas mostradas no momento do acesso, \u00e9 poss\u00edvel acessar cada uma delas a partir do URL apresentado na imagem. Por conta disso, o arquivo de texto criado conta com o URL completo para cada uma das 25 vagas, onde ser\u00e1 poss\u00edvel extrair: o t\u00edtulo da vaga, a empresa que est\u00e1 ofertando, o local de trabalho e a descri\u00e7\u00e3o; Investigando o resultado da coleta de dados da p\u00e1gina espec\u00edfica de uma vaga de emprego, s\u00e3o observadas as seguintes rela\u00e7\u00f5es com o que se deseja extrair. Com isso, \u00e9 implementada a extra\u00e7\u00e3o de informa\u00e7\u00f5es a partir dessas classes, que depois s\u00e3o armazenadas e enviadas para a planilha do Google Sheets. Scripts Estrutura arquivo \"docker-compose.yml\": para funcionamento da aplica\u00e7\u00e3o, basta subir o container presente neste arquivo. Nele constam a cria\u00e7\u00e3o da imagem e o mapeamento dos volumes; pasta \"data\": armazena o arquivo JSON com as credenciais que autorizam o acesso \u00e0 planilha; pasta \"docker-image\": armazena o arquivo do Dockerfile para cria\u00e7\u00e3o da imagem e o arquivo crontab que agenda a execu\u00e7\u00e3o dos scripts; pasta \"scripts\": armazena os scripts em shell e em python; pasta \"job-links\": armazena os arquivos resultantes das coletas de dados do LinkedIn. Dockerfile FROM python:3.8-alpine3.14 COPY ./crontab /var/spool/cron/crontabs/root RUN apk --no-cache add curl grep sed dos2unix RUN dos2unix /var/spool/cron/crontabs/root RUN pip install gspread oauth2client beautifulsoup4 CMD crond -l 2 -f && tail -f /var/log/cron.log crontab: */11 * * * * python /bin/etl-script.py */2 * * * * sh /bin/get-job-links.sh Docker compose version : '3.1' services : google-api : build : ./docker-image image : python container_name : google-api volumes : - ./data/client_secrets.json:/client_secrets.json - ./job-links/:/job-links - ./scripts/etl-script.py:/bin/etl-script.py - ./scripts/get-job-links.sh:/bin/get-job-links.sh Extrair links das vagas #!/bin/sh JOB_LINK = \"https\\:\\/\\/www.linkedin.com\\/jobs\\/view\\/\" JOB_TITLE = \"data%20engineer\" LOCATION = \"brazil\" URL = \"https://www.linkedin.com/jobs/search?keywords= ${ JOB_TITLE } &location= ${ LOCATION } \" dateAndTime = $( date -u + '%F %H-%M-%S %Z' ) OUTPUT_FILE = \"../job-links/ $dateAndTime .txt\" curl \" $URL \" | grep data-entity-urn | sed \"s/^.*urn:li:jobPosting:/ $JOB_LINK /\" | sed \"s/\\\" data-search-id.* $ //\" >> \" $OUTPUT_FILE \" Resultados da execu\u00e7\u00e3o do script Extrair informa\u00e7\u00f5es de cada link e salvar na planilha Fun\u00e7\u00e3o principal que l\u00ea os arquivos que cont\u00eam os links das vagas de emprego, extrai o conte\u00fado de cada uma delas e salva em uma planilha do Google Sheets. def main (): CREDENTIALS_FILE = \"../client_secrets.json\" SPREADSHEET_KEY = \"1KSsI8nJFyp_vYyaKFBZFJ-w-wZqou85dAdIj9E6a3zI\" JOB_LINKS_PATH = \"../job-links/\" SAVED_JOB_LINKS_PATH = JOB_LINKS_PATH + \"sent/\" spreadsheet = Spreadsheet ( CREDENTIALS_FILE , SPREADSHEET_KEY ) files_with_job_links = [ f for f in os . listdir ( JOB_LINKS_PATH ) if os . path . isfile ( os . path . join ( JOB_LINKS_PATH , f ))] for urls_file in files_with_job_links : job_data = [] for url in file_lines_into_list ( JOB_LINKS_PATH + urls_file ): job_data . append ( get_url_content_as_dict ( url )) data_keys = sorted ( get_all_keys_as_list ( job_data )) # nome da aba \u00e9 o carimbo data e hora da extra\u00e7\u00e3o dos links das vagas # = nome do arquivo sem a sua extens\u00e3o worksheet_name = urls_file . split ( '.' )[ 0 ] spreadsheet . insert_values ( job_data , data_keys , worksheet_name ) spreadsheet . consolidation () if not os . path . exists ( SAVED_JOB_LINKS_PATH ): os . makedirs ( SAVED_JOB_LINKS_PATH ) os . replace ( JOB_LINKS_PATH + urls_file , SAVED_JOB_LINKS_PATH + urls_file ) Fun\u00e7\u00f5es utilizadas para ler os arquivos que cont\u00eam os links das vagas, extrair e organizar o conte\u00fado html das p\u00e1ginas. def file_lines_into_list ( file ): with open ( file , 'r' ) as f : lines = f . read () . splitlines () return lines def filter_html_content ( html_content , filter ): try : result = html_content . find ( True , { \"class\" : filter }) . text . strip () except : result = \"\" return result def get_url_content_as_dict ( url ): content = {} page = requests . get ( url ) soup = BeautifulSoup ( page . content , \"html.parser\" ) content [ \"url\" ] = url content [ 'job title' ] = filter_html_content ( soup , \"sub-nav-cta__header\" ) content [ 'company' ] = filter_html_content ( soup , \"sub-nav-cta__optional-url\" ) content [ 'location' ] = filter_html_content ( soup , \"sub-nav-cta__meta-text\" ) content [ 'description' ] = filter_html_content ( soup , \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\" ) return content def get_all_keys_as_list ( list_of_dictionaries ): # retorna todas as chaves presentes em uma lista de dicion\u00e1rios all_keys = list ( set () . union ( * ( dict . keys () for dict in list_of_dictionaries ) ) ) return all_keys Abaixo segue o que \u00e9 a classe Spreadsheet e seus m\u00e9todos. class Spreadsheet (): SCOPE = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] CONSOLIDATION_WORKSHEET = \"DATA CONSOLIDATION\" HEADER_INDEX = 1 HEADER_FIRST_CELL = CONSOLIDATION_WORKSHEET + \"!A1\" def __init__ ( self , credentials , spreadsheet_key ): # a inicializa\u00e7\u00e3o consiste em se conectar com a referente planilha e deix\u00e1-la dispon\u00edvel para trabalho self . spreadsheet_key = spreadsheet_key self . creds = ServiceAccountCredentials . from_json_keyfile_name ( credentials , Spreadsheet . SCOPE ) self . client = gspread . authorize ( self . creds ) self . spreadsheet = self . client . open_by_key ( self . spreadsheet_key ) def _create_worksheet ( self , worksheet_name ): return self . spreadsheet . add_worksheet ( title = worksheet_name , rows = 1000 , cols = 20 ) def list_worksheets ( self ): worksheet_objs = self . spreadsheet . worksheets () worksheets_list = [ worksheet . title for worksheet in worksheet_objs ] return worksheets_list def _arrange_values_for_insertion ( self , data , column_header ): list_values_to_insert = [] # mapeamento cabe\u00e7alho e chave header_to_key = { i : i for i in column_header } # organizar os dados de acordo com o cabe\u00e7alho # caso um registro n\u00e3o tenha determinada chave, o valor referente ficar\u00e1 em branco for instance in data : arrange_key_to_header = [] for column in column_header : try : arrange_key_to_header . append ( instance [ header_to_key [ column ] ] ) except : arrange_key_to_header . append ( \"\" ) list_values_to_insert . append ( arrange_key_to_header ) return list_values_to_insert def insert_values ( self , data , column_header , worksheet_name ): self . data = data self . last_header = column_header if worksheet_name not in self . list_worksheets (): self . _create_worksheet ( worksheet_name ) self . current_worksheet = self . spreadsheet . worksheet ( worksheet_name ) # insere cabe\u00e7alho self . current_worksheet . insert_row ( self . last_header , Spreadsheet . HEADER_INDEX ) # anexa os dados alinhando conte\u00fado e cabe\u00e7alho list_values_to_insert = self . _arrange_values_for_insertion ( self . data , self . last_header ) self . spreadsheet . values_append ( worksheet_name , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : list_values_to_insert }) def consolidation ( self ): if Spreadsheet . CONSOLIDATION_WORKSHEET not in self . list_worksheets (): self . _create_worksheet ( Spreadsheet . CONSOLIDATION_WORKSHEET ) self . data_consolidation_worksheet = self . spreadsheet . worksheet ( Spreadsheet . CONSOLIDATION_WORKSHEET ) # insere cabe\u00e7alho considerando o j\u00e1 existente na aba DATA CONSOLIDATION, # al\u00e9m de poss\u00edveis novas colunas da \u00faltima aba criada self . current_header = self . data_consolidation_worksheet . row_values ( Spreadsheet . HEADER_INDEX ) self . current_header . extend ([ i for i in self . last_header if i not in self . current_header ]) self . spreadsheet . values_update ( Spreadsheet . HEADER_FIRST_CELL , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : [ self . current_header ]}) # lista com todos os urls presentes na aba DATA CONSOLIDATION key = \"url\" key_cell = self . data_consolidation_worksheet . find ( key ) values_in_data_consolidation = self . data_consolidation_worksheet . col_values ( key_cell . col )[ 1 :] # urls da \u00faltima aba criada que ainda n\u00e3o constam na aba \"DATA CONSOLIDATION\" new_rows = [ instance for instance in self . data if instance . get ( key ) not in values_in_data_consolidation ] # anexa os dados alinhando conte\u00fado e cabe\u00e7alho list_values_to_insert = self . _arrange_values_for_insertion ( new_rows , self . current_header ) self . spreadsheet . values_append ( Spreadsheet . CONSOLIDATION_WORKSHEET , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : list_values_to_insert })","title":"Raspagem de dados do LinkedIn"},{"location":"projects/de/scraping-jobs/#raspagem-de-dados-do-linkedin","text":"Requisitos para rodar a aplica\u00e7\u00e3o: arquivo com credenciais para acessar o Google Sheets e ter o Docker instalado. acesso aos arquivos","title":"Raspagem de dados do LinkedIn"},{"location":"projects/de/scraping-jobs/#overview","text":"Periodicamente extrair do LinkedIn as vagas abertas de Data Engineer no Brasil e salvar os detalhes de cada uma delas em uma planilha do Google Sheets. O script em shell extrai periodicamente as vagas abertas e salva em uma pasta compartilhada um arquivo .txt com o link de cada uma delas; O script em python: periodicamente (n\u00e3o sincronizado com o script em shell) verifica os arquivos presentes na pasta compartilhada; para cada arquivo encontrado, varre todos os links existentes; em cada link de vaga de emprego extrai-se: o t\u00edtulo da vaga, a empresa que est\u00e1 ofertando, o local de trabalho e a descri\u00e7\u00e3o; todas essas informa\u00e7\u00f5es s\u00e3o salvas em uma planilha do Google Sheets, onde cada aba corresponde a um arquivo; ap\u00f3s finalizar a carga na planilha, o arquivo com os links das vagas \u00e9 movido para uma subpasta da pasta compartilhada, de modo a manter o registro de todas as extra\u00e7\u00f5es realizadas e salvas. Resumo do funcionamento:","title":"Overview"},{"location":"projects/de/scraping-jobs/#mais-detalhes","text":"","title":"Mais detalhes"},{"location":"projects/de/scraping-jobs/#automacao-da-aplicacao","text":"a aplica\u00e7\u00e3o \u00e9 executada com um cron job e, para efeito de demonstra\u00e7\u00e3o, foram escolhidos intervalos de 2 e 11 minutos para os scripts em shell e em python, respectivamente; exist\u00eancia de uma aba de consolida\u00e7\u00e3o, que guarda um resumo dos dados de todas as outras abas existentes.","title":"Automa\u00e7\u00e3o da aplica\u00e7\u00e3o"},{"location":"projects/de/scraping-jobs/#planilha-final","text":"Abaixo \u00e9 a captura da tela assim que a planilha \u00e9 aberta. Agora, um exemplo do conte\u00fado presente nas abas referentes a cada arquivo da pasta compartilhada. \u00c9 interessante pontuar que o resultado obtido ao realizar a coleta de dados na pesquisa de vagas no LinkedIn conta sempre com 25 vagas de emprego. Quando voc\u00ea est\u00e1 conectado a uma conta do LinkedIn, o conte\u00fado \u00e9 apresentado em diversas p\u00e1ginas de 25 vagas, o que permitiria a itera\u00e7\u00e3o via c\u00f3digo ao mudar um par\u00e2metro presente no URL da pesquisa do LinkedIn. Por\u00e9m, da forma que est\u00e1 sendo feita a aplica\u00e7\u00e3o, n\u00e3o \u00e9 realizado nenhum login antes da coleta de dados. Por conta disso, a p\u00e1gina do LinkedIn \u00e9 mostrada de maneira diferente, n\u00e3o por p\u00e1ginas contendo 25 vagas de emprego, mas sim com uma rolagem (scroll) infinita que apresenta mais 25 novas vagas assim que carregada. Dessa forma, para conseguir extrair mais de 25 vagas por vez, se faz necess\u00e1rio o acr\u00e9scimo de um algoritmo que interaja com a p\u00e1gina, o que, considerando o objetivo deste projeto, n\u00e3o foi realizado aqui. Por fim, \u00e9 poss\u00edvel observar todas as abas existentes na planilha. \u00c9 interessante observar a quest\u00e3o da periodicidade de execu\u00e7\u00e3o dos scripts em shell e em python e tamb\u00e9m checar como os arquivos est\u00e3o organizados na pasta \"job-links/\". O script em shell (extrair links das vagas) executa diversas vezes antes do script em python (extrair informa\u00e7\u00f5es de cada link e salvar na planilha), por isso, as abas presentes na planilha correspondem aos arquivos presentes na pasta \"job-links/sent/\". Ainda h\u00e1 outros arquivos na pasta \"job-links/\", mas a aplica\u00e7\u00e3o foi parada antes do script em python executar novamente.","title":"Planilha final"},{"location":"projects/de/scraping-jobs/#filtragem-do-conteudo-html","text":"P\u00e1gina de vagas de emprego do LinkedIn de acordo com os filtros de t\u00edtulo e local de trabalho; No resultado da coleta de dados da p\u00e1gina que lista todas as vagas abertas, \u00e9 poss\u00edvel observar que cada uma das 25 vagas s\u00e3o identificadas por um ID, acess\u00edvel em \"data-entity-urn\"; Ap\u00f3s extrair os IDs das vagas mostradas no momento do acesso, \u00e9 poss\u00edvel acessar cada uma delas a partir do URL apresentado na imagem. Por conta disso, o arquivo de texto criado conta com o URL completo para cada uma das 25 vagas, onde ser\u00e1 poss\u00edvel extrair: o t\u00edtulo da vaga, a empresa que est\u00e1 ofertando, o local de trabalho e a descri\u00e7\u00e3o; Investigando o resultado da coleta de dados da p\u00e1gina espec\u00edfica de uma vaga de emprego, s\u00e3o observadas as seguintes rela\u00e7\u00f5es com o que se deseja extrair. Com isso, \u00e9 implementada a extra\u00e7\u00e3o de informa\u00e7\u00f5es a partir dessas classes, que depois s\u00e3o armazenadas e enviadas para a planilha do Google Sheets.","title":"Filtragem do conte\u00fado HTML"},{"location":"projects/de/scraping-jobs/#scripts","text":"","title":"Scripts"},{"location":"projects/de/scraping-jobs/#estrutura","text":"arquivo \"docker-compose.yml\": para funcionamento da aplica\u00e7\u00e3o, basta subir o container presente neste arquivo. Nele constam a cria\u00e7\u00e3o da imagem e o mapeamento dos volumes; pasta \"data\": armazena o arquivo JSON com as credenciais que autorizam o acesso \u00e0 planilha; pasta \"docker-image\": armazena o arquivo do Dockerfile para cria\u00e7\u00e3o da imagem e o arquivo crontab que agenda a execu\u00e7\u00e3o dos scripts; pasta \"scripts\": armazena os scripts em shell e em python; pasta \"job-links\": armazena os arquivos resultantes das coletas de dados do LinkedIn.","title":"Estrutura"},{"location":"projects/de/scraping-jobs/#dockerfile","text":"FROM python:3.8-alpine3.14 COPY ./crontab /var/spool/cron/crontabs/root RUN apk --no-cache add curl grep sed dos2unix RUN dos2unix /var/spool/cron/crontabs/root RUN pip install gspread oauth2client beautifulsoup4 CMD crond -l 2 -f && tail -f /var/log/cron.log crontab: */11 * * * * python /bin/etl-script.py */2 * * * * sh /bin/get-job-links.sh","title":"Dockerfile"},{"location":"projects/de/scraping-jobs/#docker-compose","text":"version : '3.1' services : google-api : build : ./docker-image image : python container_name : google-api volumes : - ./data/client_secrets.json:/client_secrets.json - ./job-links/:/job-links - ./scripts/etl-script.py:/bin/etl-script.py - ./scripts/get-job-links.sh:/bin/get-job-links.sh","title":"Docker compose"},{"location":"projects/de/scraping-jobs/#extrair-links-das-vagas","text":"#!/bin/sh JOB_LINK = \"https\\:\\/\\/www.linkedin.com\\/jobs\\/view\\/\" JOB_TITLE = \"data%20engineer\" LOCATION = \"brazil\" URL = \"https://www.linkedin.com/jobs/search?keywords= ${ JOB_TITLE } &location= ${ LOCATION } \" dateAndTime = $( date -u + '%F %H-%M-%S %Z' ) OUTPUT_FILE = \"../job-links/ $dateAndTime .txt\" curl \" $URL \" | grep data-entity-urn | sed \"s/^.*urn:li:jobPosting:/ $JOB_LINK /\" | sed \"s/\\\" data-search-id.* $ //\" >> \" $OUTPUT_FILE \" Resultados da execu\u00e7\u00e3o do script","title":"Extrair links das vagas"},{"location":"projects/de/scraping-jobs/#extrair-informacoes-de-cada-link-e-salvar-na-planilha","text":"Fun\u00e7\u00e3o principal que l\u00ea os arquivos que cont\u00eam os links das vagas de emprego, extrai o conte\u00fado de cada uma delas e salva em uma planilha do Google Sheets. def main (): CREDENTIALS_FILE = \"../client_secrets.json\" SPREADSHEET_KEY = \"1KSsI8nJFyp_vYyaKFBZFJ-w-wZqou85dAdIj9E6a3zI\" JOB_LINKS_PATH = \"../job-links/\" SAVED_JOB_LINKS_PATH = JOB_LINKS_PATH + \"sent/\" spreadsheet = Spreadsheet ( CREDENTIALS_FILE , SPREADSHEET_KEY ) files_with_job_links = [ f for f in os . listdir ( JOB_LINKS_PATH ) if os . path . isfile ( os . path . join ( JOB_LINKS_PATH , f ))] for urls_file in files_with_job_links : job_data = [] for url in file_lines_into_list ( JOB_LINKS_PATH + urls_file ): job_data . append ( get_url_content_as_dict ( url )) data_keys = sorted ( get_all_keys_as_list ( job_data )) # nome da aba \u00e9 o carimbo data e hora da extra\u00e7\u00e3o dos links das vagas # = nome do arquivo sem a sua extens\u00e3o worksheet_name = urls_file . split ( '.' )[ 0 ] spreadsheet . insert_values ( job_data , data_keys , worksheet_name ) spreadsheet . consolidation () if not os . path . exists ( SAVED_JOB_LINKS_PATH ): os . makedirs ( SAVED_JOB_LINKS_PATH ) os . replace ( JOB_LINKS_PATH + urls_file , SAVED_JOB_LINKS_PATH + urls_file ) Fun\u00e7\u00f5es utilizadas para ler os arquivos que cont\u00eam os links das vagas, extrair e organizar o conte\u00fado html das p\u00e1ginas. def file_lines_into_list ( file ): with open ( file , 'r' ) as f : lines = f . read () . splitlines () return lines def filter_html_content ( html_content , filter ): try : result = html_content . find ( True , { \"class\" : filter }) . text . strip () except : result = \"\" return result def get_url_content_as_dict ( url ): content = {} page = requests . get ( url ) soup = BeautifulSoup ( page . content , \"html.parser\" ) content [ \"url\" ] = url content [ 'job title' ] = filter_html_content ( soup , \"sub-nav-cta__header\" ) content [ 'company' ] = filter_html_content ( soup , \"sub-nav-cta__optional-url\" ) content [ 'location' ] = filter_html_content ( soup , \"sub-nav-cta__meta-text\" ) content [ 'description' ] = filter_html_content ( soup , \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\" ) return content def get_all_keys_as_list ( list_of_dictionaries ): # retorna todas as chaves presentes em uma lista de dicion\u00e1rios all_keys = list ( set () . union ( * ( dict . keys () for dict in list_of_dictionaries ) ) ) return all_keys Abaixo segue o que \u00e9 a classe Spreadsheet e seus m\u00e9todos. class Spreadsheet (): SCOPE = [ 'https://spreadsheets.google.com/feeds' , 'https://www.googleapis.com/auth/drive' ] CONSOLIDATION_WORKSHEET = \"DATA CONSOLIDATION\" HEADER_INDEX = 1 HEADER_FIRST_CELL = CONSOLIDATION_WORKSHEET + \"!A1\" def __init__ ( self , credentials , spreadsheet_key ): # a inicializa\u00e7\u00e3o consiste em se conectar com a referente planilha e deix\u00e1-la dispon\u00edvel para trabalho self . spreadsheet_key = spreadsheet_key self . creds = ServiceAccountCredentials . from_json_keyfile_name ( credentials , Spreadsheet . SCOPE ) self . client = gspread . authorize ( self . creds ) self . spreadsheet = self . client . open_by_key ( self . spreadsheet_key ) def _create_worksheet ( self , worksheet_name ): return self . spreadsheet . add_worksheet ( title = worksheet_name , rows = 1000 , cols = 20 ) def list_worksheets ( self ): worksheet_objs = self . spreadsheet . worksheets () worksheets_list = [ worksheet . title for worksheet in worksheet_objs ] return worksheets_list def _arrange_values_for_insertion ( self , data , column_header ): list_values_to_insert = [] # mapeamento cabe\u00e7alho e chave header_to_key = { i : i for i in column_header } # organizar os dados de acordo com o cabe\u00e7alho # caso um registro n\u00e3o tenha determinada chave, o valor referente ficar\u00e1 em branco for instance in data : arrange_key_to_header = [] for column in column_header : try : arrange_key_to_header . append ( instance [ header_to_key [ column ] ] ) except : arrange_key_to_header . append ( \"\" ) list_values_to_insert . append ( arrange_key_to_header ) return list_values_to_insert def insert_values ( self , data , column_header , worksheet_name ): self . data = data self . last_header = column_header if worksheet_name not in self . list_worksheets (): self . _create_worksheet ( worksheet_name ) self . current_worksheet = self . spreadsheet . worksheet ( worksheet_name ) # insere cabe\u00e7alho self . current_worksheet . insert_row ( self . last_header , Spreadsheet . HEADER_INDEX ) # anexa os dados alinhando conte\u00fado e cabe\u00e7alho list_values_to_insert = self . _arrange_values_for_insertion ( self . data , self . last_header ) self . spreadsheet . values_append ( worksheet_name , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : list_values_to_insert }) def consolidation ( self ): if Spreadsheet . CONSOLIDATION_WORKSHEET not in self . list_worksheets (): self . _create_worksheet ( Spreadsheet . CONSOLIDATION_WORKSHEET ) self . data_consolidation_worksheet = self . spreadsheet . worksheet ( Spreadsheet . CONSOLIDATION_WORKSHEET ) # insere cabe\u00e7alho considerando o j\u00e1 existente na aba DATA CONSOLIDATION, # al\u00e9m de poss\u00edveis novas colunas da \u00faltima aba criada self . current_header = self . data_consolidation_worksheet . row_values ( Spreadsheet . HEADER_INDEX ) self . current_header . extend ([ i for i in self . last_header if i not in self . current_header ]) self . spreadsheet . values_update ( Spreadsheet . HEADER_FIRST_CELL , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : [ self . current_header ]}) # lista com todos os urls presentes na aba DATA CONSOLIDATION key = \"url\" key_cell = self . data_consolidation_worksheet . find ( key ) values_in_data_consolidation = self . data_consolidation_worksheet . col_values ( key_cell . col )[ 1 :] # urls da \u00faltima aba criada que ainda n\u00e3o constam na aba \"DATA CONSOLIDATION\" new_rows = [ instance for instance in self . data if instance . get ( key ) not in values_in_data_consolidation ] # anexa os dados alinhando conte\u00fado e cabe\u00e7alho list_values_to_insert = self . _arrange_values_for_insertion ( new_rows , self . current_header ) self . spreadsheet . values_append ( Spreadsheet . CONSOLIDATION_WORKSHEET , { 'valueInputOption' : 'USER_ENTERED' }, { 'values' : list_values_to_insert })","title":"Extrair informa\u00e7\u00f5es de cada link e salvar na planilha"},{"location":"projects/ds/fraud-detection/","text":"Detec\u00e7\u00e3o de fraude em seguro de ve\u00edculo Sobre Objetivo Detectar e evitar processos fraudulentos \u00e9 um enorme desafio. Para tal, o uso de t\u00e9cnicas de machine learning tem se mostrado bastante promissor. Por\u00e9m, um bom desempenho de um modelo de machine learning depende de uma etapa que n\u00e3o \u00e9 simples: entender e tratar os diversos dados de diferentes naturezas que est\u00e3o em posse das seguradoras. E este \u00e9 o foco principal do projeto: entender cada vari\u00e1vel disponibilizada na base de dados e trat\u00e1-las adequadamente. Dessa forma, \u00e9 poss\u00edvel utiliz\u00e1-las no desenvolvimento de algoritmos de classifica\u00e7\u00e3o. O que tamb\u00e9m ser\u00e1 realizado, escolhendo alguns m\u00e9todos existentes e avaliando diferentes configura\u00e7\u00f5es destes. Base de dados A base de dados escolhida cont\u00e9m diversas informa\u00e7\u00f5es sobre pedidos de indeniza\u00e7\u00e3o de uma seguradora de ve\u00edculos. As informa\u00e7\u00f5es se referem \u00e0 pessoa envolvida em um acidente, \u00e0 ap\u00f3lice contratada, ao titular da ap\u00f3lice, ao ve\u00edculo envolvido, etc. Originalmente s\u00e3o 15420 registros, onde o atributo 'FraudFound_P' indica se uma determinada solicita\u00e7\u00e3o foi ou n\u00e3o identificada como fraudulenta. Dispon\u00edvel em: https://www.kaggle.com/datasets/shivamb/vehicle-claim-fraud-detection PARTE 1: Importar bibliotecas import numpy as np # realizar c\u00e1lculos em arrays multidimensionais import pandas as pd # manipula\u00e7\u00e3o e an\u00e1lise de dados import seaborn as sns # visualiza\u00e7\u00e3o de gr\u00e1ficos estat\u00edsticos import scipy.stats as stats # import matplotlib.pyplot as plt # cria\u00e7\u00e3o de gr\u00e1ficos e visualiza\u00e7\u00f5es de dados em geral from imblearn.over_sampling import ADASYN # m\u00e9todo de balanceamento ADASYN from sklearn.model_selection import train_test_split # dividir a base de dados em treino e teste from sklearn.ensemble import RandomForestClassifier # m\u00e9todo de classifica\u00e7\u00e3o Random Forest from sklearn.model_selection import RepeatedStratifiedKFold # m\u00e9todo de valida\u00e7\u00e3o cruzada from sklearn.model_selection import GridSearchCV # testar diferentes valores de par\u00e2metros para um m\u00e9todo from sklearn.inspection import permutation_importance # visualizar relav\u00e2ncia dos atributos em um m\u00e9todo from sklearn import metrics # importar m\u00e9tricas de avalia\u00e7\u00e3o # a sa\u00edda dos comandos de plotagem \u00e9 exibida diretamente abaixo da c\u00e9lula % matplotlib inline PARTE 2: Importar base de dados url_dataset = \"https://github.com/peuvitor/insurance-fraud-detection/blob/main/dataset/fraud_oracle.csv?raw=true\" df_dataset = pd . read_csv ( url_dataset ) PARTE 3: Entendimento da base de dados 3.1. Investigar as colunas existentes Quantas e quais s\u00e3o as colunas? Quais s\u00e3o os seus respectivos tipos? df_dataset . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 15420 entries, 0 to 15419 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Month 15420 non-null object 1 WeekOfMonth 15420 non-null int64 2 DayOfWeek 15420 non-null object 3 Make 15420 non-null object 4 AccidentArea 15420 non-null object 5 DayOfWeekClaimed 15420 non-null object 6 MonthClaimed 15420 non-null object 7 WeekOfMonthClaimed 15420 non-null int64 8 Sex 15420 non-null object 9 MaritalStatus 15420 non-null object 10 Age 15420 non-null int64 11 Fault 15420 non-null object 12 PolicyType 15420 non-null object 13 VehicleCategory 15420 non-null object 14 VehiclePrice 15420 non-null object 15 FraudFound_P 15420 non-null int64 16 PolicyNumber 15420 non-null int64 17 RepNumber 15420 non-null int64 18 Deductible 15420 non-null int64 19 DriverRating 15420 non-null int64 20 Days_Policy_Accident 15420 non-null object 21 Days_Policy_Claim 15420 non-null object 22 PastNumberOfClaims 15420 non-null object 23 AgeOfVehicle 15420 non-null object 24 AgeOfPolicyHolder 15420 non-null object 25 PoliceReportFiled 15420 non-null object 26 WitnessPresent 15420 non-null object 27 AgentType 15420 non-null object 28 NumberOfSuppliments 15420 non-null object 29 AddressChange_Claim 15420 non-null object 30 NumberOfCars 15420 non-null object 31 Year 15420 non-null int64 32 BasePolicy 15420 non-null object dtypes: int64(9), object(24) memory usage: 3.9+ MB 3.2. Valores \u00fanicos de cada coluna Verificar quais os poss\u00edveis valores encontrados na base de dados para cada atributo. Al\u00e9m disso, essa parte ser\u00e1 \u00fatil para identificar poss\u00edveis valores faltantes ou incomuns. for coluna in df_dataset . columns : valores_unicos = df_dataset [ coluna ] . unique () print ( f \"--> ' { coluna } ' possui { len ( valores_unicos ) } valores \u00fanicos, s\u00e3o eles: \\n { valores_unicos } \" ) --> 'Month' possui 12 valores \u00fanicos, s\u00e3o eles: ['Dec' 'Jan' 'Oct' 'Jun' 'Feb' 'Nov' 'Apr' 'Mar' 'Aug' 'Jul' 'May' 'Sep'] --> 'WeekOfMonth' possui 5 valores \u00fanicos, s\u00e3o eles: [5 3 2 4 1] --> 'DayOfWeek' possui 7 valores \u00fanicos, s\u00e3o eles: ['Wednesday' 'Friday' 'Saturday' 'Monday' 'Tuesday' 'Sunday' 'Thursday'] --> 'Make' possui 19 valores \u00fanicos, s\u00e3o eles: ['Honda' 'Toyota' 'Ford' 'Mazda' 'Chevrolet' 'Pontiac' 'Accura' 'Dodge' 'Mercury' 'Jaguar' 'Nisson' 'VW' 'Saab' 'Saturn' 'Porche' 'BMW' 'Mecedes' 'Ferrari' 'Lexus'] --> 'AccidentArea' possui 2 valores \u00fanicos, s\u00e3o eles: ['Urban' 'Rural'] --> 'DayOfWeekClaimed' possui 8 valores \u00fanicos, s\u00e3o eles: ['Tuesday' 'Monday' 'Thursday' 'Friday' 'Wednesday' 'Saturday' 'Sunday' '0'] --> 'MonthClaimed' possui 13 valores \u00fanicos, s\u00e3o eles: ['Jan' 'Nov' 'Jul' 'Feb' 'Mar' 'Dec' 'Apr' 'Aug' 'May' 'Jun' 'Sep' 'Oct' '0'] --> 'WeekOfMonthClaimed' possui 5 valores \u00fanicos, s\u00e3o eles: [1 4 2 3 5] --> 'Sex' possui 2 valores \u00fanicos, s\u00e3o eles: ['Female' 'Male'] --> 'MaritalStatus' possui 4 valores \u00fanicos, s\u00e3o eles: ['Single' 'Married' 'Widow' 'Divorced'] --> 'Age' possui 66 valores \u00fanicos, s\u00e3o eles: [21 34 47 65 27 20 36 0 30 42 71 52 28 61 38 41 32 40 63 31 45 60 39 55 35 44 72 29 37 59 49 50 26 48 64 33 74 23 25 56 16 68 18 51 22 53 46 43 57 54 69 67 19 78 77 75 80 58 73 24 76 62 79 70 17 66] --> 'Fault' possui 2 valores \u00fanicos, s\u00e3o eles: ['Policy Holder' 'Third Party'] --> 'PolicyType' possui 9 valores \u00fanicos, s\u00e3o eles: ['Sport - Liability' 'Sport - Collision' 'Sedan - Liability' 'Utility - All Perils' 'Sedan - All Perils' 'Sedan - Collision' 'Utility - Collision' 'Utility - Liability' 'Sport - All Perils'] --> 'VehicleCategory' possui 3 valores \u00fanicos, s\u00e3o eles: ['Sport' 'Utility' 'Sedan'] --> 'VehiclePrice' possui 6 valores \u00fanicos, s\u00e3o eles: ['more than 69000' '20000 to 29000' '30000 to 39000' 'less than 20000' '40000 to 59000' '60000 to 69000'] --> 'FraudFound_P' possui 2 valores \u00fanicos, s\u00e3o eles: [0 1] --> 'PolicyNumber' possui 15420 valores \u00fanicos, s\u00e3o eles: [ 1 2 3 ... 15418 15419 15420] --> 'RepNumber' possui 16 valores \u00fanicos, s\u00e3o eles: [12 15 7 4 3 14 1 13 11 16 6 2 8 5 9 10] --> 'Deductible' possui 4 valores \u00fanicos, s\u00e3o eles: [300 400 500 700] --> 'DriverRating' possui 4 valores \u00fanicos, s\u00e3o eles: [1 4 3 2] --> 'Days_Policy_Accident' possui 5 valores \u00fanicos, s\u00e3o eles: ['more than 30' '15 to 30' 'none' '1 to 7' '8 to 15'] --> 'Days_Policy_Claim' possui 4 valores \u00fanicos, s\u00e3o eles: ['more than 30' '15 to 30' '8 to 15' 'none'] --> 'PastNumberOfClaims' possui 4 valores \u00fanicos, s\u00e3o eles: ['none' '1' '2 to 4' 'more than 4'] --> 'AgeOfVehicle' possui 8 valores \u00fanicos, s\u00e3o eles: ['3 years' '6 years' '7 years' 'more than 7' '5 years' 'new' '4 years' '2 years'] --> 'AgeOfPolicyHolder' possui 9 valores \u00fanicos, s\u00e3o eles: ['26 to 30' '31 to 35' '41 to 50' '51 to 65' '21 to 25' '36 to 40' '16 to 17' 'over 65' '18 to 20'] --> 'PoliceReportFiled' possui 2 valores \u00fanicos, s\u00e3o eles: ['No' 'Yes'] --> 'WitnessPresent' possui 2 valores \u00fanicos, s\u00e3o eles: ['No' 'Yes'] --> 'AgentType' possui 2 valores \u00fanicos, s\u00e3o eles: ['External' 'Internal'] --> 'NumberOfSuppliments' possui 4 valores \u00fanicos, s\u00e3o eles: ['none' 'more than 5' '3 to 5' '1 to 2'] --> 'AddressChange_Claim' possui 5 valores \u00fanicos, s\u00e3o eles: ['1 year' 'no change' '4 to 8 years' '2 to 3 years' 'under 6 months'] --> 'NumberOfCars' possui 5 valores \u00fanicos, s\u00e3o eles: ['3 to 4' '1 vehicle' '2 vehicles' '5 to 8' 'more than 8'] --> 'Year' possui 3 valores \u00fanicos, s\u00e3o eles: [1994 1995 1996] --> 'BasePolicy' possui 3 valores \u00fanicos, s\u00e3o eles: ['Liability' 'Collision' 'All Perils'] 3.3. Resumo sobre as colunas Breve entendimento de cada atributo. Levando em conta o contexto do problema, em geral o nome da coluna \u00e9 autoexplicativo, por\u00e9m, em alguns momentos pode dar margem \u00e0 mais de uma interpreta\u00e7\u00e3o. Aqui explicarei o que \u00e9 cada atributo, como trabalharei com ele e problemas que precisam ser investigados. Month (object): considerarei como o m\u00eas em que o acidente ocorreu m\u00eas abreviado (3 primeiras letras, em ingl\u00eas) cont\u00e9m os 12 poss\u00edveis meses, n\u00e3o h\u00e1 valores faltantes nem incompat\u00edveis WeekOfMonth (int64): considerarei como a semana do m\u00eas em que o acidente ocorreu cont\u00e9m valores inteiros de 1 a 5 DayOfWeek (object): considerarei como o dia da semana em que o acidente ocorreu nome do dia da semana completo, em ingl\u00eas cont\u00e9m os 7 poss\u00edveis dias da semana, n\u00e3o h\u00e1 valores faltantes nem incompat\u00edveis Make (object): considerarei como a marca do ve\u00edculo envolvido no acidente cont\u00e9m uma lista com 19 fabricantes AccidentArea (object): informa se o acidente ocorreu em uma \u00e1rea rural ou urbana DayOfWeekClaimed (object): dia da semana referente ao pedido de seguro nome do dia da semana completo, em ingl\u00eas cont\u00e9m 8 valores \u00fanicos (7 poss\u00edveis dias da semana + '0'), a seguir ser\u00e3o investigados mais a fundo MonthClaimed (object): m\u00eas referente ao pedido de seguro m\u00eas abreviado (3 primeiras letras, em ingl\u00eas) cont\u00e9m 13 valores \u00fanicos (12 poss\u00edves meses + '0'), a seguir ser\u00e3o investigados mais a fundo WeekOfMonthClaimed (int64): semana do m\u00eas referente ao pedido de seguro cont\u00e9m valores inteiros de 1 a 5 Sex (object): \u00e9 o sexo biol\u00f3gico da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) pode ser: sexo masculino ou feminino MaritalStatus (object): \u00e9 o estado civil da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) pode ser: solteiro, casado, divorciado ou vi\u00favo Age (int64): \u00e9 a idade da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) valores inteiros de 16 a 80 (a seguir, o valor '0' ser\u00e1 investigado mais a fundo) Fault (object): indica quem foi considerado como o respons\u00e1vel pelo acidente pode ser: titular do seguro ou terceiro PolicyType (object): indica o tipo da ap\u00f3lice contratada cont\u00e9m uma lista com 9 tipos, onde cada um \u00e9 composto por 'categoria do ve\u00edculo' + 'tipo do seguro' categoria do ve\u00edculo: sport, sedan, utility tipo de seguro: liability, all perils, collision j\u00e1 existe um atributo para indicar a categoria do ve\u00edculo (VehicleCategory) e outra para indicar o tipo de seguro (BasePolicy), a seguir verificarei como tratar essas informa\u00e7\u00f5es duplicadas VehicleCategory (object): indica a categoria do ve\u00edculo registrado na ap\u00f3lice/envolvido no acidente pode ser: sport, sedan, utility VehiclePrice (object): indica o pre\u00e7o do ve\u00edculo envolvido no acidente cont\u00e9m 6 faixas de valores (provavelmente em d\u00f3lares) duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio FraudFound_P (int64): indica se aquela solicita\u00e7\u00e3o foi ou n\u00e3o uma fraude \u00e9 o atributo alvo PolicyNumber (int64): identifica cada pedido de seguro cont\u00e9m 15420 valores inteiros \u00fanicos, de 1 a 15420 a seguir ser\u00e1 verificada a rela\u00e7\u00e3o entre o n\u00famero da linha e o PolicyNumber, possivelmente s\u00e3o a mesma coisa RepNumber (int64): indica o n\u00famero do representante cont\u00e9m valores inteiros de 1 a 16 n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado Deductible (int64): indica o valor da franquia do seguro 4 op\u00e7\u00f5es de valores inteiros: 300, 400, 500 e 700 DriverRating (int64): alguma m\u00e9trica de avalia\u00e7\u00e3o do motorista cont\u00e9m valores inteiros de 1 a 4 n\u00e3o fica claro se esses dados se traduzem em categorias ou se est\u00e3o dentro de um certo intervalo de avalia\u00e7\u00e3o Days_Policy_Accident (object): ser\u00e1 considerado como o n\u00famero de dias que se passaram entre a compra da ap\u00f3lice e o acidente cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio Days_Policy_Claim (object): ser\u00e1 considerado como o n\u00famero de dias que se passaram entre a compra da ap\u00f3lice e o pedido de indeniza\u00e7\u00e3o cont\u00e9m 4 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio PastNumberOfClaims (object): indica quantas vezes a mesma pessoa realizou pedidos de indeniza\u00e7\u00e3o cont\u00e9m 4 faixas de valores AgeOfVehicle (object): indica a idade do ve\u00edculo (considerarei referenre ao momento do acidente) cont\u00e9m 8 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio AgeOfPolicyHolder (object): indica a idade do titular da ap\u00f3lice cont\u00e9m 9 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio PoliceReportFiled (object): indica se foi feito um boletim de ocorr\u00eancia para o acidente pode ser: sim ou n\u00e3o WitnessPresent (object): indica se uma testemunha estava presente no momento do acidente pode ser: sim ou n\u00e3o AgentType (object): classifica um agente (relacionado ao acidente ou ao pedido) como externo ou interno n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado NumberOfSuppliments (object): indica algum tipo de suplemento no seguro cont\u00e9m 4 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado AddressChange_Claim (object): indica quantos anos se passaram entre a \u00faltima vez que foi registrado uma mudan\u00e7a de endere\u00e7o e o pedido de indeniza\u00e7\u00e3o cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio NumberOfCars (object): considerarei como o n\u00famero de carros cobertos pela ap\u00f3lice (outra possibilidade seria o n\u00famero de carros envolvidos no acidente) cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio Year (int64): considerarei como o ano em que o acidente ocorreu pode ser: 1994, 1995 e 1996 BasePolicy (object): indica o tipo de seguro contratado pode ser: liability, all perils, collision PARTE 4: Tratamento dos dados df_dataset_1 = df_dataset . copy () 4.1. Verificar rela\u00e7\u00e3o entre os \u00edndices das linhas do dataset e 'PolicyNumber' PolicyNumber \u00e9 igual ao \u00edndice da linha+1? df_dataset_1 [ 'PolicyNumber' ] . describe <bound method NDFrame.describe of 0 1 1 2 2 3 3 4 4 5 ... 15415 15416 15416 15417 15417 15418 15418 15419 15419 15420 Name: PolicyNumber, Length: 15420, dtype: int64> df_dataset_total_linhas = len ( df_dataset_1 ) PolicyNumber_e_indice = sum ( 1 if df_dataset_1 [ 'PolicyNumber' ] . iloc [ indice ] == indice + 1 else 0 \\ for indice in range ( df_dataset_total_linhas )) print ( f \"N\u00famero de linhas do dataset: { df_dataset_total_linhas } \" ) print ( f \"Quantidade de linhas onde PolicyNumber = indice+1: { PolicyNumber_e_indice } \" ) N\u00famero de linhas do dataset: 15420 Quantidade de linhas onde PolicyNumber = indice+1: 15420 Como a suposi\u00e7\u00e3o foi provada verdadeira, esta coluna ser\u00e1 ignorada, pois n\u00e3o apresenta utilidade para o problema em quest\u00e3o. df_dataset_1 . drop ( columns = 'PolicyNumber' , inplace = True ) 4.2. Tratar o valor 0 encontrado em 'DayOfWeekClaimed', 'MonthClaimed' e 'Age' ##### 'DayOfWeekClaimed' e 'MonthClaimed' DayOfWeekClaimed_zeros = df_dataset_1 . index [ df_dataset_1 [ 'DayOfWeekClaimed' ] == '0' ] . tolist () MonthClaimed_zeros = df_dataset_1 . index [ df_dataset_1 [ 'MonthClaimed' ] == '0' ] . tolist () print ( \"DayOfWeekClaimed e MonthClaimed possuem o valor '0' na(s) mesma(s) linha(s):\" , DayOfWeekClaimed_zeros == MonthClaimed_zeros ) print ( \"\u00cdndice(s):\" , DayOfWeekClaimed_zeros ) # como trata-se apenas de uma \u00fanica ocorr\u00eancia, a linha correspondente ser\u00e1 eliminada df_dataset_1 = df_dataset_1 . drop ( DayOfWeekClaimed_zeros ) df_dataset_1 . reset_index ( drop = True , inplace = True ) DayOfWeekClaimed e MonthClaimed possuem o valor '0' na(s) mesma(s) linha(s): True \u00cdndice(s): [1516] ##### 'Age' Age_zeros = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 0 ] . tolist () print ( f \"Age possui o valor '0' em { len ( Age_zeros ) } linhas. \\n \" ) # partirei do seguinte questionamento: nessas ocasi\u00f5es, existe alguma rela\u00e7\u00e3o com outra coluna que pode ser \u00fatil? for coluna in df_dataset_1 . columns : linhas_age0 = df_dataset_1 . loc [ df_dataset_1 [ 'Age' ] == 0 , coluna ] . unique () if len ( linhas_age0 ) < 2 and coluna != 'Age' : print ( f \"--> Correspond\u00eancia para ' { coluna } ': { linhas_age0 } \" ) Age possui o valor '0' em 319 linhas. --> Correspond\u00eancia para 'Sex': ['Male'] --> Correspond\u00eancia para 'MaritalStatus': ['Single'] --> Correspond\u00eancia para 'Days_Policy_Accident': ['more than 30'] --> Correspond\u00eancia para 'Days_Policy_Claim': ['more than 30'] --> Correspond\u00eancia para 'AgeOfPolicyHolder': ['16 to 17'] df_dataset_1 [ 'AgeOfPolicyHolder' ] . value_counts () . sort_index () 16 to 17 319 18 to 20 15 21 to 25 108 26 to 30 613 31 to 35 5593 36 to 40 4043 41 to 50 2828 51 to 65 1392 over 65 508 Name: AgeOfPolicyHolder, dtype: int64 Verificando as colunas que possuem sempre a mesma correspond\u00eancia, \u00e0 primeira vista, aquela que pode dar uma solu\u00e7\u00e3o para Age==0 \u00e9 'AgeOfPolicyHolder'. E, de fato, considerando todo o dataset, temos uma frequ\u00eancia de 319 para '16 to 17' em AgeOfPolicyHolder. Exatamente a mesma quantidade de linhas com Age==0. Investigarei o seguinte: 'Age' sempre est\u00e1 dentro do intervalo de 'AgeOfPolicyHolder'? # extrair os limites dos intervalos em 'AgeOfPolicyHolder' get_intervalo_AgeOfPolicyHolder = lambda linha : [ int ( linha [: 2 ]), int ( linha [ - 2 :])] if 'to' in linha else [ int ( linha [ - 2 :]), 200 ] df_dataset_total_linhas = len ( df_dataset_1 ) total = 0 for indice in range ( df_dataset_total_linhas ): idade = df_dataset_1 [ 'Age' ] . iloc [ indice ] intervalo_idade = get_intervalo_AgeOfPolicyHolder ( df_dataset_1 [ 'AgeOfPolicyHolder' ] . iloc [ indice ]) if ~ ( idade >= intervalo_idade [ 0 ] and idade <= intervalo_idade [ 1 ]): total += 1 print ( f \"Em { total } linhas 'Age' n\u00e3o corresponde a 'AgeOfPolicyHolder'. O que representa { total / df_dataset_total_linhas : .2% } do total de linhas.\" ) Em 7241 linhas 'Age' n\u00e3o corresponde a 'AgeOfPolicyHolder'. O que representa 46.96% do total de linhas. A resposta para o questionamento \u00e9 'n\u00e3o'. Ou seja, o indiv\u00edduo envolvido no acidente n\u00e3o necessariamente \u00e9 a pessoa titular da ap\u00f3lice do seguro. Apesar disso, para n\u00e3o excluir essa quantidade de linhas do dataset, a op\u00e7\u00e3o escolhida para dar seguimento ao projeto \u00e9 a de substituir o valor de 0 de 'Age' pela m\u00e9dia do intervalo de 'AgeOfPolicyHolder'. df_dataset_1 . loc [ df_dataset_1 [ 'Age' ] == 0 , 'Age' ] = 16.5 Age_zeros = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 0 ] . tolist () print ( f \"Age possui o valor '0' em { len ( Age_zeros ) } linhas. \\n \" ) Age_mean = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 16.5 ] . tolist () print ( f \"Age possui o valor '16.5' em { len ( Age_mean ) } linhas. \\n \" ) Age possui o valor '0' em 0 linhas. Age possui o valor '16.5' em 319 linhas. 4.3. Verificar rela\u00e7\u00e3o entre 'PolicyType', 'VehicleCategory' e 'BasePolicy' PolicyType = \"VehicleCategory - BasePolicy\"? df_dataset_1 [[ 'PolicyType' , 'VehicleCategory' , 'BasePolicy' ]] . describe <bound method NDFrame.describe of PolicyType VehicleCategory BasePolicy 0 Sport - Liability Sport Liability 1 Sport - Collision Sport Collision 2 Sport - Collision Sport Collision 3 Sedan - Liability Sport Liability 4 Sport - Collision Sport Collision ... ... ... ... 15414 Sedan - Collision Sedan Collision 15415 Sedan - Liability Sport Liability 15416 Sedan - Collision Sedan Collision 15417 Sedan - All Perils Sedan All Perils 15418 Sedan - Collision Sedan Collision [15419 rows x 3 columns]> df_dataset_total_linhas = len ( df_dataset_1 ) total = 0 for indice in range ( df_dataset_total_linhas ): PolicyType = df_dataset_1 [ 'PolicyType' ] . iloc [ indice ] VehicleCategory_BasePolicy = f \" { df_dataset_1 [ 'VehicleCategory' ] . iloc [ indice ] } - { df_dataset_1 [ 'BasePolicy' ] . iloc [ indice ] } \" if PolicyType == VehicleCategory_BasePolicy : total += 1 print ( f \"'PolicyType' corresponde a 'VehicleCategory - BasePolicy' em { total } linhas. O que representa { total / df_dataset_total_linhas : .2% } do total de linhas.\" ) 'PolicyType' corresponde a 'VehicleCategory - BasePolicy' em 10432 linhas. O que representa 67.66% do total de linhas. A suposi\u00e7\u00e3o foi provada falsa, portanto, nada ser\u00e1 feito com essas tr\u00eas colunas. Mesmo se a suposi\u00e7\u00e3o fosse verdadeira, ter as informa\u00e7\u00f5es tanto separadas quanto combinadas pode ser \u00fatil para o modelo a ser constru\u00eddo. PARTE 5: Investigar rela\u00e7\u00f5es com a vari\u00e1vel alvo Usar de testes estat\u00edsticos e recursos gr\u00e1ficos para verificar as rela\u00e7\u00f5es entre os atributos e o target ('FraudFound_P'). 5.1 Teste de Independ\u00eancia Considerando os diferentes tipos de atributos (num\u00e9ricos, bin\u00e1rios, categ\u00f3ricos), a abordagem escolhida leva em conta a tabela de contig\u00eancia para cada par coluna e target. Ser\u00e1 realizado o teste de independ\u00eancia chi-quadrado, onde a hip\u00f3tese nula $H_0$ \u00e9 que as colunas n\u00e3o t\u00eam rela\u00e7\u00e3o e a hip\u00f3tese alternativa $H_1$ \u00e9 que existe rela\u00e7\u00e3o entre as colunas. O n\u00edvel de signific\u00e2ncia (alfa) considerado \u00e9 de 0,05. Para os resultados dos testes: Se o valor-p encontrado for maior que alfa , n\u00e3o rejeitamos a hip\u00f3tese nula. Podemos dizer que o resultado do teste n\u00e3o detecta uma rela\u00e7\u00e3o significativa entre as vari\u00e1veis; Se o valor-p encontrado for menor que alfa , podemos dizer que h\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre as vari\u00e1veis. # exemplo de tabela de contig\u00eancia pd . crosstab ( df_dataset_1 [ 'MaritalStatus' ], df_dataset_1 [ 'FraudFound_P' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FraudFound_P 0 1 MaritalStatus Divorced 73 3 Married 9986 639 Single 4405 278 Widow 32 3 # teste de independ\u00eandia chi-quadrado nivel_significancia = 0.05 teste_independencia = [] for coluna in df_dataset_1 . columns : tabela_contigencia = pd . crosstab ( df_dataset_1 [ coluna ], df_dataset_1 [ 'FraudFound_P' ]) chi2_val , valor_p , _ , __ = stats . chi2_contingency ( tabela_contigencia ) if valor_p < nivel_significancia : teste_independencia . append ([ coluna , chi2_val , valor_p ]) print ( \"H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos:\" ) df_teste_independencia = pd . DataFrame ( teste_independencia , columns = [ 'Atributo' , ' \\u03C7\\u00B2 ' , 'Valor-p' ]) df_teste_independencia . sort_values ( by = 'Valor-p' , inplace = True ) print ( df_teste_independencia ) H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos: Atributo \u03c7\u00b2 Valor-p 10 FraudFound_P 15401.236138 0.000000e+00 7 PolicyType 437.491381 1.768441e-89 20 BasePolicy 402.947238 3.170436e-88 8 VehicleCategory 290.980893 6.520817e-64 6 Fault 264.984556 1.406180e-59 18 AddressChange_Claim 104.722693 9.704718e-22 11 Deductible 72.406255 1.302831e-15 9 VehiclePrice 67.836116 2.888324e-13 13 PastNumberOfClaims 53.541755 1.405198e-11 1 Make 59.815292 2.191573e-06 3 MonthClaimed 42.200514 1.495245e-05 2 AccidentArea 16.901858 3.936304e-05 15 AgeOfPolicyHolder 33.104861 5.896560e-05 4 Sex 13.495678 2.391135e-04 17 NumberOfSuppliments 18.155527 4.085276e-04 5 Age 109.664968 4.472083e-04 0 Month 29.771469 1.720902e-03 14 AgeOfVehicle 21.995137 2.545322e-03 16 AgentType 7.380469 6.593597e-03 19 Year 9.592587 8.260307e-03 12 Days_Policy_Accident 11.569842 2.085381e-02 5.2 An\u00e1lise Gr\u00e1fica Apenas com os atributos que mostram ter uma potencial rela\u00e7\u00e3o com a vari\u00e1vel alvo, utilizarei de recursos gr\u00e1ficos para visualizar mais detalhes dessas rela\u00e7\u00f5es. def grafico_barras ( tab_cont , titulo , size ): fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = size ) tab_cont [ 0 ] . plot . bar ( ax = ax1 , color = 'green' , rot = 0 ) tab_cont [ 1 ] . plot . bar ( ax = ax2 , color = 'darkred' , rot = 0 ) ax1 . set_title ( titulo , size = 14 , fontweight = \"bold\" ) ax1 . set ( ylabel = 'Total - N\u00e3o Fraude' , xlabel = '' ) ax2 . set ( ylabel = 'Total - Fraude' , xlabel = '' ) return ax1 , ax2 coluna = 'PolicyType' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 15 , 4 )) plt . show () FraudFound_P 0 1 PolicyType Sedan - All Perils 3675 411 Sedan - Collision 5200 384 Sedan - Liability 4951 36 Sport - All Perils 22 0 Sport - Collision 300 48 Sport - Liability 1 0 Utility - All Perils 299 41 Utility - Collision 27 3 Utility - Liability 21 0 coluna = 'BasePolicy' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'VehicleCategory' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () FraudFound_P 0 1 BasePolicy All Perils 3996 452 Collision 5527 435 Liability 4973 36 FraudFound_P 0 1 VehicleCategory Sedan 8875 795 Sport 5274 84 Utility 347 44 coluna = 'Fault' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'AddressChange_Claim' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 7 , 4 )) plt . show () FraudFound_P 0 1 Fault Policy Holder 10343 886 Third Party 4153 37 FraudFound_P 0 1 AddressChange_Claim 1 year 159 11 2 to 3 years 240 51 4 to 8 years 598 33 no change 13498 825 under 6 months 1 3 coluna = 'Deductible' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'VehiclePrice' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 10 , 4 )) plt . show () FraudFound_P 0 1 Deductible 300 6 2 400 13981 856 500 216 47 700 293 18 FraudFound_P 0 1 VehiclePrice 20000 to 29000 7658 421 30000 to 39000 3358 175 40000 to 59000 430 31 60000 to 69000 83 4 less than 20000 993 103 more than 69000 1974 189 coluna = 'PastNumberOfClaims' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'Make' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 15 , 4 )) plt . show () FraudFound_P 0 1 PastNumberOfClaims 1 3351 222 2 to 4 5191 294 more than 4 1942 68 none 4012 339 FraudFound_P 0 1 Make Accura 413 59 BMW 14 1 Chevrolet 1587 94 Dodge 107 2 Ferrari 2 0 Ford 417 33 Honda 2621 179 Jaguar 6 0 Lexus 1 0 Mazda 2231 123 Mecedes 3 1 Mercury 77 6 Nisson 29 1 Pontiac 3624 213 Porche 5 0 Saab 97 11 Saturn 52 6 Toyota 2935 186 VW 275 8 coluna = 'MonthClaimed' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Month' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () FraudFound_P 0 1 MonthClaimed Apr 1189 82 Aug 1034 92 Dec 1097 49 Feb 1209 78 Jan 1354 92 Jul 1169 56 Jun 1215 78 Mar 1251 97 May 1309 102 Nov 1239 46 Oct 1266 73 Sep 1164 78 FraudFound_P 0 1 Month Apr 1200 80 Aug 1043 84 Dec 1223 62 Feb 1184 82 Jan 1324 87 Jul 1196 60 Jun 1241 80 Mar 1258 102 May 1273 94 Nov 1155 46 Oct 1235 70 Sep 1164 76 coluna = 'AccidentArea' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'NumberOfSuppliments' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () FraudFound_P 0 1 AccidentArea Rural 1464 133 Urban 13032 790 FraudFound_P 0 1 NumberOfSuppliments 1 to 2 2330 159 3 to 5 1920 97 more than 5 3672 195 none 6574 472 coluna = 'AgeOfPolicyHolder' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Age' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 28 , 10 )) plt . show () FraudFound_P 0 1 AgeOfPolicyHolder 16 to 17 288 31 18 to 20 13 2 21 to 25 92 16 26 to 30 580 33 31 to 35 5233 360 36 to 40 3806 237 41 to 50 2684 144 51 to 65 1322 70 over 65 478 30 FraudFound_P 0 1 Age 16.0 8 1 16.5 288 31 17.0 5 1 18.0 40 8 19.0 27 5 ... ... .. 76.0 39 3 77.0 28 1 78.0 33 2 79.0 19 1 80.0 31 1 [66 rows x 2 columns] coluna = 'AgeOfVehicle' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Sex' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () FraudFound_P 0 1 AgeOfVehicle 2 years 70 3 3 years 139 13 4 years 208 21 5 years 1262 95 6 years 3220 228 7 years 5482 325 more than 7 3775 206 new 340 32 FraudFound_P 0 1 Sex Female 2315 105 Male 12181 818 coluna = 'AgentType' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'Year' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'Days_Policy_Accident' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 6 , 4 )) plt . show () FraudFound_P 0 1 AgentType External 14259 919 Internal 237 4 FraudFound_P 0 1 Year 1994 5732 409 1995 4894 301 1996 3870 213 FraudFound_P 0 1 Days_Policy_Accident 1 to 7 13 1 15 to 30 46 3 8 to 15 50 5 more than 30 14341 905 none 46 9 PARTE 6: Transformar dados df_dataset_2 = df_dataset_1 . copy () 6.1. Convers\u00e3o de dados categ\u00f3ricos e bin\u00e1rios A partir da investiga\u00e7\u00e3o realizada anteriormente, os seguintes atributos categ\u00f3ricos podem ser transformados em bin\u00e1rio: AccidentArea : 1=Urban, 0=Rural Sex : 1=Female, 0=Male Fault : 1=Policy Holder, 0=Third Party PoliceReportFiled : 1=No, 0=Yes WitnessPresent : 1=No, 0=Yes AgentType : 1=External 0=Internal categorico_para_binario = [ 'AccidentArea' , 'Sex' , 'Fault' , 'PoliceReportFiled' , 'WitnessPresent' , 'AgentType' ] for coluna in categorico_para_binario : print ( coluna ) valores_unicos = df_dataset_2 [ coluna ] . unique () print ( f \"Valores antigos: { valores_unicos } \" ) df_dataset_2 [ coluna ] = df_dataset_2 [ coluna ] . replace ( valores_unicos [ 0 ], 1 ) . replace ( valores_unicos [ 1 ], 0 ) valores_unicos = df_dataset_2 [ coluna ] . unique () print ( f \"Valores novos: { valores_unicos } \\n \" ) AccidentArea Valores antigos: ['Urban' 'Rural'] Valores novos: [1 0] Sex Valores antigos: ['Female' 'Male'] Valores novos: [1 0] Fault Valores antigos: ['Policy Holder' 'Third Party'] Valores novos: [1 0] PoliceReportFiled Valores antigos: ['No' 'Yes'] Valores novos: [1 0] WitnessPresent Valores antigos: ['No' 'Yes'] Valores novos: [1 0] AgentType Valores antigos: ['External' 'Internal'] Valores novos: [1 0] df_dataset_2 [ categorico_para_binario ] . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 AccidentArea 15419 non-null int64 1 Sex 15419 non-null int64 2 Fault 15419 non-null int64 3 PoliceReportFiled 15419 non-null int64 4 WitnessPresent 15419 non-null int64 5 AgentType 15419 non-null int64 dtypes: int64(6) memory usage: 722.9 KB 6.2 Convers\u00e3o dos dados ordinais A partir da investiga\u00e7\u00e3o realizada anteriormente, s\u00e3o atributos ordinais: 'Month', 'DayOfWeek', 'DayOfWeekClaimed', 'MonthClaimed'. Estes devem corresponder \u00e0 sua ordem natural. Exemplo: para o atributo 'Month', 'Jan' deve corresponder a 1, 'Feb' deve corresponder a 2, 'Mar' deve corresponder a 3, etc. dias_da_semana = { 'Monday' : 1 , 'Tuesday' : 2 , 'Wednesday' : 3 , 'Thursday' : 4 , 'Friday' : 5 , 'Saturday' : 6 , 'Sunday' : 7 } meses = { 'Jan' : 1 , 'Feb' : 2 , 'Mar' : 3 , 'Apr' : 4 , 'May' : 5 , 'Jun' : 6 , 'Jul' : 7 , 'Aug' : 8 , 'Sep' : 9 , 'Oct' : 10 , 'Nov' : 11 , 'Dec' : 12 } Month = { 'Month' : meses } DayOfWeek = { 'DayOfWeek' : dias_da_semana } DayOfWeekClaimed = { 'DayOfWeekClaimed' : dias_da_semana } MonthClaimed = { 'MonthClaimed' : meses } mapping = [ Month , DayOfWeek , DayOfWeekClaimed , MonthClaimed ] for i in range ( len ( mapping )): df_dataset_2 . replace ( mapping [ i ], inplace = True ) ordinais = [ 'Month' , 'DayOfWeek' , 'DayOfWeekClaimed' , 'MonthClaimed' ] for coluna in ordinais : print ( coluna ) print ( f \"Valores antigos: { df_dataset_1 [ coluna ] . unique () } \" ) print ( f \"Valores novos: { df_dataset_2 [ coluna ] . unique () } \\n \" ) Month Valores antigos: ['Dec' 'Jan' 'Oct' 'Jun' 'Feb' 'Nov' 'Apr' 'Mar' 'Aug' 'Jul' 'May' 'Sep'] Valores novos: [12 1 10 6 2 11 4 3 8 7 5 9] DayOfWeek Valores antigos: ['Wednesday' 'Friday' 'Saturday' 'Monday' 'Tuesday' 'Sunday' 'Thursday'] Valores novos: [3 5 6 1 2 7 4] DayOfWeekClaimed Valores antigos: ['Tuesday' 'Monday' 'Thursday' 'Friday' 'Wednesday' 'Saturday' 'Sunday'] Valores novos: [2 1 4 5 3 6 7] MonthClaimed Valores antigos: ['Jan' 'Nov' 'Jul' 'Feb' 'Mar' 'Dec' 'Apr' 'Aug' 'May' 'Jun' 'Sep' 'Oct'] Valores novos: [ 1 11 7 2 3 12 4 8 5 6 9 10] 6.3 Convers\u00e3o dos dados de intervalo A partir da investiga\u00e7\u00e3o realizada anteriormente, s\u00e3o atributos de intervalos: 'PastNumberOfClaims', 'NumberOfSuppliments', 'VehiclePrice', 'Days_Policy_Accident', 'Days_Policy_Claim', 'AgeOfVehicle', 'AgeOfPolicyHolder', 'AddressChange_Claim', 'NumberOfCars'. Estes ser\u00e3o nivelados por baixo. Exemplo: para o atributo 'NumberOfSuppliments', 'none' deve corresponder a 0, '1 to 2' deve corresponder a 1, '3 to 5' deve corresponder a 3 e 'more than 5' deve corresponder a 6. PastNumberOfClaims = { 'PastNumberOfClaims' :{ 'none' : 0 , '1' : 1 , '2 to 4' : 2 , 'more than 4' : 5 }} NumberOfSuppliments = { 'NumberOfSuppliments' :{ 'none' : 0 , '1 to 2' : 1 , '3 to 5' : 3 , 'more than 5' : 6 }} AgeOfVehicle = { 'AgeOfVehicle' :{ '3 years' : 3 , '6 years' : 6 , '7 years' : 7 , 'more than 7' : 8 , '5 years' : 5 , 'new' : 0 , '4 years' : 4 , '2 years' : 2 }} VehiclePrice = { 'VehiclePrice' :{ 'more than 69000' : 69001 , '20000 to 29000' : 20000 , '30000 to 39000' : 30000 , 'less than 20000' : 19999 , '40000 to 59000' : 40000 , '60000 to 69000' : 60000 }} Days_Policy_Accident = { 'Days_Policy_Accident' :{ 'more than 30' : 31 , '15 to 30' : 15 , 'none' : 0 , '1 to 7' : 1 , '8 to 15' : 8 }} Days_Policy_Claim = { 'Days_Policy_Claim' :{ 'more than 30' : 31 , '15 to 30' : 15 , '8 to 15' : 8 , 'none' : 0 }} AgeOfPolicyHolder = { 'AgeOfPolicyHolder' :{ '26 to 30' : 26 , '31 to 35' : 31 , '41 to 50' : 41 , '51 to 65' : 51 , '21 to 25' : 21 , '36 to 40' : 36 , '16 to 17' : 16 , 'over 65' : 66 , '18 to 20' : 18 }} AddressChange_Claim = { 'AddressChange_Claim' :{ '1 year' : 1 , 'no change' : 0 , '4 to 8 years' : 4 , '2 to 3 years' : 2 , 'under 6 months' : 0.5 }} NumberOfCars = { 'NumberOfCars' :{ '3 to 4' : 3 , '1 vehicle' : 1 , '2 vehicles' : 2 , '5 to 8' : 5 , 'more than 8' : 9 }} mapping = [ PastNumberOfClaims , NumberOfSuppliments , VehiclePrice , AgeOfVehicle , Days_Policy_Accident , Days_Policy_Claim , AgeOfPolicyHolder , AddressChange_Claim , NumberOfCars ] for i in range ( len ( mapping )): df_dataset_2 . replace ( mapping [ i ], inplace = True ) dados_intervalos = [ 'PastNumberOfClaims' , 'NumberOfSuppliments' , 'VehiclePrice' , 'Days_Policy_Accident' , 'Days_Policy_Claim' , 'AgeOfVehicle' , 'AgeOfPolicyHolder' , 'AddressChange_Claim' , 'NumberOfCars' ] for coluna in dados_intervalos : print ( coluna ) print ( f \"Valores antigos: { df_dataset_1 [ coluna ] . unique () } \" ) print ( f \"Valores novos: { df_dataset_2 [ coluna ] . unique () } \\n \" ) PastNumberOfClaims Valores antigos: ['none' '1' '2 to 4' 'more than 4'] Valores novos: [0 1 2 5] NumberOfSuppliments Valores antigos: ['none' 'more than 5' '3 to 5' '1 to 2'] Valores novos: [0 6 3 1] VehiclePrice Valores antigos: ['more than 69000' '20000 to 29000' '30000 to 39000' 'less than 20000' '40000 to 59000' '60000 to 69000'] Valores novos: [69001 20000 30000 19999 40000 60000] Days_Policy_Accident Valores antigos: ['more than 30' '15 to 30' 'none' '1 to 7' '8 to 15'] Valores novos: [31 15 0 1 8] Days_Policy_Claim Valores antigos: ['more than 30' '15 to 30' '8 to 15'] Valores novos: [31 15 8] AgeOfVehicle Valores antigos: ['3 years' '6 years' '7 years' 'more than 7' '5 years' 'new' '4 years' '2 years'] Valores novos: [3 6 7 8 5 0 4 2] AgeOfPolicyHolder Valores antigos: ['26 to 30' '31 to 35' '41 to 50' '51 to 65' '21 to 25' '36 to 40' '16 to 17' 'over 65' '18 to 20'] Valores novos: [26 31 41 51 21 36 16 66 18] AddressChange_Claim Valores antigos: ['1 year' 'no change' '4 to 8 years' '2 to 3 years' 'under 6 months'] Valores novos: [1. 0. 4. 2. 0.5] NumberOfCars Valores antigos: ['3 to 4' '1 vehicle' '2 vehicles' '5 to 8' 'more than 8'] Valores novos: [3 1 2 5 9] 6.3 Convers\u00e3o dos dados categ\u00f3ricos e nominais A partir da investiga\u00e7\u00e3o realizada anteriormente, os seguintes atributos s\u00e3o categ\u00f3ricos e nominais: 'Make','MaritalStatus','PolicyType','VehicleCategory' e 'BasePolicy'. Agora, estes ter\u00e3o uma representa\u00e7\u00e3o vetorial bin\u00e1ria. df_dataset_2 . info ( verbose = False ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Columns: 32 entries, Month to BasePolicy dtypes: float64(2), int64(25), object(5) memory usage: 3.8+ MB # get_dummies transforma as colunas do tipo 'object' # 19 categorias de 'Make' + 4 categorias de 'MaritalStatus' + 9 categorias de 'PolicyType' # + 3 categorias de 'VehicleCategory' + 3 categorias de 'BasePolicy' # = 38 colunas df_dataset_2 = pd . get_dummies ( df_dataset_2 ); df_dataset_2 . info ( verbose = False ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Columns: 65 entries, Month to BasePolicy_Liability dtypes: float64(2), int64(25), uint8(38) memory usage: 3.7 MB PARTE 7: Investigar rela\u00e7\u00f5es com a vari\u00e1vel alvo (novamente) Ser\u00e1 realizado o mesmo teste de independ\u00eancia executado na PARTE 5. A ideia \u00e9 verificar se, ap\u00f3s as transforma\u00e7\u00f5es, aparecem novas rela\u00e7\u00f5es relevantes entre os atributos e o target ('FraudFound_P'). df_final = df_dataset_2 . copy () # teste de independ\u00eandia chi-quadrado nivel_significancia = 0.05 teste_independencia = [] for coluna in df_final . columns : tabela_contigencia = pd . crosstab ( df_final [ coluna ], df_final [ 'FraudFound_P' ]) chi2_val , valor_p , _ , __ = stats . chi2_contingency ( tabela_contigencia ) if valor_p < nivel_significancia : teste_independencia . append ([ coluna , chi2_val , valor_p ]) print ( \"H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos:\" ) df_teste_independencia = pd . DataFrame ( teste_independencia , columns = [ 'Atributo' , ' \\u03C7\\u00B2 ' , 'Valor-p' ]) df_teste_independencia . sort_values ( by = 'Valor-p' , inplace = True ) print ( df_teste_independencia ) H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos: Atributo \u03c7\u00b2 Valor-p 7 FraudFound_P 15401.236138 0.000000e+00 29 BasePolicy_Liability 364.390028 3.116620e-81 21 PolicyType_Sedan - Liability 361.581932 1.273869e-80 25 VehicleCategory_Sport 283.640594 1.208553e-63 5 Fault 264.984556 1.406180e-59 24 VehicleCategory_Sedan 229.174903 9.021607e-52 27 BasePolicy_All Perils 192.648135 8.400805e-44 19 PolicyType_Sedan - All Perils 162.856618 2.688717e-37 15 AddressChange_Claim 104.722693 9.704718e-22 8 Deductible 72.406255 1.302831e-15 6 VehiclePrice 67.836116 2.888324e-13 10 PastNumberOfClaims 53.541755 1.405198e-11 22 PolicyType_Sport - Collision 37.152501 1.092425e-09 17 Make_Accura 35.525852 2.516880e-09 28 BasePolicy_Collision 29.267146 6.305578e-08 23 PolicyType_Utility - All Perils 21.691830 3.201519e-06 26 VehicleCategory_Utility 18.827174 1.431136e-05 2 MonthClaimed 42.200514 1.495245e-05 1 AccidentArea 16.901858 3.936304e-05 12 AgeOfPolicyHolder 33.104861 5.896560e-05 3 Sex 13.495678 2.391135e-04 14 NumberOfSuppliments 18.155527 4.085276e-04 4 Age 109.664968 4.472083e-04 20 PolicyType_Sedan - Collision 12.093358 5.060177e-04 0 Month 29.771469 1.720902e-03 11 AgeOfVehicle 21.995137 2.545322e-03 13 AgentType 7.380469 6.593597e-03 16 Year 9.592587 8.260307e-03 9 Days_Policy_Accident 11.569842 2.085381e-02 18 Make_VW 4.557015 3.278417e-02 PARTE 8: Dividir vari\u00e1veis dependentes e independentes Vari\u00e1vel dependente/alvo: 'FraudFound_P' X = df_final . drop ( 'FraudFound_P' , axis = 1 ) . copy () y = df_final [ 'FraudFound_P' ] . copy () print ( X . shape , y . shape ) (15419, 64) (15419,) PARTE 9: Dividir dados de treino e de teste 70% treino e 30% teste X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , stratify = y , random_state = 42 ) print ( X_train . shape , X_test . shape , y_train . shape , y_test . shape ) (10793, 64) (4626, 64) (10793,) (4626,) PARTE 10: Balancear dados de treino M\u00e9todo escolhido: ADASYN y_train . value_counts () 0 10147 1 646 Name: FraudFound_P, dtype: int64 X_train_over , y_train_over = ADASYN () . fit_resample ( X_train , y_train ) y_train_over . value_counts () 0 10147 1 10088 Name: FraudFound_P, dtype: int64 PARTE 11: Rodar experimentos Ser\u00e3o avaliadas diferentes configura\u00e7\u00f5es do algoritmo Random Forest. 11.1 Criar o modelo de classifica\u00e7\u00e3o e definir os poss\u00edveis hiperpar\u00e2metros RF_classifier = RandomForestClassifier () n_estimators = [ 10 , 100 , 300 ] max_features = [ 'sqrt' , 'log2' ] max_depth = [ 3 , 5 , 8 ] RF_grid = dict ( n_estimators = n_estimators , max_features = max_features , max_depth = max_depth ) 11.2 Definir as m\u00e9tricas de compara\u00e7\u00e3o metricas = { 'accuracy_score' : metrics . make_scorer ( metrics . accuracy_score ), 'f1_score' : metrics . make_scorer ( metrics . f1_score ), 'roc_auc_score' : metrics . make_scorer ( metrics . roc_auc_score ) } 11.3 Definir os par\u00e2metros dos experimentos # valida\u00e7\u00e3o cruzada n_splits = 5 n_repeats = 5 cv = RepeatedStratifiedKFold ( n_splits = n_splits , n_repeats = n_repeats ) 11.4 Executar os experimentos Os experimentos ser\u00e3o realizados a partir do 'GridSearchCV', assim, ser\u00e3o realizadas n repeti\u00e7\u00f5es de valida\u00e7\u00e3o cruzada para cada poss\u00edvel configura\u00e7\u00e3o de classificador. No final, teremos dispon\u00edveis, para cada m\u00e9trica definida anteriormente, os resultados de cada experimento para cada configura\u00e7\u00e3o de classificador. grid_search = GridSearchCV ( estimator = RF_classifier , param_grid = RF_grid , cv = cv , scoring = metricas , refit = 'accuracy_score' , error_score = 0 ) grid_result = grid_search . fit ( X_train_over , y_train_over ) PARTE 12: Analisar os resultados \u00c9 poss\u00edvel observar os resultados do grid search a partir do atributo 'cv_results_'. Para criar os gr\u00e1ficos, ser\u00e1 realizado um tratamento no formato original retornado por esse atributo. id_vars = [ 'mean_fit_time' , 'std_fit_time' , 'mean_score_time' , 'std_score_time' , 'params' , 'mean_test_accuracy_score' , 'std_test_accuracy_score' , 'rank_test_accuracy_score' , 'mean_test_f1_score' , 'std_test_f1_score' , 'rank_test_f1_score' , 'mean_test_roc_auc_score' , 'std_test_roc_auc_score' , 'rank_test_roc_auc_score' ] id_vars = id_vars + [ 'param_max_features' , 'param_n_estimators' , 'param_max_depth' ] df_results_RF = pd . DataFrame ( grid_result . cv_results_ ) df_results_RF = pd . melt ( df_results_RF , id_vars = id_vars ) df_results_RF . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1350 entries, 0 to 1349 Data columns (total 19 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mean_fit_time 1350 non-null float64 1 std_fit_time 1350 non-null float64 2 mean_score_time 1350 non-null float64 3 std_score_time 1350 non-null float64 4 params 1350 non-null object 5 mean_test_accuracy_score 1350 non-null float64 6 std_test_accuracy_score 1350 non-null float64 7 rank_test_accuracy_score 1350 non-null int32 8 mean_test_f1_score 1350 non-null float64 9 std_test_f1_score 1350 non-null float64 10 rank_test_f1_score 1350 non-null int32 11 mean_test_roc_auc_score 1350 non-null float64 12 std_test_roc_auc_score 1350 non-null float64 13 rank_test_roc_auc_score 1350 non-null int32 14 param_max_features 1350 non-null object 15 param_n_estimators 1350 non-null object 16 param_max_depth 1350 non-null object 17 variable 1350 non-null object 18 value 1350 non-null float64 dtypes: float64(11), int32(3), object(5) memory usage: 184.7+ KB df_results_RF [[ 'params' , 'variable' , 'value' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } params variable value 0 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.791698 1 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.832963 2 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.835434 3 {'max_depth': 3, 'max_features': 'log2', 'n_es... split0_test_accuracy_score 0.846306 4 {'max_depth': 3, 'max_features': 'log2', 'n_es... split0_test_accuracy_score 0.843094 ... ... ... ... 1345 {'max_depth': 8, 'max_features': 'sqrt', 'n_es... split24_test_roc_auc_score 0.915609 1346 {'max_depth': 8, 'max_features': 'sqrt', 'n_es... split24_test_roc_auc_score 0.917331 1347 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.907197 1348 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.919056 1349 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.920050 1350 rows \u00d7 3 columns Agora, basta filtrar cada configura\u00e7\u00e3o dos par\u00e2metros e cada m\u00e9trica para que tenhamos os resultados de cada experimento realizado. def valores ( modelo , parametros , metrica ): col1 , param1 , col2 , param2 , col3 , param3 = parametros val = modelo [( modelo [ \"param_\" + col1 ] == param1 ) & ( modelo [ \"param_\" + col2 ] == param2 ) & ( modelo [ \"param_\" + col3 ] == param3 )] val = val . loc [ val . variable . str . contains ( f '[a-zA-Z]+[0-9]+_[a-zA-Z]+_ { metrica } +' ), 'value' ] return val metrica = [ * metricas . keys ()] dict_acuracia , dict_f1 , dict_roc = {}, {}, {} parametros_combinacoes = list ( df_results_RF [ 'params' ] . value_counts () . index ) for combinacao in [ i . items () for i in parametros_combinacoes ]: # param = (nome, valor) param1 , param2 , param3 = list ( combinacao ) parametros = [ param1 [ 0 ], param1 [ 1 ], param2 [ 0 ], param2 [ 1 ], param3 [ 0 ], param3 [ 1 ]] nome_coluna = f \" { param1 [ 0 ][: 6 ] } _ { param1 [ 1 ] } - { param2 [ 0 ][: 6 ] } _ { param2 [ 1 ] } - { param3 [ 0 ][: 6 ] } _ { param3 [ 1 ] } \" dict_acuracia [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 0 ]) dict_f1 [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 1 ]) dict_roc [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 2 ]) df_acuracia , df_f1 , df_roc = pd . DataFrame ( dict_acuracia ), pd . DataFrame ( dict_f1 ), pd . DataFrame ( dict_roc ) # exemplo de como est\u00e3o sendo armazenados os resultados df_acuracia . info () <class 'pandas.core.frame.DataFrame'> Int64Index: 450 entries, 0 to 449 Data columns (total 18 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 max_de_8-max_fe_log2-n_esti_10 25 non-null float64 1 max_de_5-max_fe_log2-n_esti_300 25 non-null float64 2 max_de_3-max_fe_log2-n_esti_100 25 non-null float64 3 max_de_5-max_fe_sqrt-n_esti_300 25 non-null float64 4 max_de_8-max_fe_sqrt-n_esti_10 25 non-null float64 5 max_de_8-max_fe_log2-n_esti_100 25 non-null float64 6 max_de_3-max_fe_sqrt-n_esti_100 25 non-null float64 7 max_de_3-max_fe_log2-n_esti_300 25 non-null float64 8 max_de_5-max_fe_log2-n_esti_10 25 non-null float64 9 max_de_8-max_fe_sqrt-n_esti_100 25 non-null float64 10 max_de_8-max_fe_log2-n_esti_300 25 non-null float64 11 max_de_3-max_fe_sqrt-n_esti_300 25 non-null float64 12 max_de_5-max_fe_sqrt-n_esti_10 25 non-null float64 13 max_de_5-max_fe_log2-n_esti_100 25 non-null float64 14 max_de_8-max_fe_sqrt-n_esti_300 25 non-null float64 15 max_de_3-max_fe_log2-n_esti_10 25 non-null float64 16 max_de_5-max_fe_sqrt-n_esti_100 25 non-null float64 17 max_de_3-max_fe_sqrt-n_esti_10 25 non-null float64 dtypes: float64(18) memory usage: 66.8 KB #plot acur\u00e1cia fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_acuracia , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"Acur\u00e1cia\" ) plt . xticks ( rotation =- 90 ) plt . show () #plot f1 score fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_f1 , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"F1\" ) plt . xticks ( rotation =- 90 ) plt . show () #plot \u00e1rea sob a curva ROC fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_roc , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"AUC ROC\" ) plt . xticks ( rotation =- 90 ) plt . show () Observando cada configura\u00e7\u00e3o, os valores para cada uma das tr\u00eas m\u00e9tricas s\u00e3o semelhantes. \u00c9 importante deixar claro que n\u00e3o h\u00e1 um significado espec\u00edfico para isso. Conhecendo as express\u00f5es matem\u00e1ticas de cada uma dessas m\u00e9tricas, esse comportamento \u00e9 previsto quando os valores de Falso Positivo e Falso Negativo s\u00e3o iguais (ou perto disso) e pequenos. Com isso, a configura\u00e7\u00e3o escolhida \u00e9 a com melhor desempenho (considerando a distribui\u00e7\u00e3o pr\u00f3ximo a 1) em todas (ou na maioria) das m\u00e9tricas. Sendo assim, foi escolhida a configura\u00e7\u00e3o com max_depth=8, n_estimators=300 e max_features='sqrt'. PARTE 13: Investigar o melhor modelo best_model = RandomForestClassifier ( max_depth = 8 , n_estimators = 300 , max_features = 'sqrt' ) . fit ( X_train_over , y_train_over ) A primeira abordagem escolhida como forma de entender quais atributos s\u00e3o considerados mais relevantes na classifica\u00e7\u00e3o do modelo com melhor desempenho foi a de observar a Import\u00e2ncia da Permuta\u00e7\u00e3o. Este m\u00e9todo embaralha aleatoriamente cada atributo e calcula a mudan\u00e7a no desempenho do modelo. Ent\u00e3o, quanto mais uma mudan\u00e7a em um determinado atributo impacta no desempenho do modelo, mais importante este atributo \u00e9. perm_importance = permutation_importance ( best_model , X_test , y_test , n_repeats = 10 ) best_model_importances = pd . Series ( perm_importance . importances_mean , index = X_test . columns ) best_model_importances = best_model_importances . sort_values ( ascending = True ) fig , ax = plt . subplots () best_model_importances . plot . barh ( yerr = perm_importance . importances_std , ax = ax , figsize = ( 15 , 9 )) ax . set_title ( \"Import\u00e2ncia da Permuta\u00e7\u00e3o (dados de treino)\" ) fig . tight_layout () plt . show () Por\u00e9m, um problema foi notado: comparando o resultado acima com o teste de independ\u00eancia realizado na PARTE 7, \u00e9 poss\u00edvel observar que os dois atributos que aparentam ter maior rela\u00e7\u00e3o com a vari\u00e1vel alvo ('BasePolicy_Liability' e 'PolicyType_Sedan - Liability'), aparecem com uma import\u00e2ncia baix\u00edssima. Isso se d\u00e1 por conta de uma limita\u00e7\u00e3o da abordagem escolhida. A Import\u00e2ncia da Permuta\u00e7\u00e3o n\u00e3o performa bem caso existam atributos que n\u00e3o sejam totalmente independentes e que, de alguma forma, se correlacionem. E, como pode ser visto abaixo, estes dois atributos citados podem ser considerados colineares (se correlacionando de alguma forma, mas n\u00e3o necessariamente com uma rela\u00e7\u00e3o estritamente linear). pd . crosstab ( df_final [ 'PolicyType_Sedan - Liability' ], df_final [ 'BasePolicy_Liability' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BasePolicy_Liability 0 1 PolicyType_Sedan - Liability 0 10410 22 1 0 4987 Uma outra forma de entender quais atributos s\u00e3o considerados mais relevantes na classifica\u00e7\u00e3o do modelo com melhor desempenho \u00e9 com a Import\u00e2ncia da Caracter\u00edstica da Floresta Aleat\u00f3ria, um recurso baseado em impureza. O princ\u00edpio desse m\u00e9todo \u00e9 baseado no crit\u00e9rio considerado na divis\u00e3o dos n\u00f3s em uma \u00e1rvore de decis\u00e3o, onde a 'impureza' se refere a uma m\u00e9trica usada para determinar como (usando qual atributo e em qual limite) dividir os dados em grupos menores. Se o atributo \u00e9 \u00fatil, este tende a dividir n\u00f3s rotulados mistos em n\u00f3s puros de classe \u00fanica. Ent\u00e3o, podemos medir como cada atributo diminui a impureza da divis\u00e3o (o recurso com maior diminui\u00e7\u00e3o \u00e9 selecionado para o n\u00f3 interno). Para cada atributo, podemos coletar como, em m\u00e9dia, diminui a impureza. A m\u00e9dia de todas as \u00e1rvores \u00e9 a medida da import\u00e2ncia da caracter\u00edstica. importances = best_model . feature_importances_ best_model_importances = pd . Series ( importances , index = X_train_over . columns ) best_model_importances = best_model_importances . sort_values ( ascending = True ) fig , ax = plt . subplots () best_model_importances . plot . barh ( ax = ax , figsize = ( 17 , 9 )) ax . set_title ( \"Import\u00e2ncia da Caracter\u00edstica da Floresta Aleat\u00f3ria - Diminui\u00e7\u00e3o M\u00e9dia de Impureza (MDI)\" ) fig . tight_layout () Como todos os valores necess\u00e1rios s\u00e3o calculados durante o treinamento, este m\u00e9todo \u00e9 executado rapidamente. Al\u00e9m disso, \u00e9 poss\u00edvel observar um comportamento mais semelhante com o resultado obtido no teste de independ\u00eancia da PARTE 7. Por\u00e9m, \u00e9 importante salientar que este m\u00e9todo tem a tend\u00eancia de preferir (selecionar como importantes) atributos num\u00e9ricos e atributos categ\u00f3ricos com alta cardinalidade (muitos valores exclusivos, como era o caso de 'PolicyNumber', exclu\u00eddo no tratamento dos dados). E, no caso de atributos correlacionados, pode-se selecionar apenas um dos atributos e negligenciar a import\u00e2ncia do outro (o que pode levar a conclus\u00f5es erradas). \u00c9 importante entender que qualquer abordagem escolhida traz consigo alguma limita\u00e7\u00e3o. Para este projeto, apesar das desvantagens comentadas, o segundo m\u00e9todo apresentou o melhor desempenho, corroborando com algumas an\u00e1lises realizadas anteriormente e permitindo uma maior compreens\u00e3o do problema. Agora, com o modelo treinado (usando dados balanceados), verificaremos a performance com os dados de teste (que mant\u00eam a propor\u00e7\u00e3o original). y_pred = best_model . predict ( X_test ) matriz_confusao = metrics . confusion_matrix ( y_test , y_pred ) ax = sns . heatmap ( matriz_confusao , annot = True , fmt = 'g' , cmap = 'Blues' ) ax . set ( xlabel = ' \\n Predi\u00e7\u00e3o' , ylabel = 'Real' ) ax . set ( xticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ], yticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ]) plt . show () ax1 = sns . heatmap ( matriz_confusao / np . sum ( matriz_confusao ), annot = True , cmap = 'Blues' ) ax1 . set ( xlabel = ' \\n Predi\u00e7\u00e3o' , ylabel = 'Real' ) ax1 . set ( xticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ], yticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ]) plt . show () print ( f \"acur\u00e1cia: { metrics . accuracy_score ( y_test , y_pred ) } \" ) print ( f \"precis\u00e3o: { metrics . precision_score ( y_test , y_pred ) } \" ) print ( f \"recall: { metrics . recall_score ( y_test , y_pred ) } \" ) print ( f \"f1-score: { metrics . f1_score ( y_test , y_pred ) } \" ) print ( f \"AUC ROC: { metrics . roc_auc_score ( y_test , y_pred ) } \" ) acur\u00e1cia: 0.8495460440985733 precis\u00e3o: 0.16264090177133655 recall: 0.36462093862815886 f1-score: 0.22494432071269488 AUC ROC: 0.6225266109558362 PARTE 14: Conclus\u00e3o e discuss\u00e3o Nota-se um desempenho diferente, em geral menor, quando observados os resultados de teste, comparando-os com os resultados da valida\u00e7\u00e3o cruzada. Por\u00e9m, isso \u00e9 esperado, pois os dados utilizados na valida\u00e7\u00e3o cruzada e no teste s\u00e3o diferentes e, ainda mais importante, os dados de teste apresentam o desbalanceamento real de solicita\u00e7\u00f5es identificadas ou n\u00e3o como fraudulentas. Dessa forma, conseguimos verificar se, de fato, o modelo aprendeu a generalizar o problema. O que \u00e9 poss\u00edvel tirar de conclus\u00e3o a partir dos resultados: a pessoa considerada culpada pelo acidente (titular da ap\u00f3lice ou terceiro) demonstra ser bastante importante; o tipo de ap\u00f3lice contratada se mostra bem relevante para algumas das suas poss\u00edveis combina\u00e7\u00f5es, como para o tipo de seguro 'Liability' (indeniza\u00e7\u00e3o \u00e0 terceiros quando o titular da ap\u00f3lice \u00e9 o culpado) e para as categorias de ve\u00edculo 'Sedan' e 'Sport'; em rela\u00e7\u00e3o ao hist\u00f3rico do titular da ap\u00f3lice vemos 'AddressChange_Claim' e 'PastNumberOfClaims' como fatores importantes. Por\u00e9m, 'Days_Policy_Accident' e 'Days_Policy_Claim' n\u00e3o parecem ser t\u00e3o significativos; j\u00e1 sobre as caracter\u00edsticas pessoais do titular da ap\u00f3lice/condutor, observa-se uma relev\u00e2ncia maior para 'Sex' e um pouco menos para 'Age' e 'MaritalStatus'; alguns fatores mostraram pouca relev\u00e2ncia para o problema, como: presen\u00e7a de testemunha no momento do acidente ('WitnessPresent'), realiza\u00e7\u00e3o de boletim de ocorr\u00eancia para o acontecido ('PoliceReportFiled') e a \u00e1rea em que o acidente ocorreu ('AccidentArea').","title":"Detec\u00e7\u00e3o de fraude"},{"location":"projects/ds/fraud-detection/#deteccao-de-fraude-em-seguro-de-veiculo","text":"","title":"Detec\u00e7\u00e3o de fraude em seguro de ve\u00edculo"},{"location":"projects/ds/fraud-detection/#sobre","text":"","title":"Sobre"},{"location":"projects/ds/fraud-detection/#objetivo","text":"Detectar e evitar processos fraudulentos \u00e9 um enorme desafio. Para tal, o uso de t\u00e9cnicas de machine learning tem se mostrado bastante promissor. Por\u00e9m, um bom desempenho de um modelo de machine learning depende de uma etapa que n\u00e3o \u00e9 simples: entender e tratar os diversos dados de diferentes naturezas que est\u00e3o em posse das seguradoras. E este \u00e9 o foco principal do projeto: entender cada vari\u00e1vel disponibilizada na base de dados e trat\u00e1-las adequadamente. Dessa forma, \u00e9 poss\u00edvel utiliz\u00e1-las no desenvolvimento de algoritmos de classifica\u00e7\u00e3o. O que tamb\u00e9m ser\u00e1 realizado, escolhendo alguns m\u00e9todos existentes e avaliando diferentes configura\u00e7\u00f5es destes.","title":"Objetivo"},{"location":"projects/ds/fraud-detection/#base-de-dados","text":"A base de dados escolhida cont\u00e9m diversas informa\u00e7\u00f5es sobre pedidos de indeniza\u00e7\u00e3o de uma seguradora de ve\u00edculos. As informa\u00e7\u00f5es se referem \u00e0 pessoa envolvida em um acidente, \u00e0 ap\u00f3lice contratada, ao titular da ap\u00f3lice, ao ve\u00edculo envolvido, etc. Originalmente s\u00e3o 15420 registros, onde o atributo 'FraudFound_P' indica se uma determinada solicita\u00e7\u00e3o foi ou n\u00e3o identificada como fraudulenta. Dispon\u00edvel em: https://www.kaggle.com/datasets/shivamb/vehicle-claim-fraud-detection","title":"Base de dados"},{"location":"projects/ds/fraud-detection/#parte-1-importar-bibliotecas","text":"import numpy as np # realizar c\u00e1lculos em arrays multidimensionais import pandas as pd # manipula\u00e7\u00e3o e an\u00e1lise de dados import seaborn as sns # visualiza\u00e7\u00e3o de gr\u00e1ficos estat\u00edsticos import scipy.stats as stats # import matplotlib.pyplot as plt # cria\u00e7\u00e3o de gr\u00e1ficos e visualiza\u00e7\u00f5es de dados em geral from imblearn.over_sampling import ADASYN # m\u00e9todo de balanceamento ADASYN from sklearn.model_selection import train_test_split # dividir a base de dados em treino e teste from sklearn.ensemble import RandomForestClassifier # m\u00e9todo de classifica\u00e7\u00e3o Random Forest from sklearn.model_selection import RepeatedStratifiedKFold # m\u00e9todo de valida\u00e7\u00e3o cruzada from sklearn.model_selection import GridSearchCV # testar diferentes valores de par\u00e2metros para um m\u00e9todo from sklearn.inspection import permutation_importance # visualizar relav\u00e2ncia dos atributos em um m\u00e9todo from sklearn import metrics # importar m\u00e9tricas de avalia\u00e7\u00e3o # a sa\u00edda dos comandos de plotagem \u00e9 exibida diretamente abaixo da c\u00e9lula % matplotlib inline","title":"PARTE 1: Importar bibliotecas"},{"location":"projects/ds/fraud-detection/#parte-2-importar-base-de-dados","text":"url_dataset = \"https://github.com/peuvitor/insurance-fraud-detection/blob/main/dataset/fraud_oracle.csv?raw=true\" df_dataset = pd . read_csv ( url_dataset )","title":"PARTE 2: Importar base de dados"},{"location":"projects/ds/fraud-detection/#parte-3-entendimento-da-base-de-dados","text":"","title":"PARTE 3: Entendimento da base de dados"},{"location":"projects/ds/fraud-detection/#31-investigar-as-colunas-existentes","text":"Quantas e quais s\u00e3o as colunas? Quais s\u00e3o os seus respectivos tipos? df_dataset . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 15420 entries, 0 to 15419 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Month 15420 non-null object 1 WeekOfMonth 15420 non-null int64 2 DayOfWeek 15420 non-null object 3 Make 15420 non-null object 4 AccidentArea 15420 non-null object 5 DayOfWeekClaimed 15420 non-null object 6 MonthClaimed 15420 non-null object 7 WeekOfMonthClaimed 15420 non-null int64 8 Sex 15420 non-null object 9 MaritalStatus 15420 non-null object 10 Age 15420 non-null int64 11 Fault 15420 non-null object 12 PolicyType 15420 non-null object 13 VehicleCategory 15420 non-null object 14 VehiclePrice 15420 non-null object 15 FraudFound_P 15420 non-null int64 16 PolicyNumber 15420 non-null int64 17 RepNumber 15420 non-null int64 18 Deductible 15420 non-null int64 19 DriverRating 15420 non-null int64 20 Days_Policy_Accident 15420 non-null object 21 Days_Policy_Claim 15420 non-null object 22 PastNumberOfClaims 15420 non-null object 23 AgeOfVehicle 15420 non-null object 24 AgeOfPolicyHolder 15420 non-null object 25 PoliceReportFiled 15420 non-null object 26 WitnessPresent 15420 non-null object 27 AgentType 15420 non-null object 28 NumberOfSuppliments 15420 non-null object 29 AddressChange_Claim 15420 non-null object 30 NumberOfCars 15420 non-null object 31 Year 15420 non-null int64 32 BasePolicy 15420 non-null object dtypes: int64(9), object(24) memory usage: 3.9+ MB","title":"3.1. Investigar as colunas existentes"},{"location":"projects/ds/fraud-detection/#32-valores-unicos-de-cada-coluna","text":"Verificar quais os poss\u00edveis valores encontrados na base de dados para cada atributo. Al\u00e9m disso, essa parte ser\u00e1 \u00fatil para identificar poss\u00edveis valores faltantes ou incomuns. for coluna in df_dataset . columns : valores_unicos = df_dataset [ coluna ] . unique () print ( f \"--> ' { coluna } ' possui { len ( valores_unicos ) } valores \u00fanicos, s\u00e3o eles: \\n { valores_unicos } \" ) --> 'Month' possui 12 valores \u00fanicos, s\u00e3o eles: ['Dec' 'Jan' 'Oct' 'Jun' 'Feb' 'Nov' 'Apr' 'Mar' 'Aug' 'Jul' 'May' 'Sep'] --> 'WeekOfMonth' possui 5 valores \u00fanicos, s\u00e3o eles: [5 3 2 4 1] --> 'DayOfWeek' possui 7 valores \u00fanicos, s\u00e3o eles: ['Wednesday' 'Friday' 'Saturday' 'Monday' 'Tuesday' 'Sunday' 'Thursday'] --> 'Make' possui 19 valores \u00fanicos, s\u00e3o eles: ['Honda' 'Toyota' 'Ford' 'Mazda' 'Chevrolet' 'Pontiac' 'Accura' 'Dodge' 'Mercury' 'Jaguar' 'Nisson' 'VW' 'Saab' 'Saturn' 'Porche' 'BMW' 'Mecedes' 'Ferrari' 'Lexus'] --> 'AccidentArea' possui 2 valores \u00fanicos, s\u00e3o eles: ['Urban' 'Rural'] --> 'DayOfWeekClaimed' possui 8 valores \u00fanicos, s\u00e3o eles: ['Tuesday' 'Monday' 'Thursday' 'Friday' 'Wednesday' 'Saturday' 'Sunday' '0'] --> 'MonthClaimed' possui 13 valores \u00fanicos, s\u00e3o eles: ['Jan' 'Nov' 'Jul' 'Feb' 'Mar' 'Dec' 'Apr' 'Aug' 'May' 'Jun' 'Sep' 'Oct' '0'] --> 'WeekOfMonthClaimed' possui 5 valores \u00fanicos, s\u00e3o eles: [1 4 2 3 5] --> 'Sex' possui 2 valores \u00fanicos, s\u00e3o eles: ['Female' 'Male'] --> 'MaritalStatus' possui 4 valores \u00fanicos, s\u00e3o eles: ['Single' 'Married' 'Widow' 'Divorced'] --> 'Age' possui 66 valores \u00fanicos, s\u00e3o eles: [21 34 47 65 27 20 36 0 30 42 71 52 28 61 38 41 32 40 63 31 45 60 39 55 35 44 72 29 37 59 49 50 26 48 64 33 74 23 25 56 16 68 18 51 22 53 46 43 57 54 69 67 19 78 77 75 80 58 73 24 76 62 79 70 17 66] --> 'Fault' possui 2 valores \u00fanicos, s\u00e3o eles: ['Policy Holder' 'Third Party'] --> 'PolicyType' possui 9 valores \u00fanicos, s\u00e3o eles: ['Sport - Liability' 'Sport - Collision' 'Sedan - Liability' 'Utility - All Perils' 'Sedan - All Perils' 'Sedan - Collision' 'Utility - Collision' 'Utility - Liability' 'Sport - All Perils'] --> 'VehicleCategory' possui 3 valores \u00fanicos, s\u00e3o eles: ['Sport' 'Utility' 'Sedan'] --> 'VehiclePrice' possui 6 valores \u00fanicos, s\u00e3o eles: ['more than 69000' '20000 to 29000' '30000 to 39000' 'less than 20000' '40000 to 59000' '60000 to 69000'] --> 'FraudFound_P' possui 2 valores \u00fanicos, s\u00e3o eles: [0 1] --> 'PolicyNumber' possui 15420 valores \u00fanicos, s\u00e3o eles: [ 1 2 3 ... 15418 15419 15420] --> 'RepNumber' possui 16 valores \u00fanicos, s\u00e3o eles: [12 15 7 4 3 14 1 13 11 16 6 2 8 5 9 10] --> 'Deductible' possui 4 valores \u00fanicos, s\u00e3o eles: [300 400 500 700] --> 'DriverRating' possui 4 valores \u00fanicos, s\u00e3o eles: [1 4 3 2] --> 'Days_Policy_Accident' possui 5 valores \u00fanicos, s\u00e3o eles: ['more than 30' '15 to 30' 'none' '1 to 7' '8 to 15'] --> 'Days_Policy_Claim' possui 4 valores \u00fanicos, s\u00e3o eles: ['more than 30' '15 to 30' '8 to 15' 'none'] --> 'PastNumberOfClaims' possui 4 valores \u00fanicos, s\u00e3o eles: ['none' '1' '2 to 4' 'more than 4'] --> 'AgeOfVehicle' possui 8 valores \u00fanicos, s\u00e3o eles: ['3 years' '6 years' '7 years' 'more than 7' '5 years' 'new' '4 years' '2 years'] --> 'AgeOfPolicyHolder' possui 9 valores \u00fanicos, s\u00e3o eles: ['26 to 30' '31 to 35' '41 to 50' '51 to 65' '21 to 25' '36 to 40' '16 to 17' 'over 65' '18 to 20'] --> 'PoliceReportFiled' possui 2 valores \u00fanicos, s\u00e3o eles: ['No' 'Yes'] --> 'WitnessPresent' possui 2 valores \u00fanicos, s\u00e3o eles: ['No' 'Yes'] --> 'AgentType' possui 2 valores \u00fanicos, s\u00e3o eles: ['External' 'Internal'] --> 'NumberOfSuppliments' possui 4 valores \u00fanicos, s\u00e3o eles: ['none' 'more than 5' '3 to 5' '1 to 2'] --> 'AddressChange_Claim' possui 5 valores \u00fanicos, s\u00e3o eles: ['1 year' 'no change' '4 to 8 years' '2 to 3 years' 'under 6 months'] --> 'NumberOfCars' possui 5 valores \u00fanicos, s\u00e3o eles: ['3 to 4' '1 vehicle' '2 vehicles' '5 to 8' 'more than 8'] --> 'Year' possui 3 valores \u00fanicos, s\u00e3o eles: [1994 1995 1996] --> 'BasePolicy' possui 3 valores \u00fanicos, s\u00e3o eles: ['Liability' 'Collision' 'All Perils']","title":"3.2. Valores \u00fanicos de cada coluna"},{"location":"projects/ds/fraud-detection/#33-resumo-sobre-as-colunas","text":"Breve entendimento de cada atributo. Levando em conta o contexto do problema, em geral o nome da coluna \u00e9 autoexplicativo, por\u00e9m, em alguns momentos pode dar margem \u00e0 mais de uma interpreta\u00e7\u00e3o. Aqui explicarei o que \u00e9 cada atributo, como trabalharei com ele e problemas que precisam ser investigados. Month (object): considerarei como o m\u00eas em que o acidente ocorreu m\u00eas abreviado (3 primeiras letras, em ingl\u00eas) cont\u00e9m os 12 poss\u00edveis meses, n\u00e3o h\u00e1 valores faltantes nem incompat\u00edveis WeekOfMonth (int64): considerarei como a semana do m\u00eas em que o acidente ocorreu cont\u00e9m valores inteiros de 1 a 5 DayOfWeek (object): considerarei como o dia da semana em que o acidente ocorreu nome do dia da semana completo, em ingl\u00eas cont\u00e9m os 7 poss\u00edveis dias da semana, n\u00e3o h\u00e1 valores faltantes nem incompat\u00edveis Make (object): considerarei como a marca do ve\u00edculo envolvido no acidente cont\u00e9m uma lista com 19 fabricantes AccidentArea (object): informa se o acidente ocorreu em uma \u00e1rea rural ou urbana DayOfWeekClaimed (object): dia da semana referente ao pedido de seguro nome do dia da semana completo, em ingl\u00eas cont\u00e9m 8 valores \u00fanicos (7 poss\u00edveis dias da semana + '0'), a seguir ser\u00e3o investigados mais a fundo MonthClaimed (object): m\u00eas referente ao pedido de seguro m\u00eas abreviado (3 primeiras letras, em ingl\u00eas) cont\u00e9m 13 valores \u00fanicos (12 poss\u00edves meses + '0'), a seguir ser\u00e3o investigados mais a fundo WeekOfMonthClaimed (int64): semana do m\u00eas referente ao pedido de seguro cont\u00e9m valores inteiros de 1 a 5 Sex (object): \u00e9 o sexo biol\u00f3gico da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) pode ser: sexo masculino ou feminino MaritalStatus (object): \u00e9 o estado civil da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) pode ser: solteiro, casado, divorciado ou vi\u00favo Age (int64): \u00e9 a idade da pessoa envolvida no acidente ou da pessoa solicitando o seguro? (considerarei a primeira op\u00e7\u00e3o) valores inteiros de 16 a 80 (a seguir, o valor '0' ser\u00e1 investigado mais a fundo) Fault (object): indica quem foi considerado como o respons\u00e1vel pelo acidente pode ser: titular do seguro ou terceiro PolicyType (object): indica o tipo da ap\u00f3lice contratada cont\u00e9m uma lista com 9 tipos, onde cada um \u00e9 composto por 'categoria do ve\u00edculo' + 'tipo do seguro' categoria do ve\u00edculo: sport, sedan, utility tipo de seguro: liability, all perils, collision j\u00e1 existe um atributo para indicar a categoria do ve\u00edculo (VehicleCategory) e outra para indicar o tipo de seguro (BasePolicy), a seguir verificarei como tratar essas informa\u00e7\u00f5es duplicadas VehicleCategory (object): indica a categoria do ve\u00edculo registrado na ap\u00f3lice/envolvido no acidente pode ser: sport, sedan, utility VehiclePrice (object): indica o pre\u00e7o do ve\u00edculo envolvido no acidente cont\u00e9m 6 faixas de valores (provavelmente em d\u00f3lares) duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio FraudFound_P (int64): indica se aquela solicita\u00e7\u00e3o foi ou n\u00e3o uma fraude \u00e9 o atributo alvo PolicyNumber (int64): identifica cada pedido de seguro cont\u00e9m 15420 valores inteiros \u00fanicos, de 1 a 15420 a seguir ser\u00e1 verificada a rela\u00e7\u00e3o entre o n\u00famero da linha e o PolicyNumber, possivelmente s\u00e3o a mesma coisa RepNumber (int64): indica o n\u00famero do representante cont\u00e9m valores inteiros de 1 a 16 n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado Deductible (int64): indica o valor da franquia do seguro 4 op\u00e7\u00f5es de valores inteiros: 300, 400, 500 e 700 DriverRating (int64): alguma m\u00e9trica de avalia\u00e7\u00e3o do motorista cont\u00e9m valores inteiros de 1 a 4 n\u00e3o fica claro se esses dados se traduzem em categorias ou se est\u00e3o dentro de um certo intervalo de avalia\u00e7\u00e3o Days_Policy_Accident (object): ser\u00e1 considerado como o n\u00famero de dias que se passaram entre a compra da ap\u00f3lice e o acidente cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio Days_Policy_Claim (object): ser\u00e1 considerado como o n\u00famero de dias que se passaram entre a compra da ap\u00f3lice e o pedido de indeniza\u00e7\u00e3o cont\u00e9m 4 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio PastNumberOfClaims (object): indica quantas vezes a mesma pessoa realizou pedidos de indeniza\u00e7\u00e3o cont\u00e9m 4 faixas de valores AgeOfVehicle (object): indica a idade do ve\u00edculo (considerarei referenre ao momento do acidente) cont\u00e9m 8 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio AgeOfPolicyHolder (object): indica a idade do titular da ap\u00f3lice cont\u00e9m 9 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio PoliceReportFiled (object): indica se foi feito um boletim de ocorr\u00eancia para o acidente pode ser: sim ou n\u00e3o WitnessPresent (object): indica se uma testemunha estava presente no momento do acidente pode ser: sim ou n\u00e3o AgentType (object): classifica um agente (relacionado ao acidente ou ao pedido) como externo ou interno n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado NumberOfSuppliments (object): indica algum tipo de suplemento no seguro cont\u00e9m 4 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio n\u00e3o fica claro a relev\u00e2ncia do par\u00e2metro para o problema, a seguir isso ser\u00e1 investigado AddressChange_Claim (object): indica quantos anos se passaram entre a \u00faltima vez que foi registrado uma mudan\u00e7a de endere\u00e7o e o pedido de indeniza\u00e7\u00e3o cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio NumberOfCars (object): considerarei como o n\u00famero de carros cobertos pela ap\u00f3lice (outra possibilidade seria o n\u00famero de carros envolvidos no acidente) cont\u00e9m 5 faixas de valores duas abordagens ser\u00e3o avaliadas: considerar como um dado categ\u00f3rico ou substituir os intervalos pelo valor m\u00e9dio Year (int64): considerarei como o ano em que o acidente ocorreu pode ser: 1994, 1995 e 1996 BasePolicy (object): indica o tipo de seguro contratado pode ser: liability, all perils, collision","title":"3.3. Resumo sobre as colunas"},{"location":"projects/ds/fraud-detection/#parte-4-tratamento-dos-dados","text":"df_dataset_1 = df_dataset . copy ()","title":"PARTE 4: Tratamento dos dados"},{"location":"projects/ds/fraud-detection/#41-verificar-relacao-entre-os-indices-das-linhas-do-dataset-e-policynumber","text":"PolicyNumber \u00e9 igual ao \u00edndice da linha+1? df_dataset_1 [ 'PolicyNumber' ] . describe <bound method NDFrame.describe of 0 1 1 2 2 3 3 4 4 5 ... 15415 15416 15416 15417 15417 15418 15418 15419 15419 15420 Name: PolicyNumber, Length: 15420, dtype: int64> df_dataset_total_linhas = len ( df_dataset_1 ) PolicyNumber_e_indice = sum ( 1 if df_dataset_1 [ 'PolicyNumber' ] . iloc [ indice ] == indice + 1 else 0 \\ for indice in range ( df_dataset_total_linhas )) print ( f \"N\u00famero de linhas do dataset: { df_dataset_total_linhas } \" ) print ( f \"Quantidade de linhas onde PolicyNumber = indice+1: { PolicyNumber_e_indice } \" ) N\u00famero de linhas do dataset: 15420 Quantidade de linhas onde PolicyNumber = indice+1: 15420 Como a suposi\u00e7\u00e3o foi provada verdadeira, esta coluna ser\u00e1 ignorada, pois n\u00e3o apresenta utilidade para o problema em quest\u00e3o. df_dataset_1 . drop ( columns = 'PolicyNumber' , inplace = True )","title":"4.1. Verificar rela\u00e7\u00e3o entre os \u00edndices das linhas do dataset e 'PolicyNumber'"},{"location":"projects/ds/fraud-detection/#42-tratar-o-valor-0-encontrado-em-dayofweekclaimed-monthclaimed-e-age","text":"##### 'DayOfWeekClaimed' e 'MonthClaimed' DayOfWeekClaimed_zeros = df_dataset_1 . index [ df_dataset_1 [ 'DayOfWeekClaimed' ] == '0' ] . tolist () MonthClaimed_zeros = df_dataset_1 . index [ df_dataset_1 [ 'MonthClaimed' ] == '0' ] . tolist () print ( \"DayOfWeekClaimed e MonthClaimed possuem o valor '0' na(s) mesma(s) linha(s):\" , DayOfWeekClaimed_zeros == MonthClaimed_zeros ) print ( \"\u00cdndice(s):\" , DayOfWeekClaimed_zeros ) # como trata-se apenas de uma \u00fanica ocorr\u00eancia, a linha correspondente ser\u00e1 eliminada df_dataset_1 = df_dataset_1 . drop ( DayOfWeekClaimed_zeros ) df_dataset_1 . reset_index ( drop = True , inplace = True ) DayOfWeekClaimed e MonthClaimed possuem o valor '0' na(s) mesma(s) linha(s): True \u00cdndice(s): [1516] ##### 'Age' Age_zeros = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 0 ] . tolist () print ( f \"Age possui o valor '0' em { len ( Age_zeros ) } linhas. \\n \" ) # partirei do seguinte questionamento: nessas ocasi\u00f5es, existe alguma rela\u00e7\u00e3o com outra coluna que pode ser \u00fatil? for coluna in df_dataset_1 . columns : linhas_age0 = df_dataset_1 . loc [ df_dataset_1 [ 'Age' ] == 0 , coluna ] . unique () if len ( linhas_age0 ) < 2 and coluna != 'Age' : print ( f \"--> Correspond\u00eancia para ' { coluna } ': { linhas_age0 } \" ) Age possui o valor '0' em 319 linhas. --> Correspond\u00eancia para 'Sex': ['Male'] --> Correspond\u00eancia para 'MaritalStatus': ['Single'] --> Correspond\u00eancia para 'Days_Policy_Accident': ['more than 30'] --> Correspond\u00eancia para 'Days_Policy_Claim': ['more than 30'] --> Correspond\u00eancia para 'AgeOfPolicyHolder': ['16 to 17'] df_dataset_1 [ 'AgeOfPolicyHolder' ] . value_counts () . sort_index () 16 to 17 319 18 to 20 15 21 to 25 108 26 to 30 613 31 to 35 5593 36 to 40 4043 41 to 50 2828 51 to 65 1392 over 65 508 Name: AgeOfPolicyHolder, dtype: int64 Verificando as colunas que possuem sempre a mesma correspond\u00eancia, \u00e0 primeira vista, aquela que pode dar uma solu\u00e7\u00e3o para Age==0 \u00e9 'AgeOfPolicyHolder'. E, de fato, considerando todo o dataset, temos uma frequ\u00eancia de 319 para '16 to 17' em AgeOfPolicyHolder. Exatamente a mesma quantidade de linhas com Age==0. Investigarei o seguinte: 'Age' sempre est\u00e1 dentro do intervalo de 'AgeOfPolicyHolder'? # extrair os limites dos intervalos em 'AgeOfPolicyHolder' get_intervalo_AgeOfPolicyHolder = lambda linha : [ int ( linha [: 2 ]), int ( linha [ - 2 :])] if 'to' in linha else [ int ( linha [ - 2 :]), 200 ] df_dataset_total_linhas = len ( df_dataset_1 ) total = 0 for indice in range ( df_dataset_total_linhas ): idade = df_dataset_1 [ 'Age' ] . iloc [ indice ] intervalo_idade = get_intervalo_AgeOfPolicyHolder ( df_dataset_1 [ 'AgeOfPolicyHolder' ] . iloc [ indice ]) if ~ ( idade >= intervalo_idade [ 0 ] and idade <= intervalo_idade [ 1 ]): total += 1 print ( f \"Em { total } linhas 'Age' n\u00e3o corresponde a 'AgeOfPolicyHolder'. O que representa { total / df_dataset_total_linhas : .2% } do total de linhas.\" ) Em 7241 linhas 'Age' n\u00e3o corresponde a 'AgeOfPolicyHolder'. O que representa 46.96% do total de linhas. A resposta para o questionamento \u00e9 'n\u00e3o'. Ou seja, o indiv\u00edduo envolvido no acidente n\u00e3o necessariamente \u00e9 a pessoa titular da ap\u00f3lice do seguro. Apesar disso, para n\u00e3o excluir essa quantidade de linhas do dataset, a op\u00e7\u00e3o escolhida para dar seguimento ao projeto \u00e9 a de substituir o valor de 0 de 'Age' pela m\u00e9dia do intervalo de 'AgeOfPolicyHolder'. df_dataset_1 . loc [ df_dataset_1 [ 'Age' ] == 0 , 'Age' ] = 16.5 Age_zeros = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 0 ] . tolist () print ( f \"Age possui o valor '0' em { len ( Age_zeros ) } linhas. \\n \" ) Age_mean = df_dataset_1 . index [ df_dataset_1 [ 'Age' ] == 16.5 ] . tolist () print ( f \"Age possui o valor '16.5' em { len ( Age_mean ) } linhas. \\n \" ) Age possui o valor '0' em 0 linhas. Age possui o valor '16.5' em 319 linhas.","title":"4.2. Tratar o valor 0 encontrado em 'DayOfWeekClaimed', 'MonthClaimed' e 'Age'"},{"location":"projects/ds/fraud-detection/#43-verificar-relacao-entre-policytype-vehiclecategory-e-basepolicy","text":"PolicyType = \"VehicleCategory - BasePolicy\"? df_dataset_1 [[ 'PolicyType' , 'VehicleCategory' , 'BasePolicy' ]] . describe <bound method NDFrame.describe of PolicyType VehicleCategory BasePolicy 0 Sport - Liability Sport Liability 1 Sport - Collision Sport Collision 2 Sport - Collision Sport Collision 3 Sedan - Liability Sport Liability 4 Sport - Collision Sport Collision ... ... ... ... 15414 Sedan - Collision Sedan Collision 15415 Sedan - Liability Sport Liability 15416 Sedan - Collision Sedan Collision 15417 Sedan - All Perils Sedan All Perils 15418 Sedan - Collision Sedan Collision [15419 rows x 3 columns]> df_dataset_total_linhas = len ( df_dataset_1 ) total = 0 for indice in range ( df_dataset_total_linhas ): PolicyType = df_dataset_1 [ 'PolicyType' ] . iloc [ indice ] VehicleCategory_BasePolicy = f \" { df_dataset_1 [ 'VehicleCategory' ] . iloc [ indice ] } - { df_dataset_1 [ 'BasePolicy' ] . iloc [ indice ] } \" if PolicyType == VehicleCategory_BasePolicy : total += 1 print ( f \"'PolicyType' corresponde a 'VehicleCategory - BasePolicy' em { total } linhas. O que representa { total / df_dataset_total_linhas : .2% } do total de linhas.\" ) 'PolicyType' corresponde a 'VehicleCategory - BasePolicy' em 10432 linhas. O que representa 67.66% do total de linhas. A suposi\u00e7\u00e3o foi provada falsa, portanto, nada ser\u00e1 feito com essas tr\u00eas colunas. Mesmo se a suposi\u00e7\u00e3o fosse verdadeira, ter as informa\u00e7\u00f5es tanto separadas quanto combinadas pode ser \u00fatil para o modelo a ser constru\u00eddo.","title":"4.3. Verificar rela\u00e7\u00e3o entre 'PolicyType', 'VehicleCategory' e 'BasePolicy'"},{"location":"projects/ds/fraud-detection/#parte-5-investigar-relacoes-com-a-variavel-alvo","text":"Usar de testes estat\u00edsticos e recursos gr\u00e1ficos para verificar as rela\u00e7\u00f5es entre os atributos e o target ('FraudFound_P').","title":"PARTE 5: Investigar rela\u00e7\u00f5es com a vari\u00e1vel alvo"},{"location":"projects/ds/fraud-detection/#51-teste-de-independencia","text":"Considerando os diferentes tipos de atributos (num\u00e9ricos, bin\u00e1rios, categ\u00f3ricos), a abordagem escolhida leva em conta a tabela de contig\u00eancia para cada par coluna e target. Ser\u00e1 realizado o teste de independ\u00eancia chi-quadrado, onde a hip\u00f3tese nula $H_0$ \u00e9 que as colunas n\u00e3o t\u00eam rela\u00e7\u00e3o e a hip\u00f3tese alternativa $H_1$ \u00e9 que existe rela\u00e7\u00e3o entre as colunas. O n\u00edvel de signific\u00e2ncia (alfa) considerado \u00e9 de 0,05. Para os resultados dos testes: Se o valor-p encontrado for maior que alfa , n\u00e3o rejeitamos a hip\u00f3tese nula. Podemos dizer que o resultado do teste n\u00e3o detecta uma rela\u00e7\u00e3o significativa entre as vari\u00e1veis; Se o valor-p encontrado for menor que alfa , podemos dizer que h\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre as vari\u00e1veis. # exemplo de tabela de contig\u00eancia pd . crosstab ( df_dataset_1 [ 'MaritalStatus' ], df_dataset_1 [ 'FraudFound_P' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FraudFound_P 0 1 MaritalStatus Divorced 73 3 Married 9986 639 Single 4405 278 Widow 32 3 # teste de independ\u00eandia chi-quadrado nivel_significancia = 0.05 teste_independencia = [] for coluna in df_dataset_1 . columns : tabela_contigencia = pd . crosstab ( df_dataset_1 [ coluna ], df_dataset_1 [ 'FraudFound_P' ]) chi2_val , valor_p , _ , __ = stats . chi2_contingency ( tabela_contigencia ) if valor_p < nivel_significancia : teste_independencia . append ([ coluna , chi2_val , valor_p ]) print ( \"H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos:\" ) df_teste_independencia = pd . DataFrame ( teste_independencia , columns = [ 'Atributo' , ' \\u03C7\\u00B2 ' , 'Valor-p' ]) df_teste_independencia . sort_values ( by = 'Valor-p' , inplace = True ) print ( df_teste_independencia ) H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos: Atributo \u03c7\u00b2 Valor-p 10 FraudFound_P 15401.236138 0.000000e+00 7 PolicyType 437.491381 1.768441e-89 20 BasePolicy 402.947238 3.170436e-88 8 VehicleCategory 290.980893 6.520817e-64 6 Fault 264.984556 1.406180e-59 18 AddressChange_Claim 104.722693 9.704718e-22 11 Deductible 72.406255 1.302831e-15 9 VehiclePrice 67.836116 2.888324e-13 13 PastNumberOfClaims 53.541755 1.405198e-11 1 Make 59.815292 2.191573e-06 3 MonthClaimed 42.200514 1.495245e-05 2 AccidentArea 16.901858 3.936304e-05 15 AgeOfPolicyHolder 33.104861 5.896560e-05 4 Sex 13.495678 2.391135e-04 17 NumberOfSuppliments 18.155527 4.085276e-04 5 Age 109.664968 4.472083e-04 0 Month 29.771469 1.720902e-03 14 AgeOfVehicle 21.995137 2.545322e-03 16 AgentType 7.380469 6.593597e-03 19 Year 9.592587 8.260307e-03 12 Days_Policy_Accident 11.569842 2.085381e-02","title":"5.1 Teste de Independ\u00eancia"},{"location":"projects/ds/fraud-detection/#52-analise-grafica","text":"Apenas com os atributos que mostram ter uma potencial rela\u00e7\u00e3o com a vari\u00e1vel alvo, utilizarei de recursos gr\u00e1ficos para visualizar mais detalhes dessas rela\u00e7\u00f5es. def grafico_barras ( tab_cont , titulo , size ): fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = size ) tab_cont [ 0 ] . plot . bar ( ax = ax1 , color = 'green' , rot = 0 ) tab_cont [ 1 ] . plot . bar ( ax = ax2 , color = 'darkred' , rot = 0 ) ax1 . set_title ( titulo , size = 14 , fontweight = \"bold\" ) ax1 . set ( ylabel = 'Total - N\u00e3o Fraude' , xlabel = '' ) ax2 . set ( ylabel = 'Total - Fraude' , xlabel = '' ) return ax1 , ax2 coluna = 'PolicyType' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 15 , 4 )) plt . show () FraudFound_P 0 1 PolicyType Sedan - All Perils 3675 411 Sedan - Collision 5200 384 Sedan - Liability 4951 36 Sport - All Perils 22 0 Sport - Collision 300 48 Sport - Liability 1 0 Utility - All Perils 299 41 Utility - Collision 27 3 Utility - Liability 21 0 coluna = 'BasePolicy' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'VehicleCategory' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () FraudFound_P 0 1 BasePolicy All Perils 3996 452 Collision 5527 435 Liability 4973 36 FraudFound_P 0 1 VehicleCategory Sedan 8875 795 Sport 5274 84 Utility 347 44 coluna = 'Fault' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'AddressChange_Claim' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 7 , 4 )) plt . show () FraudFound_P 0 1 Fault Policy Holder 10343 886 Third Party 4153 37 FraudFound_P 0 1 AddressChange_Claim 1 year 159 11 2 to 3 years 240 51 4 to 8 years 598 33 no change 13498 825 under 6 months 1 3 coluna = 'Deductible' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'VehiclePrice' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 10 , 4 )) plt . show () FraudFound_P 0 1 Deductible 300 6 2 400 13981 856 500 216 47 700 293 18 FraudFound_P 0 1 VehiclePrice 20000 to 29000 7658 421 30000 to 39000 3358 175 40000 to 59000 430 31 60000 to 69000 83 4 less than 20000 993 103 more than 69000 1974 189 coluna = 'PastNumberOfClaims' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () coluna = 'Make' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 15 , 4 )) plt . show () FraudFound_P 0 1 PastNumberOfClaims 1 3351 222 2 to 4 5191 294 more than 4 1942 68 none 4012 339 FraudFound_P 0 1 Make Accura 413 59 BMW 14 1 Chevrolet 1587 94 Dodge 107 2 Ferrari 2 0 Ford 417 33 Honda 2621 179 Jaguar 6 0 Lexus 1 0 Mazda 2231 123 Mecedes 3 1 Mercury 77 6 Nisson 29 1 Pontiac 3624 213 Porche 5 0 Saab 97 11 Saturn 52 6 Toyota 2935 186 VW 275 8 coluna = 'MonthClaimed' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Month' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () FraudFound_P 0 1 MonthClaimed Apr 1189 82 Aug 1034 92 Dec 1097 49 Feb 1209 78 Jan 1354 92 Jul 1169 56 Jun 1215 78 Mar 1251 97 May 1309 102 Nov 1239 46 Oct 1266 73 Sep 1164 78 FraudFound_P 0 1 Month Apr 1200 80 Aug 1043 84 Dec 1223 62 Feb 1184 82 Jan 1324 87 Jul 1196 60 Jun 1241 80 Mar 1258 102 May 1273 94 Nov 1155 46 Oct 1235 70 Sep 1164 76 coluna = 'AccidentArea' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'NumberOfSuppliments' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 4 , 4 )) plt . show () FraudFound_P 0 1 AccidentArea Rural 1464 133 Urban 13032 790 FraudFound_P 0 1 NumberOfSuppliments 1 to 2 2330 159 3 to 5 1920 97 more than 5 3672 195 none 6574 472 coluna = 'AgeOfPolicyHolder' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Age' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 28 , 10 )) plt . show () FraudFound_P 0 1 AgeOfPolicyHolder 16 to 17 288 31 18 to 20 13 2 21 to 25 92 16 26 to 30 580 33 31 to 35 5233 360 36 to 40 3806 237 41 to 50 2684 144 51 to 65 1322 70 over 65 478 30 FraudFound_P 0 1 Age 16.0 8 1 16.5 288 31 17.0 5 1 18.0 40 8 19.0 27 5 ... ... .. 76.0 39 3 77.0 28 1 78.0 33 2 79.0 19 1 80.0 31 1 [66 rows x 2 columns] coluna = 'AgeOfVehicle' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 8 , 4 )) plt . show () coluna = 'Sex' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () FraudFound_P 0 1 AgeOfVehicle 2 years 70 3 3 years 139 13 4 years 208 21 5 years 1262 95 6 years 3220 228 7 years 5482 325 more than 7 3775 206 new 340 32 FraudFound_P 0 1 Sex Female 2315 105 Male 12181 818 coluna = 'AgentType' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'Year' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 3 , 4 )) plt . show () coluna = 'Days_Policy_Accident' tab_cont = pd . crosstab ( index = df_dataset_1 [ coluna ], columns = df_dataset_1 [ 'FraudFound_P' ]) print ( tab_cont ) grafico_barras ( tab_cont , coluna , ( 6 , 4 )) plt . show () FraudFound_P 0 1 AgentType External 14259 919 Internal 237 4 FraudFound_P 0 1 Year 1994 5732 409 1995 4894 301 1996 3870 213 FraudFound_P 0 1 Days_Policy_Accident 1 to 7 13 1 15 to 30 46 3 8 to 15 50 5 more than 30 14341 905 none 46 9","title":"5.2 An\u00e1lise Gr\u00e1fica"},{"location":"projects/ds/fraud-detection/#parte-6-transformar-dados","text":"df_dataset_2 = df_dataset_1 . copy ()","title":"PARTE 6: Transformar dados"},{"location":"projects/ds/fraud-detection/#61-conversao-de-dados-categoricos-e-binarios","text":"A partir da investiga\u00e7\u00e3o realizada anteriormente, os seguintes atributos categ\u00f3ricos podem ser transformados em bin\u00e1rio: AccidentArea : 1=Urban, 0=Rural Sex : 1=Female, 0=Male Fault : 1=Policy Holder, 0=Third Party PoliceReportFiled : 1=No, 0=Yes WitnessPresent : 1=No, 0=Yes AgentType : 1=External 0=Internal categorico_para_binario = [ 'AccidentArea' , 'Sex' , 'Fault' , 'PoliceReportFiled' , 'WitnessPresent' , 'AgentType' ] for coluna in categorico_para_binario : print ( coluna ) valores_unicos = df_dataset_2 [ coluna ] . unique () print ( f \"Valores antigos: { valores_unicos } \" ) df_dataset_2 [ coluna ] = df_dataset_2 [ coluna ] . replace ( valores_unicos [ 0 ], 1 ) . replace ( valores_unicos [ 1 ], 0 ) valores_unicos = df_dataset_2 [ coluna ] . unique () print ( f \"Valores novos: { valores_unicos } \\n \" ) AccidentArea Valores antigos: ['Urban' 'Rural'] Valores novos: [1 0] Sex Valores antigos: ['Female' 'Male'] Valores novos: [1 0] Fault Valores antigos: ['Policy Holder' 'Third Party'] Valores novos: [1 0] PoliceReportFiled Valores antigos: ['No' 'Yes'] Valores novos: [1 0] WitnessPresent Valores antigos: ['No' 'Yes'] Valores novos: [1 0] AgentType Valores antigos: ['External' 'Internal'] Valores novos: [1 0] df_dataset_2 [ categorico_para_binario ] . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 AccidentArea 15419 non-null int64 1 Sex 15419 non-null int64 2 Fault 15419 non-null int64 3 PoliceReportFiled 15419 non-null int64 4 WitnessPresent 15419 non-null int64 5 AgentType 15419 non-null int64 dtypes: int64(6) memory usage: 722.9 KB","title":"6.1. Convers\u00e3o de dados categ\u00f3ricos e bin\u00e1rios"},{"location":"projects/ds/fraud-detection/#62-conversao-dos-dados-ordinais","text":"A partir da investiga\u00e7\u00e3o realizada anteriormente, s\u00e3o atributos ordinais: 'Month', 'DayOfWeek', 'DayOfWeekClaimed', 'MonthClaimed'. Estes devem corresponder \u00e0 sua ordem natural. Exemplo: para o atributo 'Month', 'Jan' deve corresponder a 1, 'Feb' deve corresponder a 2, 'Mar' deve corresponder a 3, etc. dias_da_semana = { 'Monday' : 1 , 'Tuesday' : 2 , 'Wednesday' : 3 , 'Thursday' : 4 , 'Friday' : 5 , 'Saturday' : 6 , 'Sunday' : 7 } meses = { 'Jan' : 1 , 'Feb' : 2 , 'Mar' : 3 , 'Apr' : 4 , 'May' : 5 , 'Jun' : 6 , 'Jul' : 7 , 'Aug' : 8 , 'Sep' : 9 , 'Oct' : 10 , 'Nov' : 11 , 'Dec' : 12 } Month = { 'Month' : meses } DayOfWeek = { 'DayOfWeek' : dias_da_semana } DayOfWeekClaimed = { 'DayOfWeekClaimed' : dias_da_semana } MonthClaimed = { 'MonthClaimed' : meses } mapping = [ Month , DayOfWeek , DayOfWeekClaimed , MonthClaimed ] for i in range ( len ( mapping )): df_dataset_2 . replace ( mapping [ i ], inplace = True ) ordinais = [ 'Month' , 'DayOfWeek' , 'DayOfWeekClaimed' , 'MonthClaimed' ] for coluna in ordinais : print ( coluna ) print ( f \"Valores antigos: { df_dataset_1 [ coluna ] . unique () } \" ) print ( f \"Valores novos: { df_dataset_2 [ coluna ] . unique () } \\n \" ) Month Valores antigos: ['Dec' 'Jan' 'Oct' 'Jun' 'Feb' 'Nov' 'Apr' 'Mar' 'Aug' 'Jul' 'May' 'Sep'] Valores novos: [12 1 10 6 2 11 4 3 8 7 5 9] DayOfWeek Valores antigos: ['Wednesday' 'Friday' 'Saturday' 'Monday' 'Tuesday' 'Sunday' 'Thursday'] Valores novos: [3 5 6 1 2 7 4] DayOfWeekClaimed Valores antigos: ['Tuesday' 'Monday' 'Thursday' 'Friday' 'Wednesday' 'Saturday' 'Sunday'] Valores novos: [2 1 4 5 3 6 7] MonthClaimed Valores antigos: ['Jan' 'Nov' 'Jul' 'Feb' 'Mar' 'Dec' 'Apr' 'Aug' 'May' 'Jun' 'Sep' 'Oct'] Valores novos: [ 1 11 7 2 3 12 4 8 5 6 9 10]","title":"6.2 Convers\u00e3o dos dados ordinais"},{"location":"projects/ds/fraud-detection/#63-conversao-dos-dados-de-intervalo","text":"A partir da investiga\u00e7\u00e3o realizada anteriormente, s\u00e3o atributos de intervalos: 'PastNumberOfClaims', 'NumberOfSuppliments', 'VehiclePrice', 'Days_Policy_Accident', 'Days_Policy_Claim', 'AgeOfVehicle', 'AgeOfPolicyHolder', 'AddressChange_Claim', 'NumberOfCars'. Estes ser\u00e3o nivelados por baixo. Exemplo: para o atributo 'NumberOfSuppliments', 'none' deve corresponder a 0, '1 to 2' deve corresponder a 1, '3 to 5' deve corresponder a 3 e 'more than 5' deve corresponder a 6. PastNumberOfClaims = { 'PastNumberOfClaims' :{ 'none' : 0 , '1' : 1 , '2 to 4' : 2 , 'more than 4' : 5 }} NumberOfSuppliments = { 'NumberOfSuppliments' :{ 'none' : 0 , '1 to 2' : 1 , '3 to 5' : 3 , 'more than 5' : 6 }} AgeOfVehicle = { 'AgeOfVehicle' :{ '3 years' : 3 , '6 years' : 6 , '7 years' : 7 , 'more than 7' : 8 , '5 years' : 5 , 'new' : 0 , '4 years' : 4 , '2 years' : 2 }} VehiclePrice = { 'VehiclePrice' :{ 'more than 69000' : 69001 , '20000 to 29000' : 20000 , '30000 to 39000' : 30000 , 'less than 20000' : 19999 , '40000 to 59000' : 40000 , '60000 to 69000' : 60000 }} Days_Policy_Accident = { 'Days_Policy_Accident' :{ 'more than 30' : 31 , '15 to 30' : 15 , 'none' : 0 , '1 to 7' : 1 , '8 to 15' : 8 }} Days_Policy_Claim = { 'Days_Policy_Claim' :{ 'more than 30' : 31 , '15 to 30' : 15 , '8 to 15' : 8 , 'none' : 0 }} AgeOfPolicyHolder = { 'AgeOfPolicyHolder' :{ '26 to 30' : 26 , '31 to 35' : 31 , '41 to 50' : 41 , '51 to 65' : 51 , '21 to 25' : 21 , '36 to 40' : 36 , '16 to 17' : 16 , 'over 65' : 66 , '18 to 20' : 18 }} AddressChange_Claim = { 'AddressChange_Claim' :{ '1 year' : 1 , 'no change' : 0 , '4 to 8 years' : 4 , '2 to 3 years' : 2 , 'under 6 months' : 0.5 }} NumberOfCars = { 'NumberOfCars' :{ '3 to 4' : 3 , '1 vehicle' : 1 , '2 vehicles' : 2 , '5 to 8' : 5 , 'more than 8' : 9 }} mapping = [ PastNumberOfClaims , NumberOfSuppliments , VehiclePrice , AgeOfVehicle , Days_Policy_Accident , Days_Policy_Claim , AgeOfPolicyHolder , AddressChange_Claim , NumberOfCars ] for i in range ( len ( mapping )): df_dataset_2 . replace ( mapping [ i ], inplace = True ) dados_intervalos = [ 'PastNumberOfClaims' , 'NumberOfSuppliments' , 'VehiclePrice' , 'Days_Policy_Accident' , 'Days_Policy_Claim' , 'AgeOfVehicle' , 'AgeOfPolicyHolder' , 'AddressChange_Claim' , 'NumberOfCars' ] for coluna in dados_intervalos : print ( coluna ) print ( f \"Valores antigos: { df_dataset_1 [ coluna ] . unique () } \" ) print ( f \"Valores novos: { df_dataset_2 [ coluna ] . unique () } \\n \" ) PastNumberOfClaims Valores antigos: ['none' '1' '2 to 4' 'more than 4'] Valores novos: [0 1 2 5] NumberOfSuppliments Valores antigos: ['none' 'more than 5' '3 to 5' '1 to 2'] Valores novos: [0 6 3 1] VehiclePrice Valores antigos: ['more than 69000' '20000 to 29000' '30000 to 39000' 'less than 20000' '40000 to 59000' '60000 to 69000'] Valores novos: [69001 20000 30000 19999 40000 60000] Days_Policy_Accident Valores antigos: ['more than 30' '15 to 30' 'none' '1 to 7' '8 to 15'] Valores novos: [31 15 0 1 8] Days_Policy_Claim Valores antigos: ['more than 30' '15 to 30' '8 to 15'] Valores novos: [31 15 8] AgeOfVehicle Valores antigos: ['3 years' '6 years' '7 years' 'more than 7' '5 years' 'new' '4 years' '2 years'] Valores novos: [3 6 7 8 5 0 4 2] AgeOfPolicyHolder Valores antigos: ['26 to 30' '31 to 35' '41 to 50' '51 to 65' '21 to 25' '36 to 40' '16 to 17' 'over 65' '18 to 20'] Valores novos: [26 31 41 51 21 36 16 66 18] AddressChange_Claim Valores antigos: ['1 year' 'no change' '4 to 8 years' '2 to 3 years' 'under 6 months'] Valores novos: [1. 0. 4. 2. 0.5] NumberOfCars Valores antigos: ['3 to 4' '1 vehicle' '2 vehicles' '5 to 8' 'more than 8'] Valores novos: [3 1 2 5 9]","title":"6.3 Convers\u00e3o dos dados de intervalo"},{"location":"projects/ds/fraud-detection/#63-conversao-dos-dados-categoricos-e-nominais","text":"A partir da investiga\u00e7\u00e3o realizada anteriormente, os seguintes atributos s\u00e3o categ\u00f3ricos e nominais: 'Make','MaritalStatus','PolicyType','VehicleCategory' e 'BasePolicy'. Agora, estes ter\u00e3o uma representa\u00e7\u00e3o vetorial bin\u00e1ria. df_dataset_2 . info ( verbose = False ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Columns: 32 entries, Month to BasePolicy dtypes: float64(2), int64(25), object(5) memory usage: 3.8+ MB # get_dummies transforma as colunas do tipo 'object' # 19 categorias de 'Make' + 4 categorias de 'MaritalStatus' + 9 categorias de 'PolicyType' # + 3 categorias de 'VehicleCategory' + 3 categorias de 'BasePolicy' # = 38 colunas df_dataset_2 = pd . get_dummies ( df_dataset_2 ); df_dataset_2 . info ( verbose = False ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 15419 entries, 0 to 15418 Columns: 65 entries, Month to BasePolicy_Liability dtypes: float64(2), int64(25), uint8(38) memory usage: 3.7 MB","title":"6.3 Convers\u00e3o dos dados categ\u00f3ricos e nominais"},{"location":"projects/ds/fraud-detection/#parte-7-investigar-relacoes-com-a-variavel-alvo-novamente","text":"Ser\u00e1 realizado o mesmo teste de independ\u00eancia executado na PARTE 5. A ideia \u00e9 verificar se, ap\u00f3s as transforma\u00e7\u00f5es, aparecem novas rela\u00e7\u00f5es relevantes entre os atributos e o target ('FraudFound_P'). df_final = df_dataset_2 . copy () # teste de independ\u00eandia chi-quadrado nivel_significancia = 0.05 teste_independencia = [] for coluna in df_final . columns : tabela_contigencia = pd . crosstab ( df_final [ coluna ], df_final [ 'FraudFound_P' ]) chi2_val , valor_p , _ , __ = stats . chi2_contingency ( tabela_contigencia ) if valor_p < nivel_significancia : teste_independencia . append ([ coluna , chi2_val , valor_p ]) print ( \"H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos:\" ) df_teste_independencia = pd . DataFrame ( teste_independencia , columns = [ 'Atributo' , ' \\u03C7\\u00B2 ' , 'Valor-p' ]) df_teste_independencia . sort_values ( by = 'Valor-p' , inplace = True ) print ( df_teste_independencia ) H\u00e1 evid\u00eancias estat\u00edsticas suficientes de que existe alguma rela\u00e7\u00e3o entre FraudFound_P e os seguintes atributos: Atributo \u03c7\u00b2 Valor-p 7 FraudFound_P 15401.236138 0.000000e+00 29 BasePolicy_Liability 364.390028 3.116620e-81 21 PolicyType_Sedan - Liability 361.581932 1.273869e-80 25 VehicleCategory_Sport 283.640594 1.208553e-63 5 Fault 264.984556 1.406180e-59 24 VehicleCategory_Sedan 229.174903 9.021607e-52 27 BasePolicy_All Perils 192.648135 8.400805e-44 19 PolicyType_Sedan - All Perils 162.856618 2.688717e-37 15 AddressChange_Claim 104.722693 9.704718e-22 8 Deductible 72.406255 1.302831e-15 6 VehiclePrice 67.836116 2.888324e-13 10 PastNumberOfClaims 53.541755 1.405198e-11 22 PolicyType_Sport - Collision 37.152501 1.092425e-09 17 Make_Accura 35.525852 2.516880e-09 28 BasePolicy_Collision 29.267146 6.305578e-08 23 PolicyType_Utility - All Perils 21.691830 3.201519e-06 26 VehicleCategory_Utility 18.827174 1.431136e-05 2 MonthClaimed 42.200514 1.495245e-05 1 AccidentArea 16.901858 3.936304e-05 12 AgeOfPolicyHolder 33.104861 5.896560e-05 3 Sex 13.495678 2.391135e-04 14 NumberOfSuppliments 18.155527 4.085276e-04 4 Age 109.664968 4.472083e-04 20 PolicyType_Sedan - Collision 12.093358 5.060177e-04 0 Month 29.771469 1.720902e-03 11 AgeOfVehicle 21.995137 2.545322e-03 13 AgentType 7.380469 6.593597e-03 16 Year 9.592587 8.260307e-03 9 Days_Policy_Accident 11.569842 2.085381e-02 18 Make_VW 4.557015 3.278417e-02","title":"PARTE 7: Investigar rela\u00e7\u00f5es com a vari\u00e1vel alvo (novamente)"},{"location":"projects/ds/fraud-detection/#parte-8-dividir-variaveis-dependentes-e-independentes","text":"Vari\u00e1vel dependente/alvo: 'FraudFound_P' X = df_final . drop ( 'FraudFound_P' , axis = 1 ) . copy () y = df_final [ 'FraudFound_P' ] . copy () print ( X . shape , y . shape ) (15419, 64) (15419,)","title":"PARTE 8: Dividir vari\u00e1veis dependentes e independentes"},{"location":"projects/ds/fraud-detection/#parte-9-dividir-dados-de-treino-e-de-teste","text":"70% treino e 30% teste X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , stratify = y , random_state = 42 ) print ( X_train . shape , X_test . shape , y_train . shape , y_test . shape ) (10793, 64) (4626, 64) (10793,) (4626,)","title":"PARTE 9: Dividir dados de treino e de teste"},{"location":"projects/ds/fraud-detection/#parte-10-balancear-dados-de-treino","text":"M\u00e9todo escolhido: ADASYN y_train . value_counts () 0 10147 1 646 Name: FraudFound_P, dtype: int64 X_train_over , y_train_over = ADASYN () . fit_resample ( X_train , y_train ) y_train_over . value_counts () 0 10147 1 10088 Name: FraudFound_P, dtype: int64","title":"PARTE 10: Balancear dados de treino"},{"location":"projects/ds/fraud-detection/#parte-11-rodar-experimentos","text":"Ser\u00e3o avaliadas diferentes configura\u00e7\u00f5es do algoritmo Random Forest.","title":"PARTE 11: Rodar experimentos"},{"location":"projects/ds/fraud-detection/#111-criar-o-modelo-de-classificacao-e-definir-os-possiveis-hiperparametros","text":"RF_classifier = RandomForestClassifier () n_estimators = [ 10 , 100 , 300 ] max_features = [ 'sqrt' , 'log2' ] max_depth = [ 3 , 5 , 8 ] RF_grid = dict ( n_estimators = n_estimators , max_features = max_features , max_depth = max_depth )","title":"11.1 Criar o modelo de classifica\u00e7\u00e3o e definir os poss\u00edveis hiperpar\u00e2metros"},{"location":"projects/ds/fraud-detection/#112-definir-as-metricas-de-comparacao","text":"metricas = { 'accuracy_score' : metrics . make_scorer ( metrics . accuracy_score ), 'f1_score' : metrics . make_scorer ( metrics . f1_score ), 'roc_auc_score' : metrics . make_scorer ( metrics . roc_auc_score ) }","title":"11.2 Definir as m\u00e9tricas de compara\u00e7\u00e3o"},{"location":"projects/ds/fraud-detection/#113-definir-os-parametros-dos-experimentos","text":"# valida\u00e7\u00e3o cruzada n_splits = 5 n_repeats = 5 cv = RepeatedStratifiedKFold ( n_splits = n_splits , n_repeats = n_repeats )","title":"11.3 Definir os par\u00e2metros dos experimentos"},{"location":"projects/ds/fraud-detection/#114-executar-os-experimentos","text":"Os experimentos ser\u00e3o realizados a partir do 'GridSearchCV', assim, ser\u00e3o realizadas n repeti\u00e7\u00f5es de valida\u00e7\u00e3o cruzada para cada poss\u00edvel configura\u00e7\u00e3o de classificador. No final, teremos dispon\u00edveis, para cada m\u00e9trica definida anteriormente, os resultados de cada experimento para cada configura\u00e7\u00e3o de classificador. grid_search = GridSearchCV ( estimator = RF_classifier , param_grid = RF_grid , cv = cv , scoring = metricas , refit = 'accuracy_score' , error_score = 0 ) grid_result = grid_search . fit ( X_train_over , y_train_over )","title":"11.4 Executar os experimentos"},{"location":"projects/ds/fraud-detection/#parte-12-analisar-os-resultados","text":"\u00c9 poss\u00edvel observar os resultados do grid search a partir do atributo 'cv_results_'. Para criar os gr\u00e1ficos, ser\u00e1 realizado um tratamento no formato original retornado por esse atributo. id_vars = [ 'mean_fit_time' , 'std_fit_time' , 'mean_score_time' , 'std_score_time' , 'params' , 'mean_test_accuracy_score' , 'std_test_accuracy_score' , 'rank_test_accuracy_score' , 'mean_test_f1_score' , 'std_test_f1_score' , 'rank_test_f1_score' , 'mean_test_roc_auc_score' , 'std_test_roc_auc_score' , 'rank_test_roc_auc_score' ] id_vars = id_vars + [ 'param_max_features' , 'param_n_estimators' , 'param_max_depth' ] df_results_RF = pd . DataFrame ( grid_result . cv_results_ ) df_results_RF = pd . melt ( df_results_RF , id_vars = id_vars ) df_results_RF . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1350 entries, 0 to 1349 Data columns (total 19 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mean_fit_time 1350 non-null float64 1 std_fit_time 1350 non-null float64 2 mean_score_time 1350 non-null float64 3 std_score_time 1350 non-null float64 4 params 1350 non-null object 5 mean_test_accuracy_score 1350 non-null float64 6 std_test_accuracy_score 1350 non-null float64 7 rank_test_accuracy_score 1350 non-null int32 8 mean_test_f1_score 1350 non-null float64 9 std_test_f1_score 1350 non-null float64 10 rank_test_f1_score 1350 non-null int32 11 mean_test_roc_auc_score 1350 non-null float64 12 std_test_roc_auc_score 1350 non-null float64 13 rank_test_roc_auc_score 1350 non-null int32 14 param_max_features 1350 non-null object 15 param_n_estimators 1350 non-null object 16 param_max_depth 1350 non-null object 17 variable 1350 non-null object 18 value 1350 non-null float64 dtypes: float64(11), int32(3), object(5) memory usage: 184.7+ KB df_results_RF [[ 'params' , 'variable' , 'value' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } params variable value 0 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.791698 1 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.832963 2 {'max_depth': 3, 'max_features': 'sqrt', 'n_es... split0_test_accuracy_score 0.835434 3 {'max_depth': 3, 'max_features': 'log2', 'n_es... split0_test_accuracy_score 0.846306 4 {'max_depth': 3, 'max_features': 'log2', 'n_es... split0_test_accuracy_score 0.843094 ... ... ... ... 1345 {'max_depth': 8, 'max_features': 'sqrt', 'n_es... split24_test_roc_auc_score 0.915609 1346 {'max_depth': 8, 'max_features': 'sqrt', 'n_es... split24_test_roc_auc_score 0.917331 1347 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.907197 1348 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.919056 1349 {'max_depth': 8, 'max_features': 'log2', 'n_es... split24_test_roc_auc_score 0.920050 1350 rows \u00d7 3 columns Agora, basta filtrar cada configura\u00e7\u00e3o dos par\u00e2metros e cada m\u00e9trica para que tenhamos os resultados de cada experimento realizado. def valores ( modelo , parametros , metrica ): col1 , param1 , col2 , param2 , col3 , param3 = parametros val = modelo [( modelo [ \"param_\" + col1 ] == param1 ) & ( modelo [ \"param_\" + col2 ] == param2 ) & ( modelo [ \"param_\" + col3 ] == param3 )] val = val . loc [ val . variable . str . contains ( f '[a-zA-Z]+[0-9]+_[a-zA-Z]+_ { metrica } +' ), 'value' ] return val metrica = [ * metricas . keys ()] dict_acuracia , dict_f1 , dict_roc = {}, {}, {} parametros_combinacoes = list ( df_results_RF [ 'params' ] . value_counts () . index ) for combinacao in [ i . items () for i in parametros_combinacoes ]: # param = (nome, valor) param1 , param2 , param3 = list ( combinacao ) parametros = [ param1 [ 0 ], param1 [ 1 ], param2 [ 0 ], param2 [ 1 ], param3 [ 0 ], param3 [ 1 ]] nome_coluna = f \" { param1 [ 0 ][: 6 ] } _ { param1 [ 1 ] } - { param2 [ 0 ][: 6 ] } _ { param2 [ 1 ] } - { param3 [ 0 ][: 6 ] } _ { param3 [ 1 ] } \" dict_acuracia [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 0 ]) dict_f1 [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 1 ]) dict_roc [ nome_coluna ] = valores ( df_results_RF , parametros , metrica [ 2 ]) df_acuracia , df_f1 , df_roc = pd . DataFrame ( dict_acuracia ), pd . DataFrame ( dict_f1 ), pd . DataFrame ( dict_roc ) # exemplo de como est\u00e3o sendo armazenados os resultados df_acuracia . info () <class 'pandas.core.frame.DataFrame'> Int64Index: 450 entries, 0 to 449 Data columns (total 18 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 max_de_8-max_fe_log2-n_esti_10 25 non-null float64 1 max_de_5-max_fe_log2-n_esti_300 25 non-null float64 2 max_de_3-max_fe_log2-n_esti_100 25 non-null float64 3 max_de_5-max_fe_sqrt-n_esti_300 25 non-null float64 4 max_de_8-max_fe_sqrt-n_esti_10 25 non-null float64 5 max_de_8-max_fe_log2-n_esti_100 25 non-null float64 6 max_de_3-max_fe_sqrt-n_esti_100 25 non-null float64 7 max_de_3-max_fe_log2-n_esti_300 25 non-null float64 8 max_de_5-max_fe_log2-n_esti_10 25 non-null float64 9 max_de_8-max_fe_sqrt-n_esti_100 25 non-null float64 10 max_de_8-max_fe_log2-n_esti_300 25 non-null float64 11 max_de_3-max_fe_sqrt-n_esti_300 25 non-null float64 12 max_de_5-max_fe_sqrt-n_esti_10 25 non-null float64 13 max_de_5-max_fe_log2-n_esti_100 25 non-null float64 14 max_de_8-max_fe_sqrt-n_esti_300 25 non-null float64 15 max_de_3-max_fe_log2-n_esti_10 25 non-null float64 16 max_de_5-max_fe_sqrt-n_esti_100 25 non-null float64 17 max_de_3-max_fe_sqrt-n_esti_10 25 non-null float64 dtypes: float64(18) memory usage: 66.8 KB #plot acur\u00e1cia fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_acuracia , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"Acur\u00e1cia\" ) plt . xticks ( rotation =- 90 ) plt . show () #plot f1 score fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_f1 , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"F1\" ) plt . xticks ( rotation =- 90 ) plt . show () #plot \u00e1rea sob a curva ROC fig , ax = plt . subplots ( figsize = ( 9 , 4 )) sns . boxplot ( ax = ax , data = df_roc , width = 0.5 , fliersize = 3 ) ax . set_xlabel ( \"Classificadores\" ) ax . set_ylabel ( \"AUC ROC\" ) plt . xticks ( rotation =- 90 ) plt . show () Observando cada configura\u00e7\u00e3o, os valores para cada uma das tr\u00eas m\u00e9tricas s\u00e3o semelhantes. \u00c9 importante deixar claro que n\u00e3o h\u00e1 um significado espec\u00edfico para isso. Conhecendo as express\u00f5es matem\u00e1ticas de cada uma dessas m\u00e9tricas, esse comportamento \u00e9 previsto quando os valores de Falso Positivo e Falso Negativo s\u00e3o iguais (ou perto disso) e pequenos. Com isso, a configura\u00e7\u00e3o escolhida \u00e9 a com melhor desempenho (considerando a distribui\u00e7\u00e3o pr\u00f3ximo a 1) em todas (ou na maioria) das m\u00e9tricas. Sendo assim, foi escolhida a configura\u00e7\u00e3o com max_depth=8, n_estimators=300 e max_features='sqrt'.","title":"PARTE 12: Analisar os resultados"},{"location":"projects/ds/fraud-detection/#parte-13-investigar-o-melhor-modelo","text":"best_model = RandomForestClassifier ( max_depth = 8 , n_estimators = 300 , max_features = 'sqrt' ) . fit ( X_train_over , y_train_over ) A primeira abordagem escolhida como forma de entender quais atributos s\u00e3o considerados mais relevantes na classifica\u00e7\u00e3o do modelo com melhor desempenho foi a de observar a Import\u00e2ncia da Permuta\u00e7\u00e3o. Este m\u00e9todo embaralha aleatoriamente cada atributo e calcula a mudan\u00e7a no desempenho do modelo. Ent\u00e3o, quanto mais uma mudan\u00e7a em um determinado atributo impacta no desempenho do modelo, mais importante este atributo \u00e9. perm_importance = permutation_importance ( best_model , X_test , y_test , n_repeats = 10 ) best_model_importances = pd . Series ( perm_importance . importances_mean , index = X_test . columns ) best_model_importances = best_model_importances . sort_values ( ascending = True ) fig , ax = plt . subplots () best_model_importances . plot . barh ( yerr = perm_importance . importances_std , ax = ax , figsize = ( 15 , 9 )) ax . set_title ( \"Import\u00e2ncia da Permuta\u00e7\u00e3o (dados de treino)\" ) fig . tight_layout () plt . show () Por\u00e9m, um problema foi notado: comparando o resultado acima com o teste de independ\u00eancia realizado na PARTE 7, \u00e9 poss\u00edvel observar que os dois atributos que aparentam ter maior rela\u00e7\u00e3o com a vari\u00e1vel alvo ('BasePolicy_Liability' e 'PolicyType_Sedan - Liability'), aparecem com uma import\u00e2ncia baix\u00edssima. Isso se d\u00e1 por conta de uma limita\u00e7\u00e3o da abordagem escolhida. A Import\u00e2ncia da Permuta\u00e7\u00e3o n\u00e3o performa bem caso existam atributos que n\u00e3o sejam totalmente independentes e que, de alguma forma, se correlacionem. E, como pode ser visto abaixo, estes dois atributos citados podem ser considerados colineares (se correlacionando de alguma forma, mas n\u00e3o necessariamente com uma rela\u00e7\u00e3o estritamente linear). pd . crosstab ( df_final [ 'PolicyType_Sedan - Liability' ], df_final [ 'BasePolicy_Liability' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BasePolicy_Liability 0 1 PolicyType_Sedan - Liability 0 10410 22 1 0 4987 Uma outra forma de entender quais atributos s\u00e3o considerados mais relevantes na classifica\u00e7\u00e3o do modelo com melhor desempenho \u00e9 com a Import\u00e2ncia da Caracter\u00edstica da Floresta Aleat\u00f3ria, um recurso baseado em impureza. O princ\u00edpio desse m\u00e9todo \u00e9 baseado no crit\u00e9rio considerado na divis\u00e3o dos n\u00f3s em uma \u00e1rvore de decis\u00e3o, onde a 'impureza' se refere a uma m\u00e9trica usada para determinar como (usando qual atributo e em qual limite) dividir os dados em grupos menores. Se o atributo \u00e9 \u00fatil, este tende a dividir n\u00f3s rotulados mistos em n\u00f3s puros de classe \u00fanica. Ent\u00e3o, podemos medir como cada atributo diminui a impureza da divis\u00e3o (o recurso com maior diminui\u00e7\u00e3o \u00e9 selecionado para o n\u00f3 interno). Para cada atributo, podemos coletar como, em m\u00e9dia, diminui a impureza. A m\u00e9dia de todas as \u00e1rvores \u00e9 a medida da import\u00e2ncia da caracter\u00edstica. importances = best_model . feature_importances_ best_model_importances = pd . Series ( importances , index = X_train_over . columns ) best_model_importances = best_model_importances . sort_values ( ascending = True ) fig , ax = plt . subplots () best_model_importances . plot . barh ( ax = ax , figsize = ( 17 , 9 )) ax . set_title ( \"Import\u00e2ncia da Caracter\u00edstica da Floresta Aleat\u00f3ria - Diminui\u00e7\u00e3o M\u00e9dia de Impureza (MDI)\" ) fig . tight_layout () Como todos os valores necess\u00e1rios s\u00e3o calculados durante o treinamento, este m\u00e9todo \u00e9 executado rapidamente. Al\u00e9m disso, \u00e9 poss\u00edvel observar um comportamento mais semelhante com o resultado obtido no teste de independ\u00eancia da PARTE 7. Por\u00e9m, \u00e9 importante salientar que este m\u00e9todo tem a tend\u00eancia de preferir (selecionar como importantes) atributos num\u00e9ricos e atributos categ\u00f3ricos com alta cardinalidade (muitos valores exclusivos, como era o caso de 'PolicyNumber', exclu\u00eddo no tratamento dos dados). E, no caso de atributos correlacionados, pode-se selecionar apenas um dos atributos e negligenciar a import\u00e2ncia do outro (o que pode levar a conclus\u00f5es erradas). \u00c9 importante entender que qualquer abordagem escolhida traz consigo alguma limita\u00e7\u00e3o. Para este projeto, apesar das desvantagens comentadas, o segundo m\u00e9todo apresentou o melhor desempenho, corroborando com algumas an\u00e1lises realizadas anteriormente e permitindo uma maior compreens\u00e3o do problema. Agora, com o modelo treinado (usando dados balanceados), verificaremos a performance com os dados de teste (que mant\u00eam a propor\u00e7\u00e3o original). y_pred = best_model . predict ( X_test ) matriz_confusao = metrics . confusion_matrix ( y_test , y_pred ) ax = sns . heatmap ( matriz_confusao , annot = True , fmt = 'g' , cmap = 'Blues' ) ax . set ( xlabel = ' \\n Predi\u00e7\u00e3o' , ylabel = 'Real' ) ax . set ( xticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ], yticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ]) plt . show () ax1 = sns . heatmap ( matriz_confusao / np . sum ( matriz_confusao ), annot = True , cmap = 'Blues' ) ax1 . set ( xlabel = ' \\n Predi\u00e7\u00e3o' , ylabel = 'Real' ) ax1 . set ( xticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ], yticklabels = [ 'N\u00e3o Fraude' , 'Fraude' ]) plt . show () print ( f \"acur\u00e1cia: { metrics . accuracy_score ( y_test , y_pred ) } \" ) print ( f \"precis\u00e3o: { metrics . precision_score ( y_test , y_pred ) } \" ) print ( f \"recall: { metrics . recall_score ( y_test , y_pred ) } \" ) print ( f \"f1-score: { metrics . f1_score ( y_test , y_pred ) } \" ) print ( f \"AUC ROC: { metrics . roc_auc_score ( y_test , y_pred ) } \" ) acur\u00e1cia: 0.8495460440985733 precis\u00e3o: 0.16264090177133655 recall: 0.36462093862815886 f1-score: 0.22494432071269488 AUC ROC: 0.6225266109558362","title":"PARTE 13: Investigar o melhor modelo"},{"location":"projects/ds/fraud-detection/#parte-14-conclusao-e-discussao","text":"Nota-se um desempenho diferente, em geral menor, quando observados os resultados de teste, comparando-os com os resultados da valida\u00e7\u00e3o cruzada. Por\u00e9m, isso \u00e9 esperado, pois os dados utilizados na valida\u00e7\u00e3o cruzada e no teste s\u00e3o diferentes e, ainda mais importante, os dados de teste apresentam o desbalanceamento real de solicita\u00e7\u00f5es identificadas ou n\u00e3o como fraudulentas. Dessa forma, conseguimos verificar se, de fato, o modelo aprendeu a generalizar o problema. O que \u00e9 poss\u00edvel tirar de conclus\u00e3o a partir dos resultados: a pessoa considerada culpada pelo acidente (titular da ap\u00f3lice ou terceiro) demonstra ser bastante importante; o tipo de ap\u00f3lice contratada se mostra bem relevante para algumas das suas poss\u00edveis combina\u00e7\u00f5es, como para o tipo de seguro 'Liability' (indeniza\u00e7\u00e3o \u00e0 terceiros quando o titular da ap\u00f3lice \u00e9 o culpado) e para as categorias de ve\u00edculo 'Sedan' e 'Sport'; em rela\u00e7\u00e3o ao hist\u00f3rico do titular da ap\u00f3lice vemos 'AddressChange_Claim' e 'PastNumberOfClaims' como fatores importantes. Por\u00e9m, 'Days_Policy_Accident' e 'Days_Policy_Claim' n\u00e3o parecem ser t\u00e3o significativos; j\u00e1 sobre as caracter\u00edsticas pessoais do titular da ap\u00f3lice/condutor, observa-se uma relev\u00e2ncia maior para 'Sex' e um pouco menos para 'Age' e 'MaritalStatus'; alguns fatores mostraram pouca relev\u00e2ncia para o problema, como: presen\u00e7a de testemunha no momento do acidente ('WitnessPresent'), realiza\u00e7\u00e3o de boletim de ocorr\u00eancia para o acontecido ('PoliceReportFiled') e a \u00e1rea em que o acidente ocorreu ('AccidentArea').","title":"PARTE 14: Conclus\u00e3o e discuss\u00e3o"}]}